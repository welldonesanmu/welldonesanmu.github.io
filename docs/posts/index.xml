<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Sanmu</title>
    <link>https://welldonesanmu.github.io/posts/</link>
    <description>Recent content in Posts on Sanmu</description>
    <image>
      <title>Sanmu</title>
      <url>https://welldonesanmu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://welldonesanmu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 19 May 2024 09:06:49 +0800</lastBuildDate><atom:link href="https://welldonesanmu.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DL</title>
      <link>https://welldonesanmu.github.io/posts/dl/</link>
      <pubDate>Sun, 19 May 2024 09:06:49 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/dl/</guid>
      <description>神经网络 DNN/MLP/ANN 全连接神经网络/多层感知器
DNN是从多层感知器发展而来的第三代神经网络，DNN的网络结构分为三部分：输入层、隐藏层、输出层。
DNN的层与层之间是全连接的，第i层的任意一个神经元一定与第i+1层的任意一个神经元连接.
https://www.cnblogs.com/init0ne/p/16630311.html
KNN KNN（K-Nearest Neighbors，K近邻算法）是一种常见的无参数（non-parametric）分类和回归算法，常用于基于实例的学习（instance-based learning）。
对于分类问题，KNN算法会查找与该未知样本最近的K个样本点，然后将这K个样本点中出现最多的类别作为该未知样本的预测类别。对于回归问题，KNN算法会查找与该未知样本最近的K个样本点，并将这K个样本点的输出值的平均值作为该未知样本的预测输出值。
KNN算法中，距离度量是非常重要的一环。通常使用欧氏距离（Euclidean distance）或曼哈顿距离（Manhattan distance）等常见的距离度量方法。另外，KNN算法还需要确定K值的大小，通常通过交叉验证（cross-validation）等技术进行调整。
KNN算法的优点是简单易懂、易于实现、对于大规模数据集也可以高效处理。缺点是需要对每个未知样本进行计算距离和排序，当数据集规模很大时，计算成本会非常高，同时对于高维度的数据集，样本之间的距离可能会失去准确性，从而影响KNN算法的表现。
RNN RNN是一种神经网络结构，它通过在不同时间步之间共享权重来处理序列数据。与其他神经网络结构不同的是，RNN 具有记忆能力，可以在处理当前数据时利用之前的信息。它们的优点在于可以接受任意长度的输入序列，并且可以利用序列中之前的信息来帮助预测下一个值。
RNN 的一个基本单元是循环单元（recurrent unit），也称为记忆单元（memory cell），可以是简单的结构（如 Vanilla RNN）或复杂的结构（如 LSTM 和 GRU）。这些循环单元接受输入和先前的隐藏状态，产生一个新的隐藏状态，并输出一个值，该值可以被用于下一时间步的计算。通过不断地将先前的隐藏状态传递给下一时间步，RNN 可以利用历史信息来影响当前的输出。
RNN 的训练可以通过反向传播算法进行，这可以通过计算当前时间步的误差来更新网络的参数，以改善其性能。但是，RNN 的训练存在梯度消失或梯度爆炸等问题，这可能导致在训练过程中难以传递长期依赖的信息。为了解决这些问题，出现了一些改进的 RNN 结构，如 LSTM（长短时记忆网络）和 GRU（门控循环单元）。这些结构可以更好地处理长期依赖和短期信息。
优缺点 优点：
RNN 具有记忆能力，可以利用序列数据之前的信息来预测下一个值，从而更好地处理序列数据。 RNN 可以接受任意长度的输入序列，并且可以在每个时间步骤使用相同的参数，从而可以处理变长的序列数据。 RNN 的参数可以通过反向传播算法来训练，这使得其可以学习到数据之间的关系，从而可以在自然语言处理、语音识别等领域中得到广泛应用。 缺点：
RNN 计算的时间复杂度与序列长度成正比，处理长序列时可能会面临较大的计算开销和内存消耗。 RNN 的训练可能会受到梯度消失或梯度爆炸等问题的影响，尤其是在处理长序列数据时，这些问题可能会导致模型难以捕捉到序列中的长期依赖关系。 RNN 的记忆能力有限，无法捕捉到非常长的历史信息，这可能会影响模型的预测性能。 RNN为什么会梯度消失或爆炸 RNN（循环神经网络）在处理长序列时，存在梯度消失或梯度爆炸的问题，这是由于其计算方式和训练过程的特性所导致的。
具体来说，RNN 的计算方式是通过重复应用相同的权重来处理序列数据，即在每个时间步使用相同的权重矩阵进行计算，这种计算方式会导致随着序列长度的增加，信息在网络中传递时会经过多次的矩阵乘法，从而使得梯度在反向传播过程中会不断地被乘以相同的权重，从而可能会出现指数级的增长或衰减，这就是梯度爆炸或梯度消失的问题。
RNN的梯度消失不是真正的梯度消失，当下的梯度总会在，只是前面的梯度没了。RNN的梯度消失指的是当下梯度用不到前面的梯度了。
当梯度爆炸发生时，梯度的值变得非常大，导致网络参数更新过快，使得网络的性能变得不稳定。当梯度消失发生时，梯度的值变得非常小，导致网络参数更新过慢，使得网络无法学习到长期依赖的信息，从而影响了网络的预测性能。
为了解决这些问题，一些改进的 RNN 结构被提出，如 LSTM（长短时记忆网络）和 GRU（门控循环单元），它们使用了一些特殊的门机制来控制信息的流动和更新，从而更好地处理了长序列数据，并有效地缓解了梯度消失或梯度爆炸的问题。
LSTM LSTM（长短时记忆网络）是一种特殊类型的循环神经网络（RNN），它在处理序列数据时能够有效地捕捉长期依赖关系，通过使用一组称为“门”的结构来控制信息的流动，包括输入门、遗忘门和输出门。这些门可以在不同的时间步骤决定从当前输入和前一个时间步骤的输出中选择哪些信息需要被传递和保留。此外，LSTM还包括一个称为细胞状态的重要部分，可以在时间步骤之间存储和传递信息。
LSTM的优点在于它可以避免梯度消失问题，而且能够处理长期依赖关系，即使序列长度很长。此外，LSTM还具有较好的鲁棒性和泛化能力，可以适应不同的任务和数据集。
三个门的作用 1)遗忘门ft控制上一个时刻的内部状态ct-1需要遗忘多少信息。
2)输入门it控制当前的候选状态有多少信息需要保存。
3)输出门ot控制当前时刻的内部状态ct有多少信息需要输出给外部状态ht
激活函数 LSTM（长短时记忆）是一种递归神经网络，由输入门、遗忘门、输出门和一个单元状态组成。每个门都有一个对应的激活函数。以下是LSTM中使用的激活函数：</description>
    </item>
    
    <item>
      <title>机器学习八股</title>
      <link>https://welldonesanmu.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AB%E8%82%A1/</link>
      <pubDate>Thu, 16 May 2024 21:24:29 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AB%E8%82%A1/</guid>
      <description>特征工程 为什么要对数值类型的特征做归一化？ 加快收敛速度：对于一些优化算法（例如梯度下降法），归一化可以加快算法的收敛速度。如果特征的值范围差异很大，那么在计算梯度时，梯度的幅度也会差异很大，可能导致收敛速度变慢或者不稳定。归一化后，特征值被压缩到相同的范围内，使得梯度更加均衡，从而加快收敛速度。 提高模型训练的效率和稳定性：许多机器学习算法在训练过程中都会计算特征之间的距离或者权重。特征值范围较大的数值会对模型的训练产生较大的影响，导致模型可能更加关注这些特征而忽略其他特征。归一化可以使所有特征的数值范围相近，从而使得模型的训练更加平衡和稳定。 提高特征之间的可比性：特征归一化可以使不同特征之间的数值范围相同，便于对不同特征进行比较和分析。例如，在主成分分析（PCA）中，归一化可以确保每个特征对主成分的贡献是基于其方差，而不是其数值大小。 常见的归一化方法包括最小-最大归一化（Min-Max Normalization）和标准化（Standardization）。最小-最大归一化将特征值缩放到[0,1]范围内，而标准化则将特征值转换为均值为0、标准差为1的标准正态分布。
经典算法 SVM的原理？ 支持向量机 SVM 的基本原理是通过找到一个最优的超平面，将不同类别的数据点分开。它主要关注以下几个方面：
最大化分类间隔：SVM在寻找超平面时，不仅仅是简单地将数据分开，而是寻找能够最大化分类间隔的超平面。间隔是从超平面到最近数据点（支持向量）的距离，最大化间隔有助于提高模型的泛化能力。
支持向量：这些是离超平面最近的点，对超平面的位置有决定性影响。优化问题的核心是这些支持向量。
软间隔和松弛变量：在处理现实中的数据时，完全线性可分的情况很少见。SVM引入软间隔和松弛变量，允许一些数据点被错误分类，以提高模型的鲁棒性和泛化能力。
核技巧：对于非线性可分的数据，SVM通过核函数将数据映射到更高维的空间，使得在高维空间中数据变得线性可分。常用的核函数包括线性核、多项式核和径向基函数（RBF）核。
优化问题：SVM通过求解一个凸优化问题来找到最优超平面，这个问题可以表示为：
$$ \min_{w, b, \xi} \frac{1}{2} |w|^2 + C \sum_{i=1}^{n} \xi_i $$
其中，$ w $ 是超平面的法向量，$b$ 是偏置项，$\xi_i$ 是松弛变量， $C$ 是正则化参数。
决策函数：训练完成后，SVM的决策函数为： $$ f(x) = \text{sign} \left( \sum_{i=1}^{n} \alpha_i y_i K(x_i, x) + b \right) \ $$
其中，$\alpha_i $是拉格朗日乘子，$y_i$ 是训练样本的标签，$x_i$ 是支持向量，$K(x_i, x) $是核函数。
通过以上步骤，SVM能够有效地处理线性和非线性数据，找到最佳的分类边界，并具有良好的泛化能力。
（总结：SVM 的基本原理是找到一个最优的超平面，将不同的数据分开，使得不同类别的数据点都在不同的一侧。具体来说，这个超平面需要满足所有样本中距离超平面最近的样本点到超平面的距离最大，这些最近的样本点也被称为支持向量。这个问题可以通过凸优化的方法求解，它本质是一个凸二次规划问题。）
为什么SVM对异常值不太敏感？ 1. 最大化间隔的目标 SVM的核心目标是找到一个最大化分类间隔的超平面。即使在数据集中存在异常值，SVM仍会尽量最大化正常数据点与超平面之间的间隔。这意味着异常值对超平面的影响相对较小，因为模型更关注间隔的最大化，而不是单个数据点的位置。在SVM中，只有离超平面最近的那些数据点（即支持向量）决定了超平面的最终位置。异常值通常不会成为支持向量，因为它们远离主流数据分布。由于支持向量对超平面的确定起主要作用，因此异常值对超平面的影响较小。
2. 核技巧的鲁棒性 SVM通过核技巧将数据映射到高维空间，从而在高维空间中实现线性可分。异常值在高维空间中的影响相对低维空间来说可能被进一步削弱，因为核技巧在某种程度上能平滑和分散异常值的影响。
什么是VAE模型？ VAE，变分自编码器，是一种生成模型，也是自编码器（Autoencoder）的一种扩展。它使用了概率统计方法来生成数据，从而可以在一定程度上避免传统自编码器过拟合的问题。VAE可以看作是由两个部分组成的网络，一个是编码器（Encoder），用来将输入的数据转换为潜在空间（Latent Space）中的概率分布，另一个是解码器（Decoder），用来将潜在空间中的随机变量采样成原始数据。</description>
    </item>
    
    <item>
      <title>推荐系统的中 EMBEDDING 的应用实践</title>
      <link>https://welldonesanmu.github.io/posts/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%AD-embedding-%E7%9A%84%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Mon, 13 May 2024 21:35:16 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%AD-embedding-%E7%9A%84%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5/</guid>
      <description>Word2Vec &amp;ndash; Item2Vec &amp;ndash; Graph Embedding Word2Vec Item2Vec Graph Embedding 1. DeepWalk 2014年的模型，主要思想是在物品组成的图结构上进行随机游走（Random Walk），产生大量物品序列，然后将这些物品序列作为训练样本输入Word2Vec进行训练，得到物品的Embedding。
DeepWalk算法流程：
2. LINE Large-scale Information Network Embedding
动机 这篇论文的初衷是解决两个问题：
大规模图节点的表示学习 有向、有权图的节点表示学习 无监督的，不使用神经网络架构的Graph Embedding技术。LINE 的方法是基于直接优化节点嵌入向量，通过显式定义的一阶和二阶相似性损失函数进行优化，而不涉及神经网络的多层结构或非线性变换。
3. Node2Vec 4. EGES 推荐系统中EMBEDDING的应用实践 - 卢明冬的博客 (lumingdong.cn)</description>
    </item>
    
    <item>
      <title>SDGCL</title>
      <link>https://welldonesanmu.github.io/posts/sdgcl/</link>
      <pubDate>Sat, 11 May 2024 17:59:03 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/sdgcl/</guid>
      <description>SDGCL 核心思想和主要贡献是提出了一个新颖的自监督学习框架——逐步扩散图对比学习（Stepwise Diffusion Graph Contrastive Learning, SDGCL），用于改进图上的节点分类性能。这个框架主要通过一个逐步扩散 (stepwise diffusion) 的过程生成多个增强视图，这一过程利用了单个神经网络的输出，从而能更有效地捕捉节点之间的复杂交互关系，同时减少了内存和计算负担 (使用一个参数矩阵这和堆叠多层的GraphSAGE等GNN模型相比，减少了n-1个W)。此外，这个框架还设计了一个逐步对比损失（stepwise contrastive loss），通过整合所有扩散视图中正负样本的比较，进一步提高了节点嵌入的区分度。
对比视图的Intuition Graph通常都是基于图同质性假设，但是异构图balabala。所以我们在一次call / forward中加入stepwise diffusion（本质是change A） 之后生成的中间embedding append并将这些中间层作为负样本，这样只需single layer就可以完成K-hop的长距离信息捕获（消融实验）。
Stepwise Contrastive Loss 设计 利用逐步扩散过程，从单一神经网络输出生成多个增强视图。对于每一个节点，其在不同视图中的表示构成正样本对，而与其他节点在相同或不同视图中的表示构成负样本对。对每一对节点嵌入（来自同一节点但位于不同扩散视图中），计算它们之间的相似度，并与其与其他节点（负样本）的相似度进行比较。
在讨论“逐步对比损失（Stepwise Contrastive Loss）”之前，我们先理解它所依据的基本原理：对比学习。对比学习主要是通过最大化相似（正）样本对之间的一致性，并最小化不相似（负）样本对之间的一致性来训练模型，以此学习数据的有效表示。
以下是具体实现：
与InfoNCE Loss和NT-Xent的区别 各部分Loss加权归一化后求和
Pivot view选择第一个视图，对于每一个视图来说（diffusion之后的embedding），把S步内的所有node也当做正样本（Future Work）！
两篇的区别？ 第一篇发现了这个特点之后，做了简单的实验发现结果也不错，使用多层堆叠的形式来生成多个对比视图（one layer one view）（自适应dropout，long range聚合进来的特征比例要小）
Graph Diffusion技术是怎么实现的？ Graph Diffusion 模拟信息在图中的传播来捕获节点之间的长距离关系。</description>
    </item>
    
    <item>
      <title>GraphSAGE</title>
      <link>https://welldonesanmu.github.io/posts/graphsage/</link>
      <pubDate>Sat, 11 May 2024 15:18:34 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/graphsage/</guid>
      <description>GraphSAGE Inductive Representation Learning on Large Graphs
两点强调：
Inductive Large Graphs 首先明确一个概念：在推荐系统中Graph embedding采用直推式或称推导式（transductive）和归纳式（Inductive）两种，区别就是对于一个新加入的节点，模型是否能够在不重新学习之前预训练好的embedding的情况下，对这个新加入节点的特征进行嵌入表征。
这个概念比较老，面试时不要被迷惑。
直推式 基于矩阵分解、DeepWalk、Node2Vec 归纳式 GraphSAGE、 GCN、 GAT、 GCLs 直推式这类方法通常包括直接优化节点嵌入的方法，如DeepWalk、Node2Vec等。这些方法在一个特定的图上学习节点的嵌入向量，并且假设训练时使用的所有节点在预测时都是可见的。这些方法的共同点是它们通常需要对图中所有节点进行处理，并且一次学习所有节点的嵌入。因此，它们通常不具备归纳能力，即难以直接应对图中新增节点的情况，除非重新进行嵌入的学习过程。 对于GCN是哪种范式一下就明了了，GCN通过学习一个卷积过程，其中节点的特征信息是通过其邻居的特征进行聚合更新的。这个过程允许GCN处理未见过的节点，所以是归纳式。但GCN的归纳能力取决于它的应用方式和具体的训练设置。例如，如果GCN在一个特定的图结构上训练，并且仅用于相同结构的图，则其行为更倾向于转导式学习。但如果其训练过程涵盖了多种图结构，并且模型设计上允许它适应不同的图，那么它就显示出强大的归纳能力。（多图案例是蛋白质结构预测） GCLs通常在无监督或自监督的设置下用于学习图或节点的表示。这种方法通过最大化正样本对的相似性和最小化负样本对的相似性来学习有效的嵌入。图对比学习的关键优势在于它不依赖于标签数据，而是通过数据本身的结构和内容来学习表示。这个也是归纳式的。 GraphSAGE（Graph Sample and Aggregation）是一种归纳式图神经网络模型，它设计用来生成节点的嵌入，即使是在训练过程中未见过的节点（这一点很好保证：1训练时隐藏节点2邻居采样）。这种归纳能力源于其独特的聚合机制，该机制能够从节点的局部邻域信息中学习如何有效地生成嵌入。
算法流程：
GraphSAGE实现归纳式学习的关键方面：
1. 局部邻域聚合 GraphSAGE的核心思想是使用聚合函数来合成一个节点的邻居信息，形成该节点的嵌入。这意味着生成节点嵌入不依赖于整个图的结构，而是依赖于每个节点的局部邻域。具体聚合函数可以是简单的均值聚合、池化聚合或LSTM聚合等。通过这种方式，模型可以灵活地应对图中的新节点，因为它只需要新节点的局部信息就能计算其嵌入。
2. 邻居采样 由于实际图往往非常大，直接使用所有邻居的信息进行聚合计算是不现实的。GraphSAGE引入了邻居采样策略，即从每个节点的邻居中随机选择一个固定大小的子集，然后只使用这些采样邻居的信息来进行聚合。这不仅减少了计算负担，而且通过随机性增加了模型的泛化能力。
3. 多层聚合 GraphSAGE通常使用多层聚合结构，每层都对应一个聚合步骤。每个节点在每一层聚合其邻居的信息，然后将聚合结果传递到下一层。这样，更高层的聚合可以间接包含更远邻居的信息。这种分层的聚合方式允许模型捕获从近邻到远邻的结构信息。
TensorFlow实现：
#!/usr/bin/env python # -*- coding:utf-8 -*- &amp;#34;&amp;#34;&amp;#34; 参考文献: [1] Hamilton W, Ying Z, Leskovec J. Inductive representation learning on large graphs[C]//Advances in Neural Information Processing Systems. 2017: 1024-1034. (https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf) &amp;#34;&amp;#34;&amp;#34; import numpy as np import tensorflow as tf from tensorflow.</description>
    </item>
    
    <item>
      <title>字节一面</title>
      <link>https://welldonesanmu.github.io/posts/%E5%AD%97%E8%8A%82%E4%B8%80%E9%9D%A2/</link>
      <pubDate>Fri, 10 May 2024 17:48:28 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/%E5%AD%97%E8%8A%82%E4%B8%80%E9%9D%A2/</guid>
      <description>论文相关
详细实现（需要重新整理一版） 如果Graph Diffusion技术里 θ_k相当于是随机游走的概率因子的话，那和GraphSAGE有什么区别？ Loss是怎么实现的？和NCELoss的区别？ 两篇文章的区别是什么？ 对比视图的anchor选取？正负样本生成数量比例？ 对比学习常用的Loss是什么？ 使用Transformer去抽取图表征是否可行？ 了解GraphTransformer吗？ 推荐系统以及项目相关
推荐系统的架构实现是什么？ 召回层常用的什么算法？ Graph embedding怎么用在推荐上面的？ 在各个阶段，图神经网络怎么应用？ 召回层除了局部敏感哈希外还有什么方法加速召回？ AUC曲线的物理意义？AUC越高越好那么超过多少算是越高？ 说一下DIN的结构？ 项目中提到的会在排序层加入组合类别的神经网络，具体讲一下？ 手撕
使用rand5实现rand7。 给出一个数组，求出其中的逆序对个数；输出个数，数组中数字小于100000。 反问
部门实现推荐系统需要哪些技术栈？答：自研系统，服务端使用C++，模型搭建使用TensorFlow。 模型怎么上线耦合？答：算法工程师不管这个，有部门专门负责。 无Hadoop、Spark，all in算法实现
项目称之为练手
自我评价：C
Timeline：5.7&amp;ndash;5.10（等挂）
更新Timeline：5.11（挂同时被捞）</description>
    </item>
    
    <item>
      <title>English writing</title>
      <link>https://welldonesanmu.github.io/posts/english_writing/</link>
      <pubDate>Wed, 12 Apr 2023 15:02:38 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/english_writing/</guid>
      <description>Notes Move 1 Establishing a research territory Move 2 Establishing a niche Move 3 Presenting the present work </description>
    </item>
    
    <item>
      <title>2023.4.11</title>
      <link>https://welldonesanmu.github.io/posts/2023.4.11/</link>
      <pubDate>Tue, 11 Apr 2023 23:46:38 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/2023.4.11/</guid>
      <description>是你吗？ 今天给我的感觉已经像是触摸在云层中的样子了，可能是可能不是，可能触碰可能没有触碰，可能只剩下了一种阳光下开心微笑的剪影，也可能是Creep在心中回荡。
是真实的吗？
是虚假的吗？
现在愈发能感觉到Eason病态三部曲的滋味，WEIRDO WEIRDO WEIRDO。
很多东西都是纸里包着的一层，无论是身旁的人还是周围的人。
所以
是你吗？
还是自己吗？
4-11于深夜。</description>
    </item>
    
    <item>
      <title>Git</title>
      <link>https://welldonesanmu.github.io/posts/git/</link>
      <pubDate>Mon, 10 Apr 2023 14:36:09 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/git/</guid>
      <description>Git 推送的用法及常见指令 echo &amp;ldquo;# Blog&amp;rdquo; &amp;raquo; README.md git init git add README.md git commit -m &amp;ldquo;first commit&amp;rdquo; git branch -M main git remote add origin git@github.com:welldonesanmu/Blog.git git push -u origin main
.md 文件换行：在每一行的末尾加上 &amp;ldquo;&amp;quot; ,然后再回车键。
新增了一个脚本，用于每次添加完之后自动deploy.</description>
    </item>
    
    <item>
      <title>2023.4.9</title>
      <link>https://welldonesanmu.github.io/posts/2023.4.9/</link>
      <pubDate>Sun, 09 Apr 2023 23:21:57 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/2023.4.9/</guid>
      <description>认识她的第9天 今天晚上看了电影，但是没有发给她，只在朋友圈发了一条复旦新闻馆动态，像预想的一样，没有收到任何评论和点赞。我深深呼吸了一口气，都是好事啦。</description>
    </item>
    
  </channel>
</rss>
