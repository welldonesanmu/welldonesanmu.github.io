<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DL on Sanmu</title>
    <link>https://welldonesanmu.github.io/tags/dl/</link>
    <description>Recent content in DL on Sanmu</description>
    <image>
      <title>Sanmu</title>
      <url>https://welldonesanmu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://welldonesanmu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 19 May 2024 09:06:49 +0800</lastBuildDate><atom:link href="https://welldonesanmu.github.io/tags/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DL</title>
      <link>https://welldonesanmu.github.io/posts/dl/</link>
      <pubDate>Sun, 19 May 2024 09:06:49 +0800</pubDate>
      
      <guid>https://welldonesanmu.github.io/posts/dl/</guid>
      <description>神经网络 DNN/MLP/ANN 全连接神经网络/多层感知器
DNN是从多层感知器发展而来的第三代神经网络，DNN的网络结构分为三部分：输入层、隐藏层、输出层。
DNN的层与层之间是全连接的，第i层的任意一个神经元一定与第i+1层的任意一个神经元连接.
https://www.cnblogs.com/init0ne/p/16630311.html
KNN KNN（K-Nearest Neighbors，K近邻算法）是一种常见的无参数（non-parametric）分类和回归算法，常用于基于实例的学习（instance-based learning）。
对于分类问题，KNN算法会查找与该未知样本最近的K个样本点，然后将这K个样本点中出现最多的类别作为该未知样本的预测类别。对于回归问题，KNN算法会查找与该未知样本最近的K个样本点，并将这K个样本点的输出值的平均值作为该未知样本的预测输出值。
KNN算法中，距离度量是非常重要的一环。通常使用欧氏距离（Euclidean distance）或曼哈顿距离（Manhattan distance）等常见的距离度量方法。另外，KNN算法还需要确定K值的大小，通常通过交叉验证（cross-validation）等技术进行调整。
KNN算法的优点是简单易懂、易于实现、对于大规模数据集也可以高效处理。缺点是需要对每个未知样本进行计算距离和排序，当数据集规模很大时，计算成本会非常高，同时对于高维度的数据集，样本之间的距离可能会失去准确性，从而影响KNN算法的表现。
RNN RNN是一种神经网络结构，它通过在不同时间步之间共享权重来处理序列数据。与其他神经网络结构不同的是，RNN 具有记忆能力，可以在处理当前数据时利用之前的信息。它们的优点在于可以接受任意长度的输入序列，并且可以利用序列中之前的信息来帮助预测下一个值。
RNN 的一个基本单元是循环单元（recurrent unit），也称为记忆单元（memory cell），可以是简单的结构（如 Vanilla RNN）或复杂的结构（如 LSTM 和 GRU）。这些循环单元接受输入和先前的隐藏状态，产生一个新的隐藏状态，并输出一个值，该值可以被用于下一时间步的计算。通过不断地将先前的隐藏状态传递给下一时间步，RNN 可以利用历史信息来影响当前的输出。
RNN 的训练可以通过反向传播算法进行，这可以通过计算当前时间步的误差来更新网络的参数，以改善其性能。但是，RNN 的训练存在梯度消失或梯度爆炸等问题，这可能导致在训练过程中难以传递长期依赖的信息。为了解决这些问题，出现了一些改进的 RNN 结构，如 LSTM（长短时记忆网络）和 GRU（门控循环单元）。这些结构可以更好地处理长期依赖和短期信息。
优缺点 优点：
RNN 具有记忆能力，可以利用序列数据之前的信息来预测下一个值，从而更好地处理序列数据。 RNN 可以接受任意长度的输入序列，并且可以在每个时间步骤使用相同的参数，从而可以处理变长的序列数据。 RNN 的参数可以通过反向传播算法来训练，这使得其可以学习到数据之间的关系，从而可以在自然语言处理、语音识别等领域中得到广泛应用。 缺点：
RNN 计算的时间复杂度与序列长度成正比，处理长序列时可能会面临较大的计算开销和内存消耗。 RNN 的训练可能会受到梯度消失或梯度爆炸等问题的影响，尤其是在处理长序列数据时，这些问题可能会导致模型难以捕捉到序列中的长期依赖关系。 RNN 的记忆能力有限，无法捕捉到非常长的历史信息，这可能会影响模型的预测性能。 RNN为什么会梯度消失或爆炸 RNN（循环神经网络）在处理长序列时，存在梯度消失或梯度爆炸的问题，这是由于其计算方式和训练过程的特性所导致的。
具体来说，RNN 的计算方式是通过重复应用相同的权重来处理序列数据，即在每个时间步使用相同的权重矩阵进行计算，这种计算方式会导致随着序列长度的增加，信息在网络中传递时会经过多次的矩阵乘法，从而使得梯度在反向传播过程中会不断地被乘以相同的权重，从而可能会出现指数级的增长或衰减，这就是梯度爆炸或梯度消失的问题。
RNN的梯度消失不是真正的梯度消失，当下的梯度总会在，只是前面的梯度没了。RNN的梯度消失指的是当下梯度用不到前面的梯度了。
当梯度爆炸发生时，梯度的值变得非常大，导致网络参数更新过快，使得网络的性能变得不稳定。当梯度消失发生时，梯度的值变得非常小，导致网络参数更新过慢，使得网络无法学习到长期依赖的信息，从而影响了网络的预测性能。
为了解决这些问题，一些改进的 RNN 结构被提出，如 LSTM（长短时记忆网络）和 GRU（门控循环单元），它们使用了一些特殊的门机制来控制信息的流动和更新，从而更好地处理了长序列数据，并有效地缓解了梯度消失或梯度爆炸的问题。
LSTM LSTM（长短时记忆网络）是一种特殊类型的循环神经网络（RNN），它在处理序列数据时能够有效地捕捉长期依赖关系，通过使用一组称为“门”的结构来控制信息的流动，包括输入门、遗忘门和输出门。这些门可以在不同的时间步骤决定从当前输入和前一个时间步骤的输出中选择哪些信息需要被传递和保留。此外，LSTM还包括一个称为细胞状态的重要部分，可以在时间步骤之间存储和传递信息。
LSTM的优点在于它可以避免梯度消失问题，而且能够处理长期依赖关系，即使序列长度很长。此外，LSTM还具有较好的鲁棒性和泛化能力，可以适应不同的任务和数据集。
三个门的作用 1)遗忘门ft控制上一个时刻的内部状态ct-1需要遗忘多少信息。
2)输入门it控制当前的候选状态有多少信息需要保存。
3)输出门ot控制当前时刻的内部状态ct有多少信息需要输出给外部状态ht
激活函数 LSTM（长短时记忆）是一种递归神经网络，由输入门、遗忘门、输出门和一个单元状态组成。每个门都有一个对应的激活函数。以下是LSTM中使用的激活函数：</description>
    </item>
    
  </channel>
</rss>
