---
title: "机器学习八股"
date: 2024-05-16T21:24:29+08:00
lastmod: 2024-05-16T21:24:29+08:00 
draft: false
author: ["Sanmu"] 
comments: true 
tags:
  - ML            
---

# 特征工程

## 为什么要对数值类型的特征做归一化？

1. **加快收敛速度**：对于一些优化算法（例如梯度下降法），归一化可以加快算法的收敛速度。如果特征的值范围差异很大，那么在计算梯度时，梯度的幅度也会差异很大，可能导致收敛速度变慢或者不稳定。归一化后，特征值被压缩到相同的范围内，使得梯度更加均衡，从而加快收敛速度。
2. **提高模型训练的效率和稳定性**：许多机器学习算法在训练过程中都会计算特征之间的距离或者权重。特征值范围较大的数值会对模型的训练产生较大的影响，导致模型可能更加关注这些特征而忽略其他特征。归一化可以使所有特征的数值范围相近，从而使得模型的训练更加平衡和稳定。
3. **提高特征之间的可比性**：特征归一化可以使不同特征之间的数值范围相同，便于对不同特征进行比较和分析。例如，在主成分分析（PCA）中，归一化可以确保每个特征对主成分的贡献是基于其方差，而不是其数值大小。

常见的归一化方法包括最小-最大归一化（Min-Max Normalization）和标准化（Standardization）。最小-最大归一化将特征值缩放到[0,1]范围内，而标准化则将特征值转换为均值为0、标准差为1的标准正态分布。

---



# 经典算法

## SVM的原理？

支持向量机 SVM 的基本原理是通过找到一个最优的超平面，将不同类别的数据点分开。它主要关注以下几个方面：

1. **最大化分类间隔**：SVM在寻找超平面时，不仅仅是简单地将数据分开，而是寻找能够最大化分类间隔的超平面。间隔是从超平面到最近数据点（支持向量）的距离，最大化间隔有助于提高模型的泛化能力。

2. **支持向量**：这些是离超平面最近的点，对超平面的位置有决定性影响。优化问题的核心是这些支持向量。

3. **软间隔和松弛变量**：在处理现实中的数据时，完全线性可分的情况很少见。SVM引入软间隔和松弛变量，允许一些数据点被错误分类，以提高模型的鲁棒性和泛化能力。

4. **核技巧**：对于非线性可分的数据，SVM通过核函数将数据映射到更高维的空间，使得在高维空间中数据变得线性可分。常用的核函数包括线性核、多项式核和径向基函数（RBF）核。

5. **优化问题**：SVM通过求解一个凸优化问题来找到最优超平面，这个问题可以表示为：

$$
\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{n} \xi_i
$$

其中，$ w $ 是超平面的法向量，$b$  是偏置项，$\xi_i$ 是松弛变量， $C$  是正则化参数。

6. **决策函数**：训练完成后，SVM的决策函数为：

$$
f(x) = \text{sign} \left( \sum_{i=1}^{n} \alpha_i y_i K(x_i, x) + b \right) \\
$$

其中，$\alpha_i $是拉格朗日乘子，$y_i$ 是训练样本的标签，$x_i$ 是支持向量，$K(x_i, x) $是核函数。

通过以上步骤，SVM能够有效地处理线性和非线性数据，找到最佳的分类边界，并具有良好的泛化能力。

**（总结：SVM 的基本原理是找到一个最优的超平面，将不同的数据分开，使得不同类别的数据点都在不同的一侧。具体来说，这个超平面需要满足所有样本中距离超平面最近的样本点到超平面的距离最大，这些最近的样本点也被称为支持向量。这个问题可以通过凸优化的方法求解，它本质是一个凸二次规划问题。）**

---



## 为什么SVM对异常值不太敏感？

### 1. **最大化间隔的目标**

SVM的核心目标是找到一个最大化分类间隔的超平面。即使在数据集中存在异常值，SVM仍会尽量最大化正常数据点与超平面之间的间隔。这意味着异常值对超平面的影响相对较小，因为模型更关注间隔的最大化，而不是单个数据点的位置。在SVM中，只有离超平面最近的那些数据点（即支持向量）决定了超平面的最终位置。异常值通常不会成为支持向量，因为它们远离主流数据分布。由于支持向量对超平面的确定起主要作用，因此异常值对超平面的影响较小。

### 2. **核技巧的鲁棒性**

SVM通过核技巧将数据映射到高维空间，从而在高维空间中实现线性可分。异常值在高维空间中的影响相对低维空间来说可能被进一步削弱，因为核技巧在某种程度上能平滑和分散异常值的影响。

---

### 什么是VAE模型？

VAE，变分自编码器，是一种生成模型，也是自编码器（Autoencoder）的一种扩展。它使用了概率统计方法来生成数据，从而可以在一定程度上避免传统自编码器过拟合的问题。VAE可以看作是由两个部分组成的网络，一个是编码器（Encoder），用来将输入的数据转换为潜在空间（Latent Space）中的概率分布，另一个是解码器（Decoder），用来将潜在空间中的随机变量采样成原始数据。

具体来说，VAE模型通过最大化对数似然函数来训练模型，这个对数似然函数由两个部分组成：重构误差和KL散度。其中，重构误差是指模型重构数据与原始数据的误差，而KL散度则是用来约束潜在空间的概率分布与标准正态分布之间的差异。

VAE模型可以用于生成各种类型的数据，包括图像、音频、文本等。它在图像生成、图像去噪、数据压缩等领域都有广泛应用。

---

## EM算法

EM算法（Expectation-Maximization algorithm）是一种用于在含有隐变量（latent variable）的概率模型中，对参数进行极大似然估计（maximum likelihood estimation, MLE）的一种迭代算法。EM算法在很多机器学习和数据挖掘问题中都有广泛的应用，如聚类、降维、半监督学习等。

EM算法由两个步骤交替进行：E步（Expectation step）和M步（Maximization step）。

在E步中，算法首先对参数进行估计，然后计算出给定数据样本的隐变量的后验概率（posterior probability）。这里，隐变量指的是概率模型中没有被观测到的变量。通过计算后验概率，算法能够更好地理解数据样本的特征，并将其用于下一步的迭代。

在M步中，算法通过最大化似然函数（likelihood function）来更新参数。由于E步已经计算出了隐变量的后验概率，所以在M步中，算法可以直接用这些后验概率来更新模型的参数。更新后的参数用于下一轮的E步迭代。

通过交替执行E步和M步，EM算法能够逐步优化模型的参数，直到达到收敛条件为止。具体来说，收敛条件通常是指连续多轮迭代后，参数的变化量小于预定的阈值。

需要注意的是，**EM算法不保证找到全局最优解，而只是保证找到一组局部最优解。**为了避免陷入局部最优解，可以多次运行算法，每次使用不同的随机初始化条件。

---

## LR逻辑回归

### 1. 什么是LR

逻辑回归是一种用于预测二元分类结果（如成功/失败，是/否，正/负）的算法。它通过一个逻辑函数（sigmoid函数）将线性回归的输出映射到一个（0,1）之间的概率值，从而进行分类。

逻辑回归使用逻辑函数（sigmoid函数）将线性回归的输出转换为概率值。逻辑函数的公式为：
$$
sigma(z) = \frac{1}{1 + e^{-z}}
$$
其中，$z = w^T x + b$， $w$  是权重向量, $x$ 是输入特征向量， $b$ 是偏置项。

通过设置一个阈值（通常为0.5），逻辑回归将概率值转换为具体的类别标签。即，如果 $ P(y=1|x) > 0.5 $，则预测 $y=1 $；否则预测 $ y=0 $。

逻辑回归使用对数损失函数（Log Loss）来衡量模型预测与真实标签之间的差异：

$$
L(y, \hat{y}) = -\left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right]
$$
其中，$ y $ 是真实标签，$ \hat{y} $ 是预测的概率值。

对于多分类问题，逻辑回归可以通过一对一（one-vs-one）或一对多（one-vs-rest）的方法进行扩展，也可以使用多项逻辑回归（softmax回归）。

### 总结
逻辑回归是一种用于分类问题的简单而有效的模型，通过使用逻辑函数将线性回归的输出转换为概率值，再通过设定阈值进行分类。它具有计算效率高、易于解释和实现等优点，广泛应用于各种二分类和多分类任务中。

### 2. 逻辑回归对单条样本的预估分数代表真实概率吗?

1. **y服从Bernoulli分布**（只有分类问题0,1值，经常会选择该假设） 

2. **η 与 x 满足线性关系**

如果你的情况满足本文所说的两个假设，那么你训练模型的过程，就确实是在对概率进行建模。这两个假设并不是那么容易满足的。所以，很多情况下，**我们得出的逻辑回归输出值，无法当作真实的概率，只能作为置信度来使用**

---

### 为什么用sigmoid函数 ?

sigmoid函数的值域在[0,1]之间，这与LR模型的输出概率值范围相匹配，因此可以用于将模型的输出转换为概率值；是单调递增的，确保模型的输出是与输入呈正相关；可微的，能够进行梯度更新。还有一种最大熵解释，最大熵原理认为，学习概率模型时，熵最大的模型是最好的模型，LR假设了 P(y|x) 服从伯努利分布，根据伯努利分布的性质，再利用最大熵学习的过程，可以推导出系数函数就是sigmoid函数。

### 交叉熵推导

交叉熵损失函数实际上等价于负的对数似然函数。在最大似然估计中，我们希望最大化给定参数下数据的似然函数：

$$
 \max \prod_{i=1}^{N} P(y^{(i)} | x^{(i)}) 
$$
取对数并取负号后，我们得到：

$$
\min -\sum_{i=1}^{N} \log P(y^{(i)} | x^{(i)}) 
$$
对于二分类问题，概率 $ P(y|x) $ 可以表示为：

$$
 P(y|x) = \hat{y}^y (1 - \hat{y})^{(1 - y)} 
$$
取对数并取负号后，我们得到：

$$
 -\log P(y|x) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})] 
$$
对于多分类问题，概率 $P(y|x) $ 表示为：

$$
 P(y|x) = \prod_{i=1}^{C} \hat{y}_i^{y_i} 
$$
取对数并取负号后，我们得到：

$$
-\log P(y|x) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
$$

### 总结

交叉熵损失函数衡量了模型预测的概率分布与真实分布之间的差异，其推导过程体现了概率统计中的最大似然估计原理。对于二分类问题，交叉熵损失考虑了预测概率的对数值；对于多分类问题，交叉熵损失考虑了独热向量对应的预测概率的对数值。

---

### 为什么二分类 (LR) 不用 MSE 损失函数？

从数据分布上，使用 MSE 损失函数的背景假设是数据误差遵循高斯分布。实际上，可以分为两类（即二分类）的数据集是遵循伯努利分布。

其次，二（多）分类我们要用sigmoid或者softmax激活，MSE加上这俩是非凸的。简而言之，如果使用 MSE 损失函数训练二分类模型，则不能保证将损失函数最小化。这是因为 MSE 函数期望实数输入在范围 中，而二分类模型通过 Sigmoid 函数输出范围为 的概率。

当将一个无界的值传递给 MSE 函数时，在目标值 处有一个明确最小值的情况下，会形成一条漂亮的 U 形（凸）曲线。另一方面，当将来自 Sigmoid 等函数的有界值传递给 MSE 函数时，可能会导致结果并不是凸的。

一侧是凹的，而另一侧是凸的，没有明确的最小值点。因此，**如果在初始化二分类神经网络权重时，权值万一设置得很大，使其落在 MSE 凹的那一侧（如下图红色标记的点），由于梯度几乎为0** ，损失函数梯度下降法将不起作用，因此网络权重可能得不到更新或改善得非常缓慢。这也是**训练时应采用小值来初始化神经网络的原因之一。**



在回归模型MSE 和L2正则经常一起使用，带有 L2 正则化的损失函数通常被称为岭回归（Ridge Regression），其公式为：

$$
\text{Loss} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} w_j^2
$$
这个公式包含了 MSE 项和 L2 正则化项。通过加入正则化项，模型不仅关注预测误差（MSE），还关注参数的大小，从而减少过拟合风险。



[AI 面试高频问题: 为什么二分类不用 MSE 损失函数？ - mathinside的个人空间 - OSCHINA - 中文开源技术交流社区](https://my.oschina.net/mathinside/blog/4537752)

---

### **可以进行多分类吗？**

可以的，其实我们可以从二分类问题过度到多分类问题(one vs rest)，思路步骤如下：

1.将类型class1看作正样本，其他类型全部看作负样本，然后我们就可以得到样本标记类型为该类型的概率p1。

2.然后再将另外类型class2看作正样本，其他类型全部看作负样本，同理得到p2。

3.以此循环，我们可以得到该待预测样本的标记类型分别为类型class i时的概率$p_i$，最后我们取$p_i$中最大的那个概率对应的样本标记类型作为我们的待预测样本类型。

---

### **逻辑回归常用的优化方法有哪些**

 **一阶方法**

梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。

  **二阶方法：牛顿法、拟牛顿法：**

这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要**求解凸优化问题**，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。

缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。

拟牛顿法： 不用二阶偏导而是构造出Hessian（黑塞）矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。

---

###  **逻辑斯特回归为什么要对特征进行离散化。**

1. 非线性！非线性！非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；
2. 速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
3. 鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
4. 方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
5. 稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
6. 简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

---

### **逻辑回归的目标函数中增大L1正则化会是什么结果。**

所有的参数w都会变成0。

---

### **MAE（平均绝对值误差 l1损失）**

均值绝对误差（Mean Absolute Error, MAE）通常与 L1 正则化（L1 Regularization）一起使用。这是因为它们在数学性质和效果上有一些共同的特性。

### 1. **均值绝对误差 (MAE)**
均值绝对误差是一种损失函数，衡量预测值与真实值之间的平均绝对差。其公式为：

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$
MAE 对异常值（outliers）不敏感，因为它使用绝对差而不是平方差。

### 2. **L1 正则化 (L1 Regularization)**

L1 正则化是一种用于防止模型过拟合的技术，它通过在损失函数中加入权重参数的绝对值和惩罚项来限制模型参数的大小。其公式为：

$$
\text{L1 Regularization Term} = \lambda \sum_{j=1}^{p} |w_j|
$$
L1 正则化的一个显著特性是它可以导致一些参数精确地等于零，从而实现特征选择（feature selection）。

在回归模型中，使用 MAE 作为损失函数并结合 L1 正则化项，可以形成一个鲁棒的回归模型，既关注预测误差又防止过拟合，并且能够进行特征选择。其损失函数通常表示为：

$$
 \text{Loss} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}i| + \lambda \sum_{j=1}^{p} |w_j|
$$
这种组合通常被称为 Lasso 回归（Least Absolute Shrinkage and Selection Operator）。

Lasso 回归在高维数据中特别有效，因为它能够将不相关的特征的权重缩小到零，从而选择出最重要的特征。

均值绝对误差（MAE）通常与 L1 正则化一起使用，因为它们在数学性质上具有一致性，都涉及绝对值操作。MAE 提供了一种对异常值不敏感的损失度量，而 L1 正则化可以限制模型参数的大小并实现特征选择。这种组合在回归问题中非常有效，特别是在高维数据和需要特征选择的场景中。

---

## **集成学习**

### **boosting和bagging的区别**

- 样本选择：bagging是有放回的随机抽样，每个训练集之间是独立的；boosting是不改变训练集，只是调整样例的权重，根据上一轮的分类结果进行调整。
- 样例权重：bagging中每个样例的权重相等；boosting中每个样例的权重不同，错误分类的样例权重会增加，正确分类的样例权重会减小。
- 预测函数：bagging中每个基分类器的预测结果相互独立，最后通过平均或投票得到最终结果；boosting中每个基分类器都依赖于前一个基分类器，最后通过加权平均得到最终结果。
- 目标：bagging主要是为了减小方差，降低过拟合风险；boosting主要是为了减小偏差，提高模型精度。

**总结：bagging是有放回的随机抽样，每个训练集之间是独立的，每个样例的权重相等；boosting是不改变训练集，只是调整样例的权重，根据上一轮的分类结果错误分类的样例权重会增加，正确分类的样例权重会减小。bagging中每个基分类器的预测结果相互独立，最后通过平均或投票得到最终结果；boosting中每个基分类器都依赖于前一个基分类器，最后通过加权平均得到最终结果。从结构上来说，bagging是并行处理，boosting是串行处理，从作用上来看，bagging是减小方差，boosting是减小bias（偏差）。**

- 方差：是指有所有采样得到的大小为m的训练数据集，训练出的所有模型的输出的方差
- 偏差：指由有所采样得到的大小为m的训练数据集，训练出的所有模型的输出的平均值和真实模型输出之间的偏差。

---

### **决策树**

通过构建树状结构来进行决策。在决策树中，每个节点代表一个特征，每个分支代表这个特征的不同取值，叶子节点代表分类结果或者回归结果。

以下是一些决策树生成和剪枝的方法：

1. ID3算法：使用信息增益来选择最佳分裂特征，适用于分类问题。
2. C4.5算法：使用信息增益比来选择最佳分裂特征，可以处理连续特征和缺失值。
3. CART算法：既可以用于分类问题，也可以用于回归问题。它使用基尼系数来选择最佳分裂特征，可以处理连续特征和缺失值。
4. CHAID算法：适用于分类问题，使用卡方检验来选择最佳分裂特征。
5. 剪枝：决策树容易出现过拟合，因此需要进行剪枝来避免这种情况。剪枝分为预剪枝和后剪枝两种。预剪枝是在生成决策树时，对于每个节点都进行判断，如果判断为不必要的节点，则直接将该节点剪掉。后剪枝是在生成完整的决策树后，对于每个节点都进行判断，如果该节点剪掉后对决策树的准确率没有影响，则将该节点剪掉。

---

###  **随机森林**

RF 算法由很多决策树组成，每一棵决策树之间没有关联。建立完森林后，当有新样本进入时，每棵决策树都会分别进行判断，然后基于投票法给出分类结果。是 Bagging 的扩展变体。

优点：

- 可以处理高维数据，不需要降维或特征选择
- 可以评估各个特征的重要性
- 可以有效地处理缺失值和异常值
- 具有很好的抗过拟合能力和泛化能力
- 训练和预测速度快，易于并行化

缺点：

- 在某些噪音较大的分类或回归问题上会过拟合
- 对于取值划分较多的特征，会产生更大的影响，导致属性权值不可信
- 当决策树个数很多时，需要占用更多的空间和时间

---

###  **随即森林的特征是如何取得的**？

---

### **AdaBoost**

初始化使每个样本具有相同权重；训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

**优点：**

1. 分类精度高；
2. 可以用各种回归分类模型来构建弱学习器，非常灵活；
3. 不容易发生过拟合。

**缺点：**

1. 对异常点敏感，异常点会获得较高权重。

---

### **GBDT**

GBDT 的核心在于累加所有树的结果作为最终结果，所以 GBDT 中的树都是回归树，不是分类树，这一点相当重要。对于分类树而言，其值加减无意义（如性别），而对于回归树而言，其值加减才是有意义的（如说年龄）。回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。

梯度迭代：GBDT 的核心在于累加所有树的结果作为最终结果，GBDT 的每一棵树都是以之前树得到的残差来更新目标值，这样每一棵树的值加起来即为 GBDT 的预测值。

GBDT 采用了决策树作为基本的分类器，每一次迭代都是为了拟合残差（残差的意思就是:A的预测值+ A的残差=A的实际值）。在训练的过程中，**GBDT 通过梯度下降的方式来不断优化每一次训练的残差，然后将每次训练得到的决策树组合起来得到最终的预测结果。**

GBDT 的优点在于其能够处理高维稀疏数据，对异常值具有鲁棒性，并且可以自动进行特征选择。此外，GBDT 还可以应用于回归和分类问题，且在多种类型的数据集上都表现出很好的效果。

GBDT 的缺点在于其训练过程较为耗时，需要调节的参数较多，模型较为复杂，可能会导致过拟合。此外，GBDT 对于样本类别不平衡的情况，需要进行特殊处理，否则可能会导致模型预测结果的偏差。

---

### **XGBoost**

作为GBDT的高效实现，XGBoost是一个上限特别高的算法，因此在算法竞赛中比较受欢迎。简单来说，对比原算法GBDT，XGBoost主要从下面三个方面做了优化： 

- 一是算法本身的优化：在算法的弱学习器模型选择上，对比GBDT只支持决策树，还可以直接很多其他的弱学习器。在算法的损失函数上，除了本身的损失，还加上了正则化部分。在算法的优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而XGBoost损失函数对误差部分做二阶泰勒展开，更加准确。算法本身的优化是我们后面讨论的重点。 
- 二是算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择合适的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。
- 三是算法健壮性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。 

缺点：计算量大，内存开销大

1. 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
2. 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。

**防止过拟合：**

XGBoost在设计时，为了防止过拟合做了很多优化，具体如下：

- 目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化
- 列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可）
- 子采样：每轮计算可以不使用全部样本，使算法更加保守
- shrinkage: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间

---

### **降维算法（SVD和PCA）**

**奇异值分解**（Singular Value Decomposition，SVD）是一种在线性代数和数学中非常重要的降维算法，**可以将高维矩阵转换为低维矩阵**。SVD算法可以**将一个复杂的矩阵分解成三个矩阵的乘积**：$A=U\Sigma V^T$，其中$A$是原始矩阵，$U$是一个正交矩阵，$\Sigma$是一个对角矩阵，$V$也是一个正交矩阵。SVD算法的步骤如下：

1. 计算原始矩阵$A$的转置矩阵$A^T$和$AA^T$的乘积；
2. 对$AA^T$做特征值分解，得到特征值和对应的特征向量，将特征向量组成正交矩阵$V$；
3. 对$A^TA$做特征值分解，得到特征值和对应的特征向量，将特征向量组成正交矩阵$U$；
4. 根据$U$和$V$，可以计算出$\Sigma$。

S**VD算法的优点在于可以在不丢失太多信息的情况下将高维数据降维到低维**，从而便于数据的可视化和分析。SVD算法也被广泛应用于图像处理、推荐系统、文本挖掘等领域。

**主成分分析**PCA是一种常见的降维算法，**可以将高维数据降维到低维**，并且尽可能地保留原始数据的信息。**基本思想是寻找原始数据中最重要的方向，这些方向被称为主成分，并将数据投影到这些方向上，从而实现降维。**

---

### **聚类算法**

**Kmeans**

优点：简单快速；可处理大数据集，高效可伸缩，复杂度O(nkt)，经常以局部最优结束；尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，而簇与簇之间区别明显时，它的聚类效果很好

缺点：对 K 值敏感；对离群点和噪声点敏感初始聚类中心的选择；只能聚凸的数据集， 即聚类的形状一般只能是球状的，不能推广到任意的形状

**DBSCAN**

优点：自适应的聚类，不需要提前设定K值大小；对噪声不敏感；能发现任意形状的簇。；聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响

缺点：对两个参数的设置敏感，即圈的半径 eps 、阈值 MinPts；数据密度不均匀时，很难使用该算法；数据样本集越大，收敛时间越长。此时可以使用 KD 树优化

**凝聚式层次聚类**

优点：距离和规则的相似度容易定义，限制少，需要预先制定聚类数，可以发现类的层次关系

缺点：计算复杂度太高；奇异值也能产生很大影响；算法很可能聚类成链状

**谱聚类**

优点：当聚类的类别个数较小的时候，谱聚类的效果会很好，但是当聚类的类别个数较大的时候，则不建议使用谱聚类；谱聚类算法使用了降维的技术，所以更加适用于高维数据的聚类；谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。这点传统聚类算法（比如K-Means）很难做到；谱聚类算法建立在谱图理论基础上，与传统的聚类算法相比，它具有能在任意形状的样本空间上聚类且收敛于全局最优解

缺点：谱聚类对相似度图的改变和聚类参数的选择非常的敏感；谱聚类适用于均衡分类问题，即各簇之间点的个数相差不大，对于簇之间点个数相差悬殊的聚类问题，谱聚类则不适用。

---

###  **K-Means（手撕代码实现）**

优点：

1. 易于实现和解释：KMeans算法是一种简单而直观的聚类算法，易于实现和解释。
2. 可扩展性：KMeans算法可应用于大规模数据集，因为其时间复杂度为O(nkdi)，其中n是数据点的数量，k是簇的数量，d是特征的数量，i是迭代次数。
3. 速度快：由于KMeans算法的时间复杂度较低，因此在处理大规模数据集时，其速度通常比其他聚类算法快。
4. 结果可解释性强：KMeans算法将数据点分成k个簇，并计算每个簇的中心点。这使得结果易于解释，可以用于数据的可视化。

缺点：

1. 对初始质心的选择敏感：KMeans算法的结果取决于初始质心的选择。不同的初始质心可能会导致不同的聚类结果。
2. 对噪声和异常值敏感：KMeans算法假定数据点服从高斯分布，且簇具有相同的方差。如果簇具有不同的方差或数据集包含噪声或异常值，则结果可能会受到影响。
3. 需要预先确定聚类簇的数量：KMeans算法需要预先确定聚类簇的数量，但在实际应用中，这通常是未知的，因此需要使用其他算法来确定最佳的聚类簇数量。

python实现k-means算法的基本步骤是：

- 随机选择k个聚类中心（centroids）
- 计算每个数据点到k个聚类中心的距离，将数据点分配到距离最近的聚类中
- 计算每个聚类中数据点的均值，更新聚类中心
- 重复上述过程，直到聚类中心不再变化或达到最大迭代次数

python实现k-means算法的示例代码如下：

```Python
import numpy as np

class KMeans:
    def __init__(self, n_clusters, max_iter=300, random_state=None):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.random_state = random_state
    
    def fit(self, X):
        np.random.seed(self.random_state)
        
         初始化质心
        centroids = X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]
        
        for i in range(self.max_iter):
             计算每个样本到质心的距离
            distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))
            
             找到距离最近的质心
            labels = np.argmin(distances, axis=0)
            
             更新质心
            for j in range(self.n_clusters):
                centroids[j] = X[labels == j].mean(axis=0)
        
        self.labels_ = labels
        self.cluster_centers_ = centroids
        
        return self
```

使用实例:

```Python
from sklearn.datasets import make_blobs

 生成样本数据
X, y = make_blobs(n_samples=1000, centers=3, random_state=42)

 创建 KMeans 模型并训练
model = KMeans(n_clusters=3, random_state=42)
model.fit(X)

 打印聚类结果
print(model.labels_)
print(model.cluster_centers_)
```

---

### **手写Adam**

```Python
import numpy as np

class AdamOptimizer:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None
        self.v = None
        self.t = 0
    
    def optimize(self, gradients):
        if self.m is None:
            self.m = np.zeros_like(gradients)
            self.v = np.zeros_like(gradients)
        
        self.t += 1
        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients
        self.v = self.beta2 * self.v + (1 - self.beta2) * gradients ** 2
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)
        update = -self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)
        
        return update
```

使用示例：

```Python
 构建Adam优化器
optimizer = AdamOptimizer(learning_rate=0.001)

 计算梯度
gradients = np.array([0.1, 0.2, 0.3])

 更新权重
weights = weights + optimizer.optimize(gradients)
```

---

### **手写Transformer**

```Python
import torch
class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size, d_model):
        super(EmbeddingLayer, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.d_model = d_model
        
    def forward(self, x):
        return self.embedding(x) * torch.sqrt(self.d_model)
        
        
class PositionEncoding(nn.Module):
    def __init__(self, d_model, max_seq_len=100):
        super(PositionEncoding, self).__init__()
        self.d_model = d_model
        self.dropout = nn.Dropout(0.1)
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        x = x * torch.sqrt(self.d_model)
        x = x + self.pe[:x.size(0), :]
        x = self.dropout(x)
        return x


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_head):
        super(MultiHeadAttention, self).__init__()
        self.n_head = n_head
        self.d_k = d_model // n_head
        self.q_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(0.1)
        self.out = nn.Linear(d_model, d_model)
        
    def attention(self, q, k, v, d_k, mask=None, dropout=None):
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        scores = torch.nn.functional.softmax(scores, dim=-1)
        if dropout is not None:
            scores = dropout(scores)
        output = torch.matmul(scores, v)
        return output
    
    def forward(self, q, k, v, mask=None):
        bs = q.size(0)
        q = self.q_linear(q).view(bs, -1, self.n_head, self.d
```

### **手写Transformer encoder**

```Python
class Encoder(nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    self.src_emb = nn.Embedding(src_vocab_size, d_model)  定义生成一个矩阵，大小是src_vocab_size × d_model
    self.pos_emb = PositionalEncoding(d_model)  位置编码情况，这里是固定的正余弦函数，也可以使用类似的nn.Embedding获得一个可以更新学习的位置编码
    self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])  使用ModuleList对多个encoder进行堆叠，因为后续的encoder并没有使用词向量和位置编码，所以抽离出来
  
  实现函数
  def forward(self, enc_inputs):
     这里的enc_inputs形状是：[batch_size × source_len]（source_len是编码端输入句子的长度）

     下面这个代码是src_emb进行索引定位，enc_outputs输出形状是[batch_size,src_len,d_model]
    enc_outputs = self.src_emb(enc_inputs)

     这里就是位置编码，把两者相加放入到了这个函数里面，从这里可以去看一下位置编码函数的实现：3
    enc_outputs = self.pos_emb(enc_outputs.transpose(0,1)).transpose(0,1)

     get_attn_pad_mask是为了得到句子中pad的位置信息，给到模型后面，在计算自注意力和交互注意力的时候去掉pad符号的影响，这个函数的实现：4
    enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)
    enc_self_attns = []
    for layer in self.layers:
       看EncoderLayer函数：5
      enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)
      enc_self_attns.append(enc_self_attn)
    return enc_outputs, enc_self_attns
```

---

### **OOV怎么办**

在自然语言处理中，"out of vocabulary"（OoV）是指一个单词或短语**在词汇表（vocabulary）中不存在**，因此无法被模型识别或处理。解决这个问题的方法有以下几种：

1. **扩大词汇表**（vocabulary）：可以通过添加新单词或扩大语料库来增加词汇表的大小，以便覆盖更多的单词和短语。但这种方法可能会增加模型的复杂度，并且需要更多的数据。
2. 采用基于字符的表示法（character-based representation）：一些模型可以采用基于字符的表示法，而不是基于单词的表示法。这种方法将单词分解为字符，并将它们作为模型的输入。这样，即使某个单词不在词汇表中，模型也可以通过其组成的字符来学习单词的含义。
3. 采用未知词汇标记（unknown token）：一些模型将未知的单词或短语替换为特殊的标记，比如“UNK”或“<UNK>”，以便将其与其他单词或短语区分开来。这样可以保持模型的一致性，但是不能学习未知单词的含义。
4. 采用预训练模型（pre-trained models）：预训练模型通常使用大规模语料库来预先训练模型，以便模型可以理解语言中的大部分单词和短语。这种方法可以增强模型的泛化能力，并且通常可以识别更多的单词和短语。
5. 使用人工干预的方法：如果一个特定的单词或短语非常重要，但在词汇表中不存在，可以**手动添加该单词或短语到词汇表中**。这种方法需要人工干预，但可以提高模型的准确性。

**（总结：如果一个单词不在词表中，则按照subword的方式逐个拆分token，如果连逐个token都找不到，则直接分配为[unknown]）**

---

###  **Softmax和Sigmoid**

#### **softmax的loss**

Softmax 函数常用于多分类问题中，将模型的输出转换为类别概率分布。在使用 Softmax 函数时，通常会配合使用交叉熵损失函数（Cross-Entropy Loss）来衡量预测概率分布与实际标签分布之间的差异。下面是 Softmax 函数及其配合交叉熵损失函数的详细解释和推导过程。

### 1. **Softmax 函数**

Softmax 函数将一个包含 C 个类别的未归一化的预测值logits转换为一个概率分布。Softmax 函数的公式为：

$$
\sigma(z) = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}
$$
其中，$z$ 是模型的输出向量，$z_i$ 是第 $i$ 类的得分，$C$是类别数，$\sigma(z)_i  $是第 $i $ 类的预测概率。

### 2. **交叉熵损失函数**

交叉熵损失函数用于衡量预测概率分布与实际标签分布之间的差异。对于一个样本，其交叉熵损失定义为：

$$
L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i) 
$$
其中， $y_i$是实际的第 $i $ 类标签（one-hot 编码），$\hat{y}_i $是预测的第 $ i $ 类概率。

### 5. **示例代码**

以下是一个使用 Python 和 TensorFlow 实现 Softmax 和交叉熵损失函数的示例代码：

```python
import tensorflow as tf

# 假设 logits 是模型的输出，labels 是实际标签
logits = tf.constant([[2.0, 1.0, 0.1]])
labels = tf.constant([[1, 0, 0]])

# 计算 softmax 输出
softmax_output = tf.nn.softmax(logits)

# 计算交叉熵损失
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))

print("Softmax Output:", softmax_output.numpy())
print("Cross-Entropy Loss:", loss.numpy())
```

### 总结

在多分类问题中，Softmax 函数用于将模型的输出转换为概率分布，交叉熵损失函数则用于衡量预测的概率分布与实际标签分布之间的差异。二者结合在一起，可以有效地训练多分类模型。交叉熵损失的计算通过 Softmax 输出简化为对正确类别的预测概率取对数的负值，这使得计算更加高效。

---

#### **softmax的数值问题**

Softmax 函数在处理大数值或小数值时容易出现数值稳定性问题，导致数值溢出或下溢，从而影响计算的准确性和稳定性。为了解决这个问题，通常会进行数值稳定性处理。以下是 Softmax 函数数值问题的详细解释以及解决方案。

Softmax 函数的公式为：

$$
\sigma(z) = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}
$$
在这个公式中，如果 $z_i $ 的值很大，则 $e^{z_i} $ 会变得非常大，可能导致数值溢出（Overflow）。同样，如果 $z_i $ 的值很小，$e^{z_i} $ 会变得非常小，可能导致数值下溢（Underflow）。

#### 2. **解决方案：数值稳定的 Softmax**

为了避免这些数值问题，可以对 Softmax 函数进行数值稳定性处理，通常的方法是减去输入向量 $z $ 中的最大值。这种方法既简单又有效，不改变 Softmax 输出的相对比例。公式如下：

$$
\sigma(z) = \frac{e^{z_i - \max(z)}}{\sum_{j=1}^{C} e^{z_j - \max(z)}}
$$
其中，$\max(z) $ 是向量 $z $ 中的最大值。

通过减去最大值，可以有效避免指数运算中的数值溢出和下溢问题，因为所有 $z_i - \max(z) $ 的值都会在一个较小的范围内，通常在 0 和负数之间。

####  **总结**

Softmax 函数在处理大数值或小数值时容易出现数值稳定性问题，导致数值溢出或下溢。通过在计算 Softmax 之前减去输入向量中的最大值，可以有效避免这些问题。这种数值稳定的 Softmax 函数在实践中广泛应用，确保计算过程的准确性和稳定性。

---

####  **Sigmoid**

Sigmoid 函数是一种常用的激活函数，常用于二分类问题的输出层中。它将任意实数输入映射到 (0, 1) 的区间，表示概率。Sigmoid 函数的公式为：

$$
\sigma(x) = \frac{1}{1 + e^{-x}} 
$$


Sigmoid 函数的导数公式为：

$$
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x)) 
$$
其中，$ \sigma(x) $是 Sigmoid 函数的值。

---

###  **正则化**

#### **L1和L2的会有啥现象、解释原因**

L1和L2正则是机器学习中常用的两种正则化方法，用于防止过拟合和提高模型的泛化能力。它们的主要区别在于正则项的计算方式不同。

L1正则化（也称为Lasso正则化）的正则项为模型参数的绝对值之和，即L1范数。L1正则化会使一些参数变为0，从而实现特征选择的效果。因此，L1正则化在稀疏数据集上表现较好。

L2正则化（也称为岭回归）的正则项为模型参数的平方和，即L2范数。L2正则化对参数的影响是连续的，不会强制使参数为0，但可以降低参数值，从而降低模型的复杂度。L2正则化在处理线性相关的特征时表现较好。

总体来说，L1正则化可以用于特征选择和稀疏数据集，而L2正则化可以用于降低模型复杂度和处理线性相关的特征。在实际应用中，选择哪种正则化方法取决于数据集的特征和模型的表现。

**（总结：L1输出稀疏，它会把不重要的特征直接置零，是一个天然的特征选择器，这是因为L1是绝对值之和，图像是V，导数是小于0的部分为-1， 大于0是1，在梯度更新时，不管 L1 的大小是多少（只要不是0）梯度都是1或者-1，所以每次更新时，它都是稳步向0前进。而L2是平方和，图像是U，导数是/，它的梯度会越靠近0，就变得越小，所以在更新的过程中不容易变成0。这就是 L1 输出稀疏L2输出平滑的原因。L2 一定只有一条最好的预测线，L1 则可能存在多个最优解，但是L1 鲁棒性更强，对异常值更不敏感。）**

---

#### **分别代表什么先验**

L1 正则是拉普拉斯先验,L2 是高斯先验。

---

#### **bias要不要正则**

偏置bias不需要正则，在求导的时候b是直接约掉的，对于输入的改变是不敏感的，对bias进行正则化容易引起欠拟合。

---

###  **初始化**

**不同网络初始化的区别**

不同的神经网络初始化方法适用于不同的激活函数和网络结构，选择适合的初始化方法可以加速网络的训练过程，提高网络的性能。

随机初始化：在神经网络训练开始之前，将网络中的权重随机初始化为一些较小的值，这种方法是最常见的初始化方法。**随机初始化的优点是方便、简单**，可以避免权重在同一数值附近出现的情况，从而**避免了梯度消失或梯度爆炸等问题**。但是随机初始化也存在一些缺点，比如**无法确保网络的收敛性和稳定性。**

Xavier初始化：Xavier初始化是一种比较流行的初始化方法，它的思想是根据输入和输出的神经元数量来决定权重初始化的范围。具体来说，**Xavier初始化**将权重初始化为一个均值为0，方差为2/(输入神经元数量+输出神经元数量)的高斯分布。这种方法**适用于sigmoid激活函数**。

He初始化：He初始化是Xavier初始化的改进版本，适用于ReLU激活函数。He初始化将权重初始化为一个均值为0，方差为2/输入神经元数量的高斯分布。相对于Xavier初始化，**He初始化的权重范围更小，更适合ReLU激活函数的性质。**

---

#### **神经网络隐层可以全部初始化为0吗?**

不可以。如果神经网络的隐层权重全部初始化为0，那么梯度下降算法将不会起作用，因为每个神经元的输出和梯度都相同，无法进行有效的学习。

---

## 损失函数

### **用过哪些损失函数**

以下是一些常见的损失函数：

1. **均方误差（MSE）**：均方误差是**回归问题**中常用的损失函数，它的计算公式是将每个样本的预测值与真实值之差的平方求和后取平均数。

2. **交叉熵（CE）**：交叉熵是**分类问题**中常用的损失函数，它的计算公式包括二分类交叉熵和多分类交叉熵两种形式。

3. **KL散度**：KL散度是**衡量两个概率分布之间的差异**的一种度量方式，它在训练生成模型时经常使用。

4. **Hinge Loss**：Hinge损失函数是支持向量机（SVM）中使用的一种损失函数，它能够使得分类边界更加稳健。

5. **InfoNCE (Information Noise Contrastive Estimation) Loss** 是一种用于训练神经网络的损失函数，特别常用于对比学习 (Contrastive Learning) 中。它通过最大化相似样本对之间的相似度，同时最小化不同样本对之间的相似度，来学习有意义的表示。

   InfoNCE Loss 最初是由 Aaron van den Oord 等人在论文《Representation Learning with Contrastive Predictive Coding》中提出的，用于无监督学习的表示学习任务。

   ### InfoNCE Loss的公式

   设 $\mathbf{z}_i$ 和 $\mathbf{z}_j $是一对正样本的表示（例如，两个增强后的同一图像的表示），$\mathbf{z}_k $是负样本的表示。InfoNCE Loss 可以定义为：

   $$
   \mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau)}{\sum{k=1}^{N}  \exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau)}
   $$
   其中：
   - $\text{sim}(\mathbf{z}_a, \mathbf{z}_b) $是两个表示之间的相似度（通常使用点积或余弦相似度）。
   - $\tau$ 是温度参数（temperature parameter），用于控制分布的平滑程度。
   - $N$ 是包括正样本在内的总样本数。

   ### InfoNCE Loss的解释

   1. **相似度计算**：计算正样本对 $\mathbf{z}_i, \mathbf{z}_j $之间的相似度，以及所有样本对 $\mathbf{z}_i, \mathbf{z}_k$（包括正样本对和负样本对）之间的相似度。
   2. **归一化**：使用 softmax 函数对相似度进行归一化，使得相似度值可以解释为概率分布。
   3. **最大化正样本对的相似度**：通过最大化正样本对的相似度，同时最小化正样本对与所有负样本对的相似度比率，从而提高模型的区分能力。

   ### 应用场景

   InfoNCE Loss 被广泛应用于各种对比学习任务中 (图学习)，包括但不限于：
   - **无监督表示学习**：如 SimCLR 和 MoCo，这些方法通过增强的图像对来学习图像表示。
   - **自监督学习**：通过预训练模型来学习有意义的表示，之后在下游任务（如分类、检测等）中进行微调。
   - **自然语言处理**：用于学习句子或文本的表示，如在对比学习的文本嵌入模型中。

   ### 示例代码

   以下是使用 PyTorch 实现 InfoNCE Loss 的示例代码：

   ```python
   import torch
   import torch.nn.functional as F
   
   def info_nce_loss(anchor, positive, negatives, temperature=0.07):
       # 计算相似度
       positive_similarity = torch.matmul(anchor, positive.T) / temperature
       negative_similarity = torch.matmul(anchor, negatives.T) / temperature
       
       # 拼接正负样本相似度
       logits = torch.cat([positive_similarity, negative_similarity], dim=1)
       
       # 创建标签
       labels = torch.zeros(logits.shape[0], dtype=torch.long, device=logits.device)
       
       # 计算 InfoNCE Loss
       loss = F.cross_entropy(logits, labels)
       
       return loss
   
   # 示例使用
   batch_size = 32
   embedding_dim = 128
   
   # 随机生成样本
   anchor = torch.randn(batch_size, embedding_dim)
   positive = torch.randn(batch_size, embedding_dim)
   negatives = torch.randn(batch_size * 10, embedding_dim)  # 10个负样本
   
   loss = info_nce_loss(anchor, positive, negatives)
   print(loss)
   ```

   ---

   ### **为啥分类不用MSE**

   分类任务不用MSE损失的原因是：

   - MSE损失当与Sigmoid或Softmax搭配使用时，loss的偏导数的变化趋势和预测值及真实值之间的差值的变化趋势不一致。也就是说当预测值与真实值的差值变大的时候，其偏导数反而可能变小。
   - MSE损失是非凸的，存在很多局部极小值点。
   - MSE是计算预测值和真实值的欧式距离，回归任务才有欧式距离的说法，分类一般认为跟欧式距离没关，所以MSE不适用于分类问题。
   - MSE损失在接近0或1的位置，偏导数非常接近0，导致学习的会非常慢。

---

### 为啥分类任务不用KL散度？

由于KL散度中的前一部分−H(y)不变（目标分布的熵为常数），故在优化过程中，只需要关注交叉熵就可以了。但是如果目标分布是有变化的（如同为猫的样本，不同的样本，其值也会有差异），那么就不能使用交叉熵。例如蒸馏模型的损失函数就是KL散度，因为蒸馏模型的目标分布也是一个模型，该模型针对同类别的不同样本，会给出不同的预测值（如两张猫的图片a和b，目标模型对a预测为猫的值是0.6，对b预测为猫的值是0.8）。

---

### 不直接用距离相关的均方差：

假设神经网络的最后一层激活函数为sigmoid,它的两头异常的平，也就是说在那些地方的导数接近于0。而反向传播是需要求导的，用了均方差损失函数之后求导结果包含y(y-1) ，y接近于0或者1的时候都趋于0，会导致梯度消失，网络训练不下去（梯度弥散）。但如果用相对熵衍生出来的交叉熵作为损失函数则没有这个问题。因此虽然相对熵的距离特性不是特别好，但总归好过直接梯度消失玩不下去，因此很多用sigmoid作为激活函数的神经网络还是选择了用相对熵衍生出来的交叉熵作为损失函数。

---

### **为什么用交叉熵**

原因一，使用交叉熵loss下降的更快；反向梯度更新的时候，分类越正确越不需要更新权重，与预期一致

原因二，使用交叉熵是凸优化，MSE是非凸优化

---

### **交叉熵对噪声敏感吗**

**在理论上，交叉熵对噪声是敏感的**，因为噪声可以干扰模型的预测结果，导致交叉熵损失函数的值变得更大。特别地，当噪声比真实信号更强时，噪声对交叉熵的影响可能更加明显。

然而，**在实践中，交叉熵通常被认为是相对稳健的损失函数**，尤其是在深度学习中。这是因为深度学习模型通常具有强大的鲁棒性，能够对噪声进行适当的平滑化处理，从而减轻噪声对交叉熵损失函数的影响。此外，许多**正则化技术和数据增强技术**也可以用来提高模型的鲁棒性，从而进一步降低噪声的影响。

---

## **归一化**

### **为什么要做归一化**

网络模型中的归一化操作通常是指对输入数据进行归一化处理，即将数据按照一定的比例缩放到一个特定的范围内，使得数据的分布更加均匀，便于模型的训练和优化。

归一化的作用主要有以下几个方面：

1. 提高模型的训练速度和稳定性。**输入数据的分布越均匀，模型的训练越容易收敛，训练速度也会更快。同时，归一化还能够减少数据的方差，从而避免梯度消失或爆炸的问题，使得模型的训练更加稳定。**
2. 提高模型的泛化能力。归一化可以将不同特征的数据尺度统一，避免了不同特征对模型训练的影响不同的问题，从而使得模型更加关注数据的本质特征，提高了模型的泛化能力。
3. 减少模型对参数的依赖性。归一化可以将数据的尺度缩放到一个相对固定的范围内，使得模型对参数的变化更加鲁棒，减少了模型对参数的依赖性。

综上所述，**网络模型中的归一化操作可以有效地提高模型的训练速度和稳定性，提高模型的泛化能力，减少模型对参数的依赖性，从而提高模型的性能和效果。**

---

### **BN**

BN是通过在每个batch内计算均值和方差，对每个神经元的输入进行归一化。**在训练过程中，**BN对每个batch内的数据进行统计，**对于每个特征，计算该batch内所有样本对应特征的均值和标准差，然后对该特征进行归一化。在测试过程中，BN使用训练过程中计算得到的全域均值和标准差进行归一化。**

- 优点：能够有效地加速神经网络的收敛速度，使得网络更加稳定，提高了泛化能力，可以减少对学习率的依赖。
- 缺点：在小批量数据上可能存在较大的精度损失，由于归一化操作会打乱特征之间的相关性，可能会影响模型的表达能力。
- **总结：BN可以防止梯度消失或爆炸，加速训练过程，使网络更稳定，但对批次大小敏感**

---

### LN 

LN是通过在每层内计算均值和方差，对每个样本的所有特征进行归一化。**对于每个样本，计算该样本对应层内所有特征的均值和标准差，然后对该样本进行归一化。**

- 优点：能够有效地减少对batch size的依赖，能够处理长度不同的序列数据，如自然语言处理中的句子长度不一的问题。
- 缺点：相对于BN，LN在卷积神经网络中的表现不如BN。
- **总结：能够有效地减少对batch size的依赖，能够处理长度不同的序列数据，如自然语言处理中的句子长度不一的问题，不过在一些神经网络中的表现可能不如BN。**

---

### **NLP为啥不用BatchNorm**

- [BatchNorm对批次大小敏感，而NLP中的文本长度和批次大小往往不固定](https://blog.csdn.net/u013250861/article/details/119281548)[1](https://blog.csdn.net/u013250861/article/details/119281548)[2](https://zhuanlan.zhihu.com/p/126749311)。
- [BatchNorm需要计算每个特征的均值和方差，而NLP中的特征维度往往较高，计算量较大](https://zhuanlan.zhihu.com/p/126749311)[2](https://zhuanlan.zhihu.com/p/126749311)[3](https://zhuanlan.zhihu.com/p/74516930)。
- [BatchNorm可能会破坏词嵌入的语义信息，导致模型性能下降](https://zhuanlan.zhihu.com/p/126749311)[2](https://zhuanlan.zhihu.com/p/126749311)[3](https://zhuanlan.zhihu.com/p/74516930)。

[因此，NLP中更常用的归一化方法是LayerNorm，它可以适应任意批次大小和网络结构，主要对RNN效果明显](https://blog.csdn.net/u013250861/article/details/119281548)[1](https://blog.csdn.net/u013250861/article/details/119281548)[3](https://zhuanlan.zhihu.com/p/74516930)。

---

LN（Layer Normalization）和 BN（Batch Normalization）是两种常用的神经网络正则化技术，用于加速训练过程并提高模型的泛化性能。它们的主要特点和区别如下：

### Batch Normalization (BN)

**特点**：
1. **归一化方法**：BN 在每一批次的数据上进行归一化，即对每个 mini-batch 中的所有样本在同一层的同一维度进行归一化。
2. **计算公式**：
    - 对 mini-batch 中的每个特征 \( x_i \)，计算均值和方差：
      $$
      \mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i 
       \sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2
      $$
      
    - 使用均值和方差归一化：
      $$
       \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} 
      $$
      
    - 应用可学习的缩放和偏移参数：
      $$
       y_i = \gamma \hat{x}_i + \beta 
      $$
      其中，$\gamma$ 和 $\beta$是可学习的参数。
3. **位置**：BN 通常在卷积层之后和激活函数之前使用。
4. **加速收敛**：通过减小内部协变量偏移（Internal Covariate Shift），BN 可以加速模型训练并允许使用更高的学习率。
5. **依赖批次大小**：由于 BN 在 mini-batch 上进行归一化，批次大小对其效果有显著影响。

### Layer Normalization (LN)

**特点**：
1. **归一化方法**：LN 在每个单独的样本上进行归一化，即对每个样本在同一层的所有特征进行归一化。
2. **计算公式**：
    - 对每个样本 \( x \) 中的所有特征计算均值和方差：
      $$
      \mu = \frac{1}{H} \sum_{i=1}^{H} x_i 
       \sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2 
      $$
      
    - 使用均值和方差归一化：
      $$
      \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} 
      $$
      
    - 应用可学习的缩放和偏移参数：
      $$
      y_i = \gamma \hat{x}_i + \beta
      $$
      其中，$\gamma$ 和 $\beta $是可学习的参数。
3. **位置**：LN 通常应用于输入的每个特征维度，因此更适合于循环神经网络（RNN）和Transformer等序列模型。
4. **独立于批次大小**：由于 LN 在单个样本上进行归一化，它不依赖于批次大小，因此在小批次甚至单样本情况下表现稳定。

### 主要区别

1. **归一化维度**：
    - **BN**：对每个 mini-batch 的每一层的特征维度进行归一化。
    - **LN**：对每个样本的每一层的特征维度进行归一化。
    
2. **应用场景**：
    - **BN**：通常用于卷积神经网络（CNN）中，尤其在大型 mini-batch 训练时效果显著。
    - **LN**：通常用于序列模型，如 RNN 和 Transformer，尤其在小批次或单样本训练时效果更好。

3. **依赖性**：
    - **BN**：依赖 mini-batch 的大小和分布。
    - **LN**：不依赖 mini-batch 的大小和分布。

4. **训练稳定性**：
    - **BN**：需要较大的 mini-batch 来稳定均值和方差的估计。
    - **LN**：在小批次甚至单样本训练情况下表现稳定，不依赖 mini-batch 统计信息。

### 示例代码

以下是 PyTorch 中使用 BN 和 LN 的示例代码：

```python
import torch
import torch.nn as nn

# 示例数据
x = torch.randn(32, 128)  # 32 个样本，每个样本 128 维特征

# Batch Normalization
bn = nn.BatchNorm1d(128)
x_bn = bn(x)

# Layer Normalization
ln = nn.LayerNorm(128)
x_ln = ln(x)
```

总结来说，**BN 和 LN 都是用于加速训练和提高模型泛化性能的正则化技术。BN 在处理大批量数据时效果显著，而 LN 在处理小批量数据或序列数据时更稳定。**

---

### 在推理阶段BN是怎么工作的？

在 Batch Normalization (BN) 中，训练阶段和推理阶段的处理方式有所不同。具体来说，训练阶段通过 mini-batch 统计数据进行归一化，而推理阶段则使用整个训练过程中估计的全局统计数据。

### BN在推理阶段的处理方式

在训练阶段，每个 mini-batch 都会计算其均值和方差，用于归一化当前 mini-batch 的数据。这些 mini-batch 统计数据在推理阶段是不再可用的，因为推理阶段的输入数据通常是单个样本或较小的批次，为此我们需要使用训练阶段的全局统计数据。

为了获得这些全局统计数据，Batch Normalization 使用一种称为**指数加权平均 (Exponential Moving Average, EMA)**的方法。在训练过程中，BN 会对每个 mini-batch 计算的均值和方差进行 EMA 更新，从而得到整个训练过程中的全局均值和方差的估计值。

### EMA的工作原理

EMA 是一种平滑技术，它对新的值和前一状态的值赋予不同的权重，以便随着时间的推移逐渐更新统计数据。具体的公式如下：

对于均值$ \mu$ 和方差 $\sigma^2 $的 EMA 更新公式为：
$$
\mu_{t+1} = \alpha \cdot \mu_t + (1 - \alpha) \cdot \mu_{\text{batch}} \\
 \sigma_{t+1}^2 = \alpha \cdot \sigma_t^2 + (1 - \alpha) \cdot \sigma_{\text{batch}}^2
$$

### 训练和推理阶段的处理方式

1. **训练阶段**：
   - 计算每个 mini-batch 的均值 $\mu_{\text{batch}} $和方差 $\sigma_{\text{batch}}^2$.
   - 使用这些 mini-batch 统计数据对当前 mini-batch 的数据进行归一化。
   - 更新全局均值和方差的 EMA，以便在推理阶段使用。

2. **推理阶段**：
   - 使用训练阶段通过 EMA 得到的全局均值和方差对输入数据进行归一化，而不再使用 mini-batch 统计数据。

### PyTorch中的实现

在 PyTorch 中，BatchNorm 层自动处理训练和推理阶段的不同处理方式。以下是 PyTorch 中 BatchNorm 的实现示例：

```python
import torch
import torch.nn as nn

# 示例数据
x = torch.randn(32, 128)  # 32 个样本，每个样本 128 维特征

# 定义 BatchNorm 层
bn = nn.BatchNorm1d(128)

# 训练阶段
bn.train()
x_train = bn(x)  # 使用 mini-batch 统计数据进行归一化

# 推理阶段
bn.eval()
x_test = bn(x)  # 使用全局 EMA 统计数据进行归一化
```

**在训练阶段（`bn.train()`），BatchNorm 层使用当前 mini-batch 的均值和方差进行归一化，并更新全局的 EMA 均值和方差。在推理阶段（`bn.eval()`），BatchNorm 层使用训练阶段通过 EMA 计算得到的全局均值和方差进行归一化。**

### **总结**

- **训练阶段：使用 mini-batch 的均值和方差进行归一化，同时更新全局的 EMA 均值和方差。**
- **推理阶段：使用训练阶段通过 EMA 计算得到的全局均值和方差进行归一化。**

**这种方法确保了在推理阶段即使批次大小较小或为单个样本，也能够稳定地进行归一化处理，提高模型的泛化能力。**

---

