<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DL | Sanmu</title>
<meta name="keywords" content="DL">
<meta name="description" content="神经网络 DNN/MLP/ANN 全连接神经网络/多层感知器
DNN是从多层感知器发展而来的第三代神经网络，DNN的网络结构分为三部分：输入层、隐藏层、输出层。
DNN的层与层之间是全连接的，第i层的任意一个神经元一定与第i&#43;1层的任意一个神经元连接.
https://www.cnblogs.com/init0ne/p/16630311.html
KNN KNN（K-Nearest Neighbors，K近邻算法）是一种常见的无参数（non-parametric）分类和回归算法，常用于基于实例的学习（instance-based learning）。
对于分类问题，KNN算法会查找与该未知样本最近的K个样本点，然后将这K个样本点中出现最多的类别作为该未知样本的预测类别。对于回归问题，KNN算法会查找与该未知样本最近的K个样本点，并将这K个样本点的输出值的平均值作为该未知样本的预测输出值。
KNN算法中，距离度量是非常重要的一环。通常使用欧氏距离（Euclidean distance）或曼哈顿距离（Manhattan distance）等常见的距离度量方法。另外，KNN算法还需要确定K值的大小，通常通过交叉验证（cross-validation）等技术进行调整。
KNN算法的优点是简单易懂、易于实现、对于大规模数据集也可以高效处理。缺点是需要对每个未知样本进行计算距离和排序，当数据集规模很大时，计算成本会非常高，同时对于高维度的数据集，样本之间的距离可能会失去准确性，从而影响KNN算法的表现。
RNN RNN是一种神经网络结构，它通过在不同时间步之间共享权重来处理序列数据。与其他神经网络结构不同的是，RNN 具有记忆能力，可以在处理当前数据时利用之前的信息。它们的优点在于可以接受任意长度的输入序列，并且可以利用序列中之前的信息来帮助预测下一个值。
RNN 的一个基本单元是循环单元（recurrent unit），也称为记忆单元（memory cell），可以是简单的结构（如 Vanilla RNN）或复杂的结构（如 LSTM 和 GRU）。这些循环单元接受输入和先前的隐藏状态，产生一个新的隐藏状态，并输出一个值，该值可以被用于下一时间步的计算。通过不断地将先前的隐藏状态传递给下一时间步，RNN 可以利用历史信息来影响当前的输出。
RNN 的训练可以通过反向传播算法进行，这可以通过计算当前时间步的误差来更新网络的参数，以改善其性能。但是，RNN 的训练存在梯度消失或梯度爆炸等问题，这可能导致在训练过程中难以传递长期依赖的信息。为了解决这些问题，出现了一些改进的 RNN 结构，如 LSTM（长短时记忆网络）和 GRU（门控循环单元）。这些结构可以更好地处理长期依赖和短期信息。
优缺点 优点：
RNN 具有记忆能力，可以利用序列数据之前的信息来预测下一个值，从而更好地处理序列数据。 RNN 可以接受任意长度的输入序列，并且可以在每个时间步骤使用相同的参数，从而可以处理变长的序列数据。 RNN 的参数可以通过反向传播算法来训练，这使得其可以学习到数据之间的关系，从而可以在自然语言处理、语音识别等领域中得到广泛应用。 缺点：
RNN 计算的时间复杂度与序列长度成正比，处理长序列时可能会面临较大的计算开销和内存消耗。 RNN 的训练可能会受到梯度消失或梯度爆炸等问题的影响，尤其是在处理长序列数据时，这些问题可能会导致模型难以捕捉到序列中的长期依赖关系。 RNN 的记忆能力有限，无法捕捉到非常长的历史信息，这可能会影响模型的预测性能。 RNN为什么会梯度消失或爆炸 RNN（循环神经网络）在处理长序列时，存在梯度消失或梯度爆炸的问题，这是由于其计算方式和训练过程的特性所导致的。
具体来说，RNN 的计算方式是通过重复应用相同的权重来处理序列数据，即在每个时间步使用相同的权重矩阵进行计算，这种计算方式会导致随着序列长度的增加，信息在网络中传递时会经过多次的矩阵乘法，从而使得梯度在反向传播过程中会不断地被乘以相同的权重，从而可能会出现指数级的增长或衰减，这就是梯度爆炸或梯度消失的问题。
RNN的梯度消失不是真正的梯度消失，当下的梯度总会在，只是前面的梯度没了。RNN的梯度消失指的是当下梯度用不到前面的梯度了。
当梯度爆炸发生时，梯度的值变得非常大，导致网络参数更新过快，使得网络的性能变得不稳定。当梯度消失发生时，梯度的值变得非常小，导致网络参数更新过慢，使得网络无法学习到长期依赖的信息，从而影响了网络的预测性能。
为了解决这些问题，一些改进的 RNN 结构被提出，如 LSTM（长短时记忆网络）和 GRU（门控循环单元），它们使用了一些特殊的门机制来控制信息的流动和更新，从而更好地处理了长序列数据，并有效地缓解了梯度消失或梯度爆炸的问题。
LSTM LSTM（长短时记忆网络）是一种特殊类型的循环神经网络（RNN），它在处理序列数据时能够有效地捕捉长期依赖关系，通过使用一组称为“门”的结构来控制信息的流动，包括输入门、遗忘门和输出门。这些门可以在不同的时间步骤决定从当前输入和前一个时间步骤的输出中选择哪些信息需要被传递和保留。此外，LSTM还包括一个称为细胞状态的重要部分，可以在时间步骤之间存储和传递信息。
LSTM的优点在于它可以避免梯度消失问题，而且能够处理长期依赖关系，即使序列长度很长。此外，LSTM还具有较好的鲁棒性和泛化能力，可以适应不同的任务和数据集。
三个门的作用 1)遗忘门ft控制上一个时刻的内部状态ct-1需要遗忘多少信息。
2)输入门it控制当前的候选状态有多少信息需要保存。
3)输出门ot控制当前时刻的内部状态ct有多少信息需要输出给外部状态ht
激活函数 LSTM（长短时记忆）是一种递归神经网络，由输入门、遗忘门、输出门和一个单元状态组成。每个门都有一个对应的激活函数。以下是LSTM中使用的激活函数：">
<meta name="author" content="Sanmu">
<link rel="canonical" href="https://welldonesanmu.github.io/posts/dl/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.76360485988b2c36106a3ffacc6d884d6bd71cabfcc81ca9a4964617d251b660.css" integrity="sha256-djYEhZiLLDYQaj/6zG2ITWvXHKv8yByppJZGF9JRtmA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://welldonesanmu.github.io/img/avatars.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="https://welldonesanmu.github.io/img/avatars.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="https://welldonesanmu.github.io/img/avatars.jpg">
<link rel="apple-touch-icon" href="https://welldonesanmu.github.io/img/avatars.jpg">
<link rel="mask-icon" href="https://welldonesanmu.github.io/img/avatars.jpg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<meta property="og:title" content="DL" />
<meta property="og:description" content="神经网络 DNN/MLP/ANN 全连接神经网络/多层感知器
DNN是从多层感知器发展而来的第三代神经网络，DNN的网络结构分为三部分：输入层、隐藏层、输出层。
DNN的层与层之间是全连接的，第i层的任意一个神经元一定与第i&#43;1层的任意一个神经元连接.
https://www.cnblogs.com/init0ne/p/16630311.html
KNN KNN（K-Nearest Neighbors，K近邻算法）是一种常见的无参数（non-parametric）分类和回归算法，常用于基于实例的学习（instance-based learning）。
对于分类问题，KNN算法会查找与该未知样本最近的K个样本点，然后将这K个样本点中出现最多的类别作为该未知样本的预测类别。对于回归问题，KNN算法会查找与该未知样本最近的K个样本点，并将这K个样本点的输出值的平均值作为该未知样本的预测输出值。
KNN算法中，距离度量是非常重要的一环。通常使用欧氏距离（Euclidean distance）或曼哈顿距离（Manhattan distance）等常见的距离度量方法。另外，KNN算法还需要确定K值的大小，通常通过交叉验证（cross-validation）等技术进行调整。
KNN算法的优点是简单易懂、易于实现、对于大规模数据集也可以高效处理。缺点是需要对每个未知样本进行计算距离和排序，当数据集规模很大时，计算成本会非常高，同时对于高维度的数据集，样本之间的距离可能会失去准确性，从而影响KNN算法的表现。
RNN RNN是一种神经网络结构，它通过在不同时间步之间共享权重来处理序列数据。与其他神经网络结构不同的是，RNN 具有记忆能力，可以在处理当前数据时利用之前的信息。它们的优点在于可以接受任意长度的输入序列，并且可以利用序列中之前的信息来帮助预测下一个值。
RNN 的一个基本单元是循环单元（recurrent unit），也称为记忆单元（memory cell），可以是简单的结构（如 Vanilla RNN）或复杂的结构（如 LSTM 和 GRU）。这些循环单元接受输入和先前的隐藏状态，产生一个新的隐藏状态，并输出一个值，该值可以被用于下一时间步的计算。通过不断地将先前的隐藏状态传递给下一时间步，RNN 可以利用历史信息来影响当前的输出。
RNN 的训练可以通过反向传播算法进行，这可以通过计算当前时间步的误差来更新网络的参数，以改善其性能。但是，RNN 的训练存在梯度消失或梯度爆炸等问题，这可能导致在训练过程中难以传递长期依赖的信息。为了解决这些问题，出现了一些改进的 RNN 结构，如 LSTM（长短时记忆网络）和 GRU（门控循环单元）。这些结构可以更好地处理长期依赖和短期信息。
优缺点 优点：
RNN 具有记忆能力，可以利用序列数据之前的信息来预测下一个值，从而更好地处理序列数据。 RNN 可以接受任意长度的输入序列，并且可以在每个时间步骤使用相同的参数，从而可以处理变长的序列数据。 RNN 的参数可以通过反向传播算法来训练，这使得其可以学习到数据之间的关系，从而可以在自然语言处理、语音识别等领域中得到广泛应用。 缺点：
RNN 计算的时间复杂度与序列长度成正比，处理长序列时可能会面临较大的计算开销和内存消耗。 RNN 的训练可能会受到梯度消失或梯度爆炸等问题的影响，尤其是在处理长序列数据时，这些问题可能会导致模型难以捕捉到序列中的长期依赖关系。 RNN 的记忆能力有限，无法捕捉到非常长的历史信息，这可能会影响模型的预测性能。 RNN为什么会梯度消失或爆炸 RNN（循环神经网络）在处理长序列时，存在梯度消失或梯度爆炸的问题，这是由于其计算方式和训练过程的特性所导致的。
具体来说，RNN 的计算方式是通过重复应用相同的权重来处理序列数据，即在每个时间步使用相同的权重矩阵进行计算，这种计算方式会导致随着序列长度的增加，信息在网络中传递时会经过多次的矩阵乘法，从而使得梯度在反向传播过程中会不断地被乘以相同的权重，从而可能会出现指数级的增长或衰减，这就是梯度爆炸或梯度消失的问题。
RNN的梯度消失不是真正的梯度消失，当下的梯度总会在，只是前面的梯度没了。RNN的梯度消失指的是当下梯度用不到前面的梯度了。
当梯度爆炸发生时，梯度的值变得非常大，导致网络参数更新过快，使得网络的性能变得不稳定。当梯度消失发生时，梯度的值变得非常小，导致网络参数更新过慢，使得网络无法学习到长期依赖的信息，从而影响了网络的预测性能。
为了解决这些问题，一些改进的 RNN 结构被提出，如 LSTM（长短时记忆网络）和 GRU（门控循环单元），它们使用了一些特殊的门机制来控制信息的流动和更新，从而更好地处理了长序列数据，并有效地缓解了梯度消失或梯度爆炸的问题。
LSTM LSTM（长短时记忆网络）是一种特殊类型的循环神经网络（RNN），它在处理序列数据时能够有效地捕捉长期依赖关系，通过使用一组称为“门”的结构来控制信息的流动，包括输入门、遗忘门和输出门。这些门可以在不同的时间步骤决定从当前输入和前一个时间步骤的输出中选择哪些信息需要被传递和保留。此外，LSTM还包括一个称为细胞状态的重要部分，可以在时间步骤之间存储和传递信息。
LSTM的优点在于它可以避免梯度消失问题，而且能够处理长期依赖关系，即使序列长度很长。此外，LSTM还具有较好的鲁棒性和泛化能力，可以适应不同的任务和数据集。
三个门的作用 1)遗忘门ft控制上一个时刻的内部状态ct-1需要遗忘多少信息。
2)输入门it控制当前的候选状态有多少信息需要保存。
3)输出门ot控制当前时刻的内部状态ct有多少信息需要输出给外部状态ht
激活函数 LSTM（长短时记忆）是一种递归神经网络，由输入门、遗忘门、输出门和一个单元状态组成。每个门都有一个对应的激活函数。以下是LSTM中使用的激活函数：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://welldonesanmu.github.io/posts/dl/" /><meta property="og:image" content="https://welldonesanmu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-19T09:06:49+08:00" />
<meta property="article:modified_time" content="2024-05-19T09:06:49+08:00" /><meta property="og:site_name" content="Sanmu" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://welldonesanmu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="DL"/>
<meta name="twitter:description" content="神经网络 DNN/MLP/ANN 全连接神经网络/多层感知器
DNN是从多层感知器发展而来的第三代神经网络，DNN的网络结构分为三部分：输入层、隐藏层、输出层。
DNN的层与层之间是全连接的，第i层的任意一个神经元一定与第i&#43;1层的任意一个神经元连接.
https://www.cnblogs.com/init0ne/p/16630311.html
KNN KNN（K-Nearest Neighbors，K近邻算法）是一种常见的无参数（non-parametric）分类和回归算法，常用于基于实例的学习（instance-based learning）。
对于分类问题，KNN算法会查找与该未知样本最近的K个样本点，然后将这K个样本点中出现最多的类别作为该未知样本的预测类别。对于回归问题，KNN算法会查找与该未知样本最近的K个样本点，并将这K个样本点的输出值的平均值作为该未知样本的预测输出值。
KNN算法中，距离度量是非常重要的一环。通常使用欧氏距离（Euclidean distance）或曼哈顿距离（Manhattan distance）等常见的距离度量方法。另外，KNN算法还需要确定K值的大小，通常通过交叉验证（cross-validation）等技术进行调整。
KNN算法的优点是简单易懂、易于实现、对于大规模数据集也可以高效处理。缺点是需要对每个未知样本进行计算距离和排序，当数据集规模很大时，计算成本会非常高，同时对于高维度的数据集，样本之间的距离可能会失去准确性，从而影响KNN算法的表现。
RNN RNN是一种神经网络结构，它通过在不同时间步之间共享权重来处理序列数据。与其他神经网络结构不同的是，RNN 具有记忆能力，可以在处理当前数据时利用之前的信息。它们的优点在于可以接受任意长度的输入序列，并且可以利用序列中之前的信息来帮助预测下一个值。
RNN 的一个基本单元是循环单元（recurrent unit），也称为记忆单元（memory cell），可以是简单的结构（如 Vanilla RNN）或复杂的结构（如 LSTM 和 GRU）。这些循环单元接受输入和先前的隐藏状态，产生一个新的隐藏状态，并输出一个值，该值可以被用于下一时间步的计算。通过不断地将先前的隐藏状态传递给下一时间步，RNN 可以利用历史信息来影响当前的输出。
RNN 的训练可以通过反向传播算法进行，这可以通过计算当前时间步的误差来更新网络的参数，以改善其性能。但是，RNN 的训练存在梯度消失或梯度爆炸等问题，这可能导致在训练过程中难以传递长期依赖的信息。为了解决这些问题，出现了一些改进的 RNN 结构，如 LSTM（长短时记忆网络）和 GRU（门控循环单元）。这些结构可以更好地处理长期依赖和短期信息。
优缺点 优点：
RNN 具有记忆能力，可以利用序列数据之前的信息来预测下一个值，从而更好地处理序列数据。 RNN 可以接受任意长度的输入序列，并且可以在每个时间步骤使用相同的参数，从而可以处理变长的序列数据。 RNN 的参数可以通过反向传播算法来训练，这使得其可以学习到数据之间的关系，从而可以在自然语言处理、语音识别等领域中得到广泛应用。 缺点：
RNN 计算的时间复杂度与序列长度成正比，处理长序列时可能会面临较大的计算开销和内存消耗。 RNN 的训练可能会受到梯度消失或梯度爆炸等问题的影响，尤其是在处理长序列数据时，这些问题可能会导致模型难以捕捉到序列中的长期依赖关系。 RNN 的记忆能力有限，无法捕捉到非常长的历史信息，这可能会影响模型的预测性能。 RNN为什么会梯度消失或爆炸 RNN（循环神经网络）在处理长序列时，存在梯度消失或梯度爆炸的问题，这是由于其计算方式和训练过程的特性所导致的。
具体来说，RNN 的计算方式是通过重复应用相同的权重来处理序列数据，即在每个时间步使用相同的权重矩阵进行计算，这种计算方式会导致随着序列长度的增加，信息在网络中传递时会经过多次的矩阵乘法，从而使得梯度在反向传播过程中会不断地被乘以相同的权重，从而可能会出现指数级的增长或衰减，这就是梯度爆炸或梯度消失的问题。
RNN的梯度消失不是真正的梯度消失，当下的梯度总会在，只是前面的梯度没了。RNN的梯度消失指的是当下梯度用不到前面的梯度了。
当梯度爆炸发生时，梯度的值变得非常大，导致网络参数更新过快，使得网络的性能变得不稳定。当梯度消失发生时，梯度的值变得非常小，导致网络参数更新过慢，使得网络无法学习到长期依赖的信息，从而影响了网络的预测性能。
为了解决这些问题，一些改进的 RNN 结构被提出，如 LSTM（长短时记忆网络）和 GRU（门控循环单元），它们使用了一些特殊的门机制来控制信息的流动和更新，从而更好地处理了长序列数据，并有效地缓解了梯度消失或梯度爆炸的问题。
LSTM LSTM（长短时记忆网络）是一种特殊类型的循环神经网络（RNN），它在处理序列数据时能够有效地捕捉长期依赖关系，通过使用一组称为“门”的结构来控制信息的流动，包括输入门、遗忘门和输出门。这些门可以在不同的时间步骤决定从当前输入和前一个时间步骤的输出中选择哪些信息需要被传递和保留。此外，LSTM还包括一个称为细胞状态的重要部分，可以在时间步骤之间存储和传递信息。
LSTM的优点在于它可以避免梯度消失问题，而且能够处理长期依赖关系，即使序列长度很长。此外，LSTM还具有较好的鲁棒性和泛化能力，可以适应不同的任务和数据集。
三个门的作用 1)遗忘门ft控制上一个时刻的内部状态ct-1需要遗忘多少信息。
2)输入门it控制当前的候选状态有多少信息需要保存。
3)输出门ot控制当前时刻的内部状态ct有多少信息需要输出给外部状态ht
激活函数 LSTM（长短时记忆）是一种递归神经网络，由输入门、遗忘门、输出门和一个单元状态组成。每个门都有一个对应的激活函数。以下是LSTM中使用的激活函数："/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://welldonesanmu.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "DL",
      "item": "https://welldonesanmu.github.io/posts/dl/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DL",
  "name": "DL",
  "description": "神经网络 DNN/MLP/ANN 全连接神经网络/多层感知器\nDNN是从多层感知器发展而来的第三代神经网络，DNN的网络结构分为三部分：输入层、隐藏层、输出层。\nDNN的层与层之间是全连接的，第i层的任意一个神经元一定与第i+1层的任意一个神经元连接.\nhttps://www.cnblogs.com/init0ne/p/16630311.html\nKNN KNN（K-Nearest Neighbors，K近邻算法）是一种常见的无参数（non-parametric）分类和回归算法，常用于基于实例的学习（instance-based learning）。\n对于分类问题，KNN算法会查找与该未知样本最近的K个样本点，然后将这K个样本点中出现最多的类别作为该未知样本的预测类别。对于回归问题，KNN算法会查找与该未知样本最近的K个样本点，并将这K个样本点的输出值的平均值作为该未知样本的预测输出值。\nKNN算法中，距离度量是非常重要的一环。通常使用欧氏距离（Euclidean distance）或曼哈顿距离（Manhattan distance）等常见的距离度量方法。另外，KNN算法还需要确定K值的大小，通常通过交叉验证（cross-validation）等技术进行调整。\nKNN算法的优点是简单易懂、易于实现、对于大规模数据集也可以高效处理。缺点是需要对每个未知样本进行计算距离和排序，当数据集规模很大时，计算成本会非常高，同时对于高维度的数据集，样本之间的距离可能会失去准确性，从而影响KNN算法的表现。\nRNN RNN是一种神经网络结构，它通过在不同时间步之间共享权重来处理序列数据。与其他神经网络结构不同的是，RNN 具有记忆能力，可以在处理当前数据时利用之前的信息。它们的优点在于可以接受任意长度的输入序列，并且可以利用序列中之前的信息来帮助预测下一个值。\nRNN 的一个基本单元是循环单元（recurrent unit），也称为记忆单元（memory cell），可以是简单的结构（如 Vanilla RNN）或复杂的结构（如 LSTM 和 GRU）。这些循环单元接受输入和先前的隐藏状态，产生一个新的隐藏状态，并输出一个值，该值可以被用于下一时间步的计算。通过不断地将先前的隐藏状态传递给下一时间步，RNN 可以利用历史信息来影响当前的输出。\nRNN 的训练可以通过反向传播算法进行，这可以通过计算当前时间步的误差来更新网络的参数，以改善其性能。但是，RNN 的训练存在梯度消失或梯度爆炸等问题，这可能导致在训练过程中难以传递长期依赖的信息。为了解决这些问题，出现了一些改进的 RNN 结构，如 LSTM（长短时记忆网络）和 GRU（门控循环单元）。这些结构可以更好地处理长期依赖和短期信息。\n优缺点 优点：\nRNN 具有记忆能力，可以利用序列数据之前的信息来预测下一个值，从而更好地处理序列数据。 RNN 可以接受任意长度的输入序列，并且可以在每个时间步骤使用相同的参数，从而可以处理变长的序列数据。 RNN 的参数可以通过反向传播算法来训练，这使得其可以学习到数据之间的关系，从而可以在自然语言处理、语音识别等领域中得到广泛应用。 缺点：\nRNN 计算的时间复杂度与序列长度成正比，处理长序列时可能会面临较大的计算开销和内存消耗。 RNN 的训练可能会受到梯度消失或梯度爆炸等问题的影响，尤其是在处理长序列数据时，这些问题可能会导致模型难以捕捉到序列中的长期依赖关系。 RNN 的记忆能力有限，无法捕捉到非常长的历史信息，这可能会影响模型的预测性能。 RNN为什么会梯度消失或爆炸 RNN（循环神经网络）在处理长序列时，存在梯度消失或梯度爆炸的问题，这是由于其计算方式和训练过程的特性所导致的。\n具体来说，RNN 的计算方式是通过重复应用相同的权重来处理序列数据，即在每个时间步使用相同的权重矩阵进行计算，这种计算方式会导致随着序列长度的增加，信息在网络中传递时会经过多次的矩阵乘法，从而使得梯度在反向传播过程中会不断地被乘以相同的权重，从而可能会出现指数级的增长或衰减，这就是梯度爆炸或梯度消失的问题。\nRNN的梯度消失不是真正的梯度消失，当下的梯度总会在，只是前面的梯度没了。RNN的梯度消失指的是当下梯度用不到前面的梯度了。\n当梯度爆炸发生时，梯度的值变得非常大，导致网络参数更新过快，使得网络的性能变得不稳定。当梯度消失发生时，梯度的值变得非常小，导致网络参数更新过慢，使得网络无法学习到长期依赖的信息，从而影响了网络的预测性能。\n为了解决这些问题，一些改进的 RNN 结构被提出，如 LSTM（长短时记忆网络）和 GRU（门控循环单元），它们使用了一些特殊的门机制来控制信息的流动和更新，从而更好地处理了长序列数据，并有效地缓解了梯度消失或梯度爆炸的问题。\nLSTM LSTM（长短时记忆网络）是一种特殊类型的循环神经网络（RNN），它在处理序列数据时能够有效地捕捉长期依赖关系，通过使用一组称为“门”的结构来控制信息的流动，包括输入门、遗忘门和输出门。这些门可以在不同的时间步骤决定从当前输入和前一个时间步骤的输出中选择哪些信息需要被传递和保留。此外，LSTM还包括一个称为细胞状态的重要部分，可以在时间步骤之间存储和传递信息。\nLSTM的优点在于它可以避免梯度消失问题，而且能够处理长期依赖关系，即使序列长度很长。此外，LSTM还具有较好的鲁棒性和泛化能力，可以适应不同的任务和数据集。\n三个门的作用 1)遗忘门ft控制上一个时刻的内部状态ct-1需要遗忘多少信息。\n2)输入门it控制当前的候选状态有多少信息需要保存。\n3)输出门ot控制当前时刻的内部状态ct有多少信息需要输出给外部状态ht\n激活函数 LSTM（长短时记忆）是一种递归神经网络，由输入门、遗忘门、输出门和一个单元状态组成。每个门都有一个对应的激活函数。以下是LSTM中使用的激活函数：",
  "keywords": [
    "DL"
  ],
  "articleBody": "神经网络 DNN/MLP/ANN 全连接神经网络/多层感知器\nDNN是从多层感知器发展而来的第三代神经网络，DNN的网络结构分为三部分：输入层、隐藏层、输出层。\nDNN的层与层之间是全连接的，第i层的任意一个神经元一定与第i+1层的任意一个神经元连接.\nhttps://www.cnblogs.com/init0ne/p/16630311.html\nKNN KNN（K-Nearest Neighbors，K近邻算法）是一种常见的无参数（non-parametric）分类和回归算法，常用于基于实例的学习（instance-based learning）。\n对于分类问题，KNN算法会查找与该未知样本最近的K个样本点，然后将这K个样本点中出现最多的类别作为该未知样本的预测类别。对于回归问题，KNN算法会查找与该未知样本最近的K个样本点，并将这K个样本点的输出值的平均值作为该未知样本的预测输出值。\nKNN算法中，距离度量是非常重要的一环。通常使用欧氏距离（Euclidean distance）或曼哈顿距离（Manhattan distance）等常见的距离度量方法。另外，KNN算法还需要确定K值的大小，通常通过交叉验证（cross-validation）等技术进行调整。\nKNN算法的优点是简单易懂、易于实现、对于大规模数据集也可以高效处理。缺点是需要对每个未知样本进行计算距离和排序，当数据集规模很大时，计算成本会非常高，同时对于高维度的数据集，样本之间的距离可能会失去准确性，从而影响KNN算法的表现。\nRNN RNN是一种神经网络结构，它通过在不同时间步之间共享权重来处理序列数据。与其他神经网络结构不同的是，RNN 具有记忆能力，可以在处理当前数据时利用之前的信息。它们的优点在于可以接受任意长度的输入序列，并且可以利用序列中之前的信息来帮助预测下一个值。\nRNN 的一个基本单元是循环单元（recurrent unit），也称为记忆单元（memory cell），可以是简单的结构（如 Vanilla RNN）或复杂的结构（如 LSTM 和 GRU）。这些循环单元接受输入和先前的隐藏状态，产生一个新的隐藏状态，并输出一个值，该值可以被用于下一时间步的计算。通过不断地将先前的隐藏状态传递给下一时间步，RNN 可以利用历史信息来影响当前的输出。\nRNN 的训练可以通过反向传播算法进行，这可以通过计算当前时间步的误差来更新网络的参数，以改善其性能。但是，RNN 的训练存在梯度消失或梯度爆炸等问题，这可能导致在训练过程中难以传递长期依赖的信息。为了解决这些问题，出现了一些改进的 RNN 结构，如 LSTM（长短时记忆网络）和 GRU（门控循环单元）。这些结构可以更好地处理长期依赖和短期信息。\n优缺点 优点：\nRNN 具有记忆能力，可以利用序列数据之前的信息来预测下一个值，从而更好地处理序列数据。 RNN 可以接受任意长度的输入序列，并且可以在每个时间步骤使用相同的参数，从而可以处理变长的序列数据。 RNN 的参数可以通过反向传播算法来训练，这使得其可以学习到数据之间的关系，从而可以在自然语言处理、语音识别等领域中得到广泛应用。 缺点：\nRNN 计算的时间复杂度与序列长度成正比，处理长序列时可能会面临较大的计算开销和内存消耗。 RNN 的训练可能会受到梯度消失或梯度爆炸等问题的影响，尤其是在处理长序列数据时，这些问题可能会导致模型难以捕捉到序列中的长期依赖关系。 RNN 的记忆能力有限，无法捕捉到非常长的历史信息，这可能会影响模型的预测性能。 RNN为什么会梯度消失或爆炸 RNN（循环神经网络）在处理长序列时，存在梯度消失或梯度爆炸的问题，这是由于其计算方式和训练过程的特性所导致的。\n具体来说，RNN 的计算方式是通过重复应用相同的权重来处理序列数据，即在每个时间步使用相同的权重矩阵进行计算，这种计算方式会导致随着序列长度的增加，信息在网络中传递时会经过多次的矩阵乘法，从而使得梯度在反向传播过程中会不断地被乘以相同的权重，从而可能会出现指数级的增长或衰减，这就是梯度爆炸或梯度消失的问题。\nRNN的梯度消失不是真正的梯度消失，当下的梯度总会在，只是前面的梯度没了。RNN的梯度消失指的是当下梯度用不到前面的梯度了。\n当梯度爆炸发生时，梯度的值变得非常大，导致网络参数更新过快，使得网络的性能变得不稳定。当梯度消失发生时，梯度的值变得非常小，导致网络参数更新过慢，使得网络无法学习到长期依赖的信息，从而影响了网络的预测性能。\n为了解决这些问题，一些改进的 RNN 结构被提出，如 LSTM（长短时记忆网络）和 GRU（门控循环单元），它们使用了一些特殊的门机制来控制信息的流动和更新，从而更好地处理了长序列数据，并有效地缓解了梯度消失或梯度爆炸的问题。\nLSTM LSTM（长短时记忆网络）是一种特殊类型的循环神经网络（RNN），它在处理序列数据时能够有效地捕捉长期依赖关系，通过使用一组称为“门”的结构来控制信息的流动，包括输入门、遗忘门和输出门。这些门可以在不同的时间步骤决定从当前输入和前一个时间步骤的输出中选择哪些信息需要被传递和保留。此外，LSTM还包括一个称为细胞状态的重要部分，可以在时间步骤之间存储和传递信息。\nLSTM的优点在于它可以避免梯度消失问题，而且能够处理长期依赖关系，即使序列长度很长。此外，LSTM还具有较好的鲁棒性和泛化能力，可以适应不同的任务和数据集。\n三个门的作用 1)遗忘门ft控制上一个时刻的内部状态ct-1需要遗忘多少信息。\n2)输入门it控制当前的候选状态有多少信息需要保存。\n3)输出门ot控制当前时刻的内部状态ct有多少信息需要输出给外部状态ht\n激活函数 LSTM（长短时记忆）是一种递归神经网络，由输入门、遗忘门、输出门和一个单元状态组成。每个门都有一个对应的激活函数。以下是LSTM中使用的激活函数：\nSigmoid函数：用于输入门、遗忘门和输出门的门控制器。Sigmoid函数将输入值映射到0到1之间的输出值，用于控制门的开放程度。 双曲正切函数（Tanh）：用于更新单元状态。Tanh函数将输入值映射到-1到1之间的输出值，用于控制单元状态的更新。 优缺点 优点:\n解决了梯度消失问题：在传统的循环神经网络中，随着时间步长的增加，梯度可以变得非常小甚至消失。LSTM 使用门结构来控制信息的流动，从而避免了梯度消失问题。 能够处理长期依赖关系：LSTM 在细胞状态中存储信息，能够在时间步骤之间传递长期依赖关系。 对于不同的序列长度有良好的适应性：LSTM 可以自适应地调整序列长度，并对输入和输出数据的变化进行处理。 可以处理多种类型的数据：LSTM 适用于多种类型的数据，如文本、语音、图像等。 缺点:\n计算代价高：LSTM 中包含大量的参数，因此需要更多的计算代价。此外，LSTM 的训练也需要更多的计算资源。 容易出现过拟合：在使用 LSTM 时，由于模型的复杂性，容易出现过拟合的问题，因此需要采取适当的正则化方法。 需要大量的数据训练：为了训练 LSTM，需要大量的数据进行训练。如果数据集较小，则容易出现过拟合的问题。 lstm怎么缓解梯度消失和爆炸问题 LSTM通过门控结构来解决梯度消失或爆炸问题123。LSTM的遗忘门可以选择性地保留或删除远距离信息，从而缓解梯度消失1。LSTM的输入门和输出门可以控制信息的流入和流出，从而减少梯度爆炸的风险24。LSTM还使用了tanh激活函数，使得梯度不会无限增大或减小4。\nLSTM和Transformer的比较 Transformer和LSTM都是深度学习中常用的序列模型，但它们在结构和应用方面有一些不同。\nTransformer和LSTM的最大区别，就是LSTM的训练是迭代的、串行的，必须要等当前字处理完，才可以处理下一个字。而Transformer的训练时并行的，即所有字是同时训练的，这样就大大增加了计算效率。\nLSTM是一种递归神经网络（Recurrent Neural Network, RNN），用于处理序列数据。它的主要结构是一个神经元，它接收输入和上一个时间步的隐藏状态，并输出当前时间步的隐藏状态和输出。LSTM的主要组成部分包括输入门、遗忘门和输出门。\nTransformer是一种基于注意力机制（Attention Mechanism）的神经网络，用于处理序列数据。它不依赖于递归结构，而是使用自注意力机制来建立全局联系。Transformer由编码器和解码器组成，其中编码器和解码器都由多个编码器层和解码器层组成。\nLSTM在处理长序列时容易出现梯度消失或梯度爆炸的问题。为了解决这个问题，LSTM通常采用反向传播和门控制技术。\nTransformer的训练相对于LSTM更容易收敛，因为它没有递归结构和门控制机制。Transformer通常采用自适应优化器（如Adam）和残差连接来训练。\nTransformer、Attention Transformer的设计基于一个核心思想，即用自注意力机制（self-attention）来捕捉序列之间的依赖关系，从而更好地处理序列数据。自注意力机制允许模型对输入序列中的所有位置进行编码，而不像传统的RNN只能处理有限的局部上下文。因此，Transformer能够同时处理长序列和短序列，从而提高模型的效率和准确性。\ntransformer的结构 Transformer的激活函数 ReLU：用于前馈神经网络层中的激活函数。 GELU：用于一些Transformer变体（如BERT）中的前馈神经网络层。 Softmax：用于自注意力机制中的权重归一化。 GELU也是Transformer中常用的激活函数，尤其是在一些变体中，如BERT（Bidirectional Encoder Representations from Transformers）。与ReLU相比，GELU在激活时考虑了输入的标准正态分布。\nTransformer的优化器 Adam\n什么时候只使用encoder，什么时候只使用decoder，什么时候二者一起使用 Transformer是一个非常流行的深度学习模型，通常被用于自然语言处理和其他序列数据的任务。它包含了encoder和decoder两个部分。\n当只需要对输入序列进行编码表示，而不需要生成输出序列时，通常只使用Transformer的encoder部分。例如，当我们需要对一组文本进行聚类或分类时，可以使用Transformer的encoder部分对每个文本进行编码表示，然后对这些表示进行聚类或分类。\n当需要从输入序列生成输出序列时，需要使用Transformer的decoder部分。例如，当我们需要进行机器翻译或文本摘要时，可以使用Transformer的decoder部分从输入序列生成相应的输出序列。\n当需要同时进行输入序列编码和输出序列生成时，需要使用Transformer的encoder和decoder部分。例如，在对话系统中，我们需要将用户输入编码为表示，然后使用decoder生成系统的回复。\n总之，根据具体任务的要求，我们可以灵活地使用Transformer的encoder、decoder或两者结合的部分来构建模型。\n为什么用position encoding Transformer之所以使用position encoding而不是position embedding，有以下几个原因：\nposition encoding可以表示任意长度的序列，而不受限于固定的最大长度。12 position encoding可以捕捉到相对距离和绝对距离的信息，而不仅仅是一个序号。12 position encoding可以与词向量进行线性变换，从而适应不同层次的特征提取。12 当然，position embedding也有其优点，例如：\nposition embedding可以通过训练学习到更合适的位置表示，而不需要人为地设计函数形式。34 position embedding可以更容易地处理非连续或无序的序列，例如图像或树状结构。34 相对位置编码和绝对位置编码 Transformer使用的三角式绝对位置编码。BERT使用的是训练出来的绝对位置编码\n绝对位置编码是指为序列中每个位置固定地分配一个编码向量。 相对位置编码是指为序列中的每个位置分配一个相对于其他位置的编码向量。 绝对位置编码适用于序列长度较小的任务，而相对位置编码适用于序列长度较大的任务。绝对位置编码可以提供每个位置的绝对位置信息，但是由于它是固定的，无法适应不同任务和不同长度的序列。相对位置编码则可以根据不同任务和不同长度的序列自适应地学习到不同位置之间的相对位置关系，因此更加灵活和适用于更多的任务。\n位置编码还有什么 静态编码、绝对编码、相对编码、旋转编码、复数域编码、Neural ODE编码\nfeed forward 第一层的激活函数为 Relu，第二层不使用激活函数\nattention公式 $$ \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $$\nquery向量类比于询问。某个token问：“其余的token都和我有多大程度的相关呀？” key向量类比于索引。某个token说：“我把每个询问内容的回答都压缩了下装在我的key里” value向量类比于回答。某个token说：“我把我自身涵盖的信息又抽取了一层装在我的value里” dk是Q、K的维度 首先Q、K、V都源于输入特征本身，是根据输入特征产生的向量，但目前我们现在无需关注是如何产生这组向量的。\nV可以看做表示单个输入特征的向量。当我们直接把一组V输入到网络中进行训练，那这个网络就是没有引入Attention机制的网络。\n但如果引入Attention，就需要将这组V分别乘以一组权重 α ，那么就可以做到有重点性地关注输入特征，如同人的注意力一般\nattention为何选择点乘而不是加法 1. 计算效率 点乘运算可以使用高度优化的矩阵乘法算法（如BLAS库），并且可以很好地并行化，适合在GPU等硬件上高效计算。\n2. 向量相似性度量 点乘是衡量向量相似性的有效方法。对于两个向量 a 和 b，它们的点乘 a⋅b 的值与它们的夹角相关。点乘结果越大，表示两个向量越相似。这种相似性度量对计算注意力权重非常合适。\n3. 线性变换的性质 自注意力机制中的查询（Query）、键（Key）和值（Value）向量都是通过线性变换得到的。点乘操作保留了这些线性变换的性质，便于梯度计算和参数更新。\nself-attention的伪代码 https://blog.csdn.net/qq_44949041/article/details/128087174\n准备输入 import torch x = [ [1, 0, 1, 0], Input 1 [0, 2, 0, 2], Input 2 [1, 1, 1, 1] Input 3 ] x = torch.tensor(x, dtype=torch.float32) torch.rand(2,3) 初始化权重 w_key = [ [0, 0, 1], [1, 1, 0], [0, 1, 0], [1, 1, 0] ] w_query = [ [1, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 1] ] w_value = [ [0, 2, 0], [0, 3, 0], [1, 0, 3], [1, 1, 0] ] w_key = torch.tensor(w_key, dtype=torch.float32) w_query = torch.tensor(w_query, dtype=torch.float32) w_value = torch.tensor(w_value, dtype=torch.float32) 导出key, query and value的表示 keys = x @ w_key querys = x @ w_query values = x @ w_value print(keys) tensor([[0., 1., 1.], [4., 4., 0.], [2., 3., 1.]]) print(querys) tensor([[1., 0., 2.], [2., 2., 2.], [2., 1., 3.]]) print(values) tensor([[1., 2., 3.], [2., 8., 0.], [2., 6., 3.]]) 计算输入的注意力得分(attention scores) attn_scores = querys @ keys.T tensor([[ 2., 4., 4.], attention scores from Query 1 [ 4., 16., 12.], attention scores from Query 2 [ 4., 12., 10.]]) attention scores from Query 3 计算softmax from torch.nn.functional import softmax attn_scores_softmax = softmax(attn_scores, dim=-1) tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01], [6.0337e-06, 9.8201e-01, 1.7986e-02], [2.9539e-04, 8.8054e-01, 1.1917e-01]]) For readability, approximate the above as follows attn_scores_softmax = [ [0.0, 0.5, 0.5], [0.0, 1.0, 0.0], [0.0, 0.9, 0.1] ] attn_scores_softmax = torch.tensor(attn_scores_softmax) 将attention scores乘以value weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None] tensor([[[0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000]], [[1.0000, 4.0000, 0.0000], [2.0000, 8.0000, 0.0000], [1.8000, 7.2000, 0.0000]], [[1.0000, 3.0000, 1.5000], [0.0000, 0.0000, 0.0000], [0.2000, 0.6000, 0.3000]]]) 对加权后的value求和以得到输出 outputs = weighted_values.sum(dim=0) tensor([[2.0000, 7.0000, 1.5000], Output 1 [2.0000, 8.0000, 0.0000], Output 2 [2.0000, 7.8000, 0.3000]]) Output 3 self-attention的时间复杂度 Transformer的self attention机制因为要对任意两个单词做attention计算，所以时间复杂度是$n^2$，n是输入的长度。\nself-attention的优化 Linformer：论文核心贡献是发现self attention矩阵P是低秩矩阵，从而可以通过SVD构建另一个维度小的低秩矩阵P\nCCNet：论文主要思路，区别与Non-Local 中的全局attention，这篇文章提出只在特征点所对应的十字上进行attention。从而将复杂度从 O(N2) 降低到 O(N)\nself-attention有什么优点？ 可以建模长程依赖关系 传统的RNN模型在处理长序列时容易出现梯度消失或梯度爆炸的问题，而**Self-Attention模型可以捕捉序列中不同位置之间的依赖关系，从而能够更好地处理长序列数据。**由于Self-Attention模型中每个位置的表示都是通过与其他位置的表示进行加权求和得到的，因此模型能够在不同位置之间建立直接的联系，而不需要通过逐个时间步地传递信息。\n可以并行化计算 传统的RNN模型在处理长序列时需要逐个时间步地计算，因此难以实现高效的并行计算。而Self-Attention模型中的每个位置都可以同时计算，因此可以通过矩阵乘法等高效的并行计算方式来加速计算。\n可以学习不同位置之间的关系 Self-Attention模型中，每个位置的表示都是通过与其他位置的表示进行加权求和得到的，从而能够捕捉不同位置之间的关系。这种方法可以学习到不同位置之间的相互作用，例如文本中不同单词之间的关系，从而能够更好地理解文本的含义。\n可以学习多个关注点 Self-Attention模型中，通过计算Query和Key的点积得到Attention权重，可以学习到不同的关注点。这使得模型能够同时关注多个方面的信息，例如机器翻译中源语言和目标语言之间的对应关系。\nself-attention为什么要除根号dk（进行scaled） self-attention为什么要除根号dk的原因是为了防止梯度消失12。\nQ和K的内积结果随着维度变大，方差会变大，可能会出现很大的值234。 这样会导致softmax函数的饱和区间，梯度几乎为012。 除根号dk可以降低Q和K的数值，保证输出结果不会过大234。 使softmax的输入输出方差保持一致，防止梯度消失，加速收敛。\nself-attention不除根号dk有哪些替代方案 在Transformer模型中，Self-Attention层的公式中除以根号d_k是为了缩放注意力权重，使其具有更合理的尺度。如果不除以根号d_k，则可能导致注意力权重的尺度过大或过小，从而影响模型的训练效果。\n如果不想使用除以根号d_k的缩放方法，也可以使用其他的缩放方法。以下是几种常见的替代方案：\n缩放到单位范数 除了除以根号d_k之外，可以将每个Query向量、Key向量和Value向量除以其向量的范数（L2范数）来进行缩放，使它们成为单位向量。这种方法可以保证每个向量的尺度都是一致的，而且每个向量的尺度不受向量维度的影响。\n缩放到[-1, 1]或[0, 1]范围内 可以将点积的结果除以d_k，然后使用tanh或sigmoid等函数将结果缩放到[-1, 1]或[0, 1]范围内。这种方法可以使点积的结果具有一定的尺度，并且不会出现梯度消失或梯度爆炸的问题。\n缩放到常数范围内 可以将点积的结果除以一个常数（比如8），以缩放到一个较小的范围内。这种方法可以避免注意力权重过大或过小的问题，同时也可以使模型的训练更加稳定。\n总之，只要能做到每层参数的梯度保持在训练敏感的范围内，不要太大，不要太小就行了。\nMask！！！ 在Transformer中，由于自注意力机制（self-attention mechanism）的存在，每个词都要与其它所有词计算相似度得到加权和作为其新的表示。如果不考虑mask，模型将会把当前位置之后的词也考虑进来，这样就泄露了未来信息，导致模型在预测时出现了信息泄露的问题。\n在Transformer中，有两种常见的mask，分别是padding mask和sequence mask。\npadding mask用于将填充的位置掩盖掉：保证句子的长度一样所以需要进行填充。 **sequence mask用于将当前位置之后的位置掩盖掉。**这样，在进行self-attention计算时，模型就不会考虑掩盖的位置，从而保证了模型的正确性。 self-attention怎么处理padding self-attention处理padding的一种方法是使用mask1。\nmask是一个二维矩阵，用来表示哪些位置是padding，哪些位置是有效的1。 mask的值为0或1，0表示padding，1表示有效1。 在计算attention时，将mask与点积结果相加，使得padding位置的值变为负无穷1。 这样在softmax时，padding位置的权重就会变为0，不影响有效位置的权重1。 总结：使padding位置的值变为负无穷，这样softmax之后padding位置的权重就会变为0，不影响有效位置的权重。\n不同batch可以padding到不同长度吗 同一个batch内要padding到一样长度，不同batch之间可以不一样\n一般batch个句子经过embedding层之后，它的维度会变成 [batch_size, seq_length, embedding_size]，\n不同batch之间padding的长度可以不一样，是因为神经网络模型的参数与 seq_length 这个维度并不相关。如对于RNN/LSTM 等循环神经网络，seq_length 这个维度只决定 RNN/LSTM 循环计算多少步，而不会影响模型的参数。对于Transformer，输入batch个句子中的每一个token，都是乘以同一个 Q K V参数矩阵，再去做 Self-Attention ，所以参数量和参数维度都是和 seq_length 无关的。\nmulti-head attention 优势 Multi-head attention是一种注意力机制，它可以让模型同时关注不同位置和不同表示子空间的信息12。它的优势有：\n扩展了模型专注于不同位置的能力，比如解决代词指代的问题3。\n为注意力层提供了多个表示子空间，增加了模型的表达能力和泛化能力12。\n可以并行计算多个注意力头，提高了计算效率4。\n由于神经网络的参数可以表示为在高维空间的稀疏矩阵，而高维的稀疏矩阵是可以分解为多个低维的稠密矩阵\n如果不采用多头注意力机制的话，万一注意力矩阵中出现了某个极大值，再经过softmax就会使得网络只关注到极大值处的特征，而忽略了其它处的特征。多头注意力机制则可以缓解这个问题，因为将原本的特征拆分成多个头一个，每个样本可以关注到的特征变丰富了，不会因为某个头内的注意力矩阵出现了极大值而忽略了其它特征。（简单点理解就是原本的attention只是一个attention score的计算， 而转换成多头之后变成了多个attention score的叠加，减小了模型的方差，类似于bagging\n并行化怎么体现 Encoder侧：模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。\nDecode引入sequence mask就是为了并行化训练，Decoder推理过程没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。\nmulti-head attention怎么并行计算 ​\tmulti-head attention怎么并行计算的原理是12：\n将Q，K，V分别经过一个线性层，再分解为h个头12。 每个头分别计算自己的attention向量12。 将h个attention向量拼接起来，再经过一个线性层输出12。 multi-head attention为什么需要对每个head进行降维 将原有的高维空间转化为多个低维空间并再最后进行拼接，形成同样维度的输出，借此丰富特性信息\nqkv矩阵可以一样吗 在通常的注意力机制中，Q (query)、K (key)、V (value) 三个矩阵都是用于计算注意力权重和上下文向量的重要参数。\n理论上来说，Q、K、V 三个矩阵可以是相同的，但通常情况下它们是不同的矩阵。如果这三个矩阵相同，表示所有的输入都使用相同的权重计算注意力向量。这种情况下，模型可能会更加简单，但是可能会限制模型的表达能力，从而导致模型的性能不佳。\n因此，一般来说，为了使模型具有更强的表达能力，同时避免过多的参数，通常会使用不同的矩阵来计算 Q、K、V。这也是在 Transformer 中使用了多头注意力机制的原因之一，通过多个 Q、K、V 矩阵的计算，可以让模型学习到更加丰富的特征。\nqkv可以加激活吗 一般情况下，qkv的线性变换不需要加激活函数，因为它们的输出是用于计算注意力权重和输出的中间结果，不同于神经网络中的隐藏层输出。\n在Transformer中，当需要加入激活函数时，一般是在Multi-Head Attention的输出上进行操作，而不是在qkv的线性变换上进行操作。例如，在Transformer Decoder中，Multi-Head Attention的输出会与解码器的输入进行拼接后再通过一个前馈神经网络（Feed-Forward Network）进行处理，在前馈神经网络中可以加入激活函数。\n总的来说，在Transformer等模型中，qkv的线性变换不需要加激活函数，而具体的激活函数一般是在后续的网络层中进行添加的。\n两个词相互attention结果一样吗 在一般的Attention机制中，一个词所对应的Query向量和另一个词所对应的Key向量是不同的，因此a对b做attention和b对a做attention的值一般是不同的。\n具体地说，设$a$和$b$分别为输入句子中的两个词，$q_a$和$q_b$分别为它们对应的Query向量，$k_a$和$k_b$分别为它们对应的Key向量，$v$为Value向量，则a对b做attention的值为：\n$$\\text{Attention}(q_a, k_b, v) = \\frac{\\exp(q_a k_b^\\top)}{\\sum_{j=1}^n \\exp(q_a k_j^\\top)} v_b$$\n而b对a做attention的值为：\n$$\\text{Attention}(q_b, k_a, v) = \\frac{\\exp(q_b k_a^\\top)}{\\sum_{j=1}^n \\exp(q_b k_j^\\top)} v_a$$\n其中$n$为输入句子的长度。因为$q_a$和$q_b$、$k_a$和$k_b$是不同的向量，所以两个Attention的值一般是不同的。\nBahdanau Attention和 Luong Attenion的区别 Bahdanau Attention计算过程\n前一次的隐藏状态和encoder的output match计算得到attention weight attention weight 和 encoder output计算得到context vector context vector作为当前时间步的输入，同时当前时间步的输入还有前一次的隐藏状态 得到当前时间步的输出和隐藏状态 Luong Attenion计算过程\nGRU计算得到的decoder的hidden_state(若没有则可以初始化一个) hidden state 和encoder的hidden计算得到a_t(attention weight) attention weight 和 encoder 的output计算得到context vector context vector 和 GRU的当前时间步的output合并计算得到最终的输出 交叉熵的预测值p是怎么来的 cls降维，768维降到1维。\n残差和LN的作用 防止梯度消失，加速收敛\nTransformer中有哪些结构缓解梯度消失和爆炸 残差连接、层归一化\n样本不均衡/长尾分布 Focal Loss、过/欠采样、以搜代分、词典匹配、Prompt\n欠/过采样 随机过采样（Random Oversampling）：在正例样本中随机选择一些样本进行复制。\nSMOTE算法（Synthetic Minority Over-sampling Technique）：在正例样本中，对于每个样本，选择最近的K个正例样本，并随机从这K个样本中选择一个，根据正例样本与其最近邻的差值向量，按照一定比例生成新的正例样本。\nADASYN算法（Adaptive Synthetic Sampling）：在SMOTE算法的基础上，考虑到在样本分布不均匀的情况下，一些正例样本可能比其他正例样本更容易被分类错误，因此对每个正例样本增加的数量是不同的，并且与其最近邻的负例样本的数量成正比。\nFocal Loss Focal loss是基于二分类交叉熵CE的。它是一个动态缩放的交叉熵损失，通过一个动态缩放因子，可以动态降低训练过程中易区分样本的权重，从而将重心快速聚焦在那些难区分的样本上。 $$ \\text{FL}(p_t) = -\\alpha_t (1-p_t)^\\gamma \\log(p_t) $$ 引入了一个权重因子α ∈ [ 0 , 1 ] ，当为正样本时，权重因子就是α，当为负样本时，权重因子为1-α。\ngamma是为一个参数，范围在 [0,5]。当Pt趋向于1，即说明该样本是易区分样本，此时调制因子 是趋向于0，即减低了易区分样本的损失比例。\n**多分类focal loss：**权重系数α维护成一个列表，三分类中第0类权重0.2，第1类权重0.3，第2类权重0.5\n过拟合 正则化、增加训练数据、数据增强、标签平滑、BatchNorm、Early-Stop、交叉验证、Dropout、Pre-trained、引入先验知识\nDropout Dropout说的简单一点就是：我们在前向传播的时候，随机丢弃掉一些神经元（让某个神经元的激活值以一定的概率p停止工作），让它不会太依赖某些局部的特征，使模型泛化性更强。\n在进行Dropout操作时，每个神经元的输出被置为0的概率为p，保留的概率为1-p。因此，**为了保持网络输出的期望不变，需要在训练过程中对Dropout层的输出进行rescale，即将输出值乘以1/(1-p)。**这样，即使某些神经元被Dropout，网络的输出仍能保持期望不变。\n需要注意的是，当进行模型推理时，不需要进行Dropout操作，因此也不需要进行rescale，不然得到的就是一个随机的结果了。在模型推理时，需要将Dropout层的保留概率设置为1，即保留所有神经元的输出。\n在什么地方做Dropout Dropout是一种在神经网络中用于减少过拟合的正则化技术，它随机地将一些神经元输出设置为零，这样可以减少神经元之间的依赖性，从而减少模型的复杂度。在训练过程中，Dropout应该在神经网络的隐藏层上进行，而不是在输入层或输出层上进行，而且是在激活函数之前。\n通常，Dropout会在神经网络的每个隐藏层中进行。在实践中，一般将Dropout放置在卷积层和全连接层之间。在卷积层后面的Dropout有助于减少特征图之间的依赖性，而在全连接层之前的Dropout有助于减少神经元之间的依赖性。\n需要注意的是，Dropout在训练过程中起作用，而在测试过程中不会起作用，因此在测试时不需要使用Dropout。在测试过程中，可以通过缩放每个神经元的输出来近似Dropout的效果，通常使用的比例为0.5。\n为什么在激活函数之后 在神经网络的Dropout层一般放在激活函数之后。这是因为Dropout是一种正则化方法，它的作用是随机地让一部分神经元的输出值变为0，从而减少了神经元之间的相互依赖性，防止过拟合。如果将Dropout放在激活函数之前，那么就会使得神经元输出值为0的概率变成一个常数，这样就会破坏掉激活函数的非线性特性，从而影响模型的表达能力。\n优化器 为什么采用梯度下降来进行优化 梯度下降是一种常用的数值优化算法，被广泛应用于机器学习和深度学习中。其优点如下：\n高效：梯度下降算法的计算量相对较小，能够快速收敛到局部最优解。 可扩展性：梯度下降算法可以应用于大规模数据集，支持并行化处理。 可解释性：梯度下降算法能够生成梯度信息，可以帮助我们理解模型的行为和优化过程。 通用性：梯度下降算法是一种通用的优化方法，可以用于各种模型的训练和参数优化。 梯度下降算法的核心思想是利用函数的梯度信息来不断更新模型参数，以达到最小化损失函数的目的。虽然梯度下降算法可能会收敛到局部最优解，但是在实际应用中，它仍然是一种非常有效和可靠的优化方法。\nSGD 梯度下降，BGD整个数据集计算平均梯度（计算所有数据的梯度非常耗时），SGD随机抽样一个样本的梯度更新（很快，但随机性高，噪声影响严重，不一定向整体最优点下降），miniGD随机选一个小batch的样本计算平均梯度更新（得到的梯度下降方向是局部最优的，整体速度快）\nAdam SGD下降方法的缺点是参数更新方向只依赖于当前batch计算出的梯度，因此十分的不稳定。为了抑制SGD的震荡，引入了动量，认为梯度下降的过程中可以加入惯性。Adam的出现就自然而然了，它是前述方法的集大成者。动量在SGD的基础上增加了一阶动量，AdaGrad和AdaDelta在SGD的基础上增加了二阶动量。Adam实际上就是将Momentum和RMSprop集合在一起，把一阶动量和二阶动量都使用起来了\n1.一阶动量：表示参数梯度的一阶矩估计，它类似于SGD中的动量项。它的作用是累积过去梯度的方向信息，从而更好地控制参数更新的方向和速度。\n2.二阶动量：表示参数梯度的二阶矩估计，它可以理解为对梯度的方差进行估计。它的作用是调整每个参数的学习率，从而更好地控制参数更新的步长。\nAdam-W Adamw 即 Adam + weight decate（权重衰减） ,效果与 Adam + L2正则相同,但是计算效率更高,因为L2正则化需要在loss中加入正则项,之后再算梯度,最后在反向传播,而Adamw直接将正则项的梯度加入反向传播的公式中,省去了手动在loss中加正则项这一步\n评价指标 （Acc、Precision、Recall、F1、ROC、AUC、代码实现AUC）\nAcc、Precision、Recall 这些指标是评估二分类或多分类模型性能的常见指标，下面是它们的计算方法：\n假设我们有一个二分类问题，其中有两个类别，分别为正类（positive）和负类（negative）。\n准确率（Accuracy）：是分类正确的样本数与总样本数之比，表示模型分类的准确程度。\nAccuracy=TP+TN/TP+TN+FP+FN\n其中，TP（True Positive）表示真正类，即被正确地预测为正类的样本数；TN（True Negative）表示真负类，即被正确地预测为负类的样本数；FP（False Positive）表示假正类，即被错误地预测为正类的样本数；FN（False Negative）表示假负类，即被错误地预测为负类的样本数。\n精确率（Precision）：表示预测为正类的样本中真正为正类的比例，是衡量模型的预测精度的指标。\nPrecision=TP/TP+FP\n召回率（Recall）：表示真正为正类的样本中被正确预测为正类的比例，是衡量模型识别正例的能力的指标。\nRecall=TP/TP+FN\nF1值（F1 score）：综合了精确率和召回率，是一个综合指标，用于平衡模型的精度和召回率。\nF1=2×（Precision×Recall）/（Precision+Recall）\n其中，精确率和召回率越高，模型的性能就越好，F1值也就越高。\nNDCG NDCG（Normalized Discounted Cumulative Gain）是一种用于评估推荐系统排序性能的指标，它是DCG（Discounted Cumulative Gain）的归一化版本。NDCG可以衡量推荐系统在不同的位置推荐的物品对用户的吸引力程度。\n在计算NDCG之前，我们首先需要计算DCG。DCG的计算方法如下：\n假设我们有一个用户历史上的行为序列和一个推荐列表。 首先计算每个物品的相关度得分 $r_i$。可以根据用户历史行为数据，给已经发生过的行为打分，例如点击、购买等。 推荐列表中第 $$i$$ 个物品的得分为 $r_i$。 根据推荐列表中的顺序计算每个物品的排名权重 $w_i$，其中第 $$i$$ 个物品的排名权重为 $w_i = \\frac{1}{\\log_2(i+1)}$。这个公式表示排名越靠前的物品权重越大。 计算DCG，即对于前 $$k$$ 个推荐物品，计算它们的加权得分之和，其中 $$k$$ 通常是推荐列表的长度。具体来说，DCG 的计算公式为： $$DCG_k = \\sum_{i=1}^k \\frac{r_i}{\\log_2(i+1)}$$\n计算NDCG，即将DCG归一化，具体做法是将DCG除以理论最优DCG。理论最优DCG是将推荐列表中的物品按照相关度得分从高到低排序后的DCG值，即 ROC ROC曲线是一种常用的评价二分类模型性能的方法，其全称为\"Receiver Operating Characteristic curve\"，中文名为“受试者工作特征曲线”。\nROC曲线是一条图形化的曲线，其横轴代表假正率（False Positive Rate，FPR），纵轴代表真正率（True Positive Rate，TPR），在二分类问题中，真正率表示正确预测为正例的样本数占所有正例样本的比例，假正率则表示错误预测为正例的样本数占所有负例样本的比例。\nROC曲线是通过改变分类器的阈值来绘制的，当阈值从最大值开始逐渐降低时，分类器会将一些本来被分类为负例的样本改为正例，从而增加真正率，但也会增加假正率，ROC曲线反映了这种折衷关系。理想情况下，ROC曲线应该尽可能地靠近左上角，此时真正率高、假正率低，表示分类器的性能更优。\n在ROC曲线上，每一个点对应了一个不同的阈值下的真正率和假正率，曲线下方的面积（Area Under the Curve，AUC）则是ROC曲线的总体性能指标，AUC的取值范围在0-1之间，越接近1表示分类器性能越好。\nAUC 在ROC曲线上，每一个点对应了一个不同的阈值下的真正率和假正率，曲线下方的面积（Area Under the Curve，AUC）则是ROC曲线的总体性能指标，AUC的取值范围在0-1之间，越接近1表示分类器性能越好。\nAUC不受正负样本比例的影响，对样本不均衡的问题鲁棒。\n原因：\n在很多排序场景下，尤其是当前许多数据集正负样例都不太均衡；或者说因训练集过大，可能会对数据进行负采样等操作。这擦操作的前提是建立在AUC值不会受到正负样本比例的影响。看过很多博客也都在讨论：为什么AUC不会受正负样例不平衡的影响？为什么排序喜欢选择AUC作为评判指标。\nAUC不受正负样本比例影响的核心原因在于它评估的是模型对正负样本的相对排序能力，而不是具体的分类结果。TPR和FPR在不同阈值下的变化构成了ROC曲线，AUC通过计算这条曲线下的面积来反映模型性能。因此，无论正负样本的比例如何变化，只要模型对样本的排序能力不变，AUC值也不会显著变化。这使得AUC在处理不均衡数据和排序任务中尤为适用。\nAUC的计算 https://zhuanlan.zhihu.com/p/462734871\nAUC（Area Under the Curve）通常用于评估二分类模型的性能，其计算涉及ROC曲线的构建及其下方面积的求解。计算AUC的步骤如下：\n1. 计算ROC曲线 ROC曲线是通过遍历不同的阈值来绘制的，反映模型的真正率（True Positive Rate, TPR）和假正率（False Positive Rate, FPR）之间的关系。\n计算TPR和FPR： 真正率（TPR）: 在不同阈值下，所有正样本中被正确识别为正样本的比例： $$ \\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n其中，TP是True Positives，FN是False Negatives。\n假正率（FPR）: 在不同阈值下，所有负样本中被错误识别为正样本的比例： $$ \\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} $$ 其中，FP是False Positives，TN是True Negatives。\nAUC的计算过程涉及：\n根据模型的预测概率对样本排序。 在不同阈值下计算TPR和FPR。 使用梯形法则计算ROC曲线下的面积，得到AUC。 AUC值介于0到1之间，越接近1表示分类器性能越好。AUC是一个衡量模型整体性能的有效指标，特别适用于样本不均衡的情况。\nROC和PR曲线 AUC是ROC曲线下方区域面积。（PR曲线下的面积叫做AP即Average Precision）。这里想先对比ROC和PR曲线。\nROC曲线：\n- 横轴是FPR（负类样本中被判定为正类的比例)\n- 纵轴是TPR（recall）（正类样本中被判定为正类的样本）\nPR曲线：\n- 横轴是**召回率（Recall）**查全率（TPR）。即正确预测为正的占全部实际为正的比例\n- 纵轴是**精确率（Precision）查准率、**灵敏性（Sensitivity）。即正确预测为正的占全部预测为正的比例\n一些性质：\n如果一条曲线在ROC曲线中压过另一条曲线，那么他在PR曲线中也会相同的全面优于另一条曲线 （挂枝儿：ROC曲线与PR曲线的关系）\n如果一条曲线在PR曲线中压过另一条曲线，那么他在ROC曲线中也会相同的全面优于另一条曲线，通过下图应该能比较好的理解 （挂枝儿：ROC曲线与PR曲线的关系）\n一个好的模型在ROC图上的表现一定是偏左上角的，而在PR曲线中一定是偏右上角的。在同一个数据集下。（挂枝儿：ROC曲线与PR曲线的关系）\n由上面的公式可知，ROC曲线不在意真实正类的概率，而PR曲线中的precision会关注“真实的正类”，所以更适合于大量负类、少量正类这种“大海捞针”问题。（参考：ROC vs precision-and-recall curves）\nROC曲线的估计过于乐观（对负例错判为正例不敏感）。这是因为ROC横轴是FPR，而FPR对于负样本的数量变化不敏感。当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。（参考：wdmad：机器学习之类别不平衡问题 (2) —— ROC和PR曲线）\n当测试集中的**正负样本的分布变化的时候，ROC曲线能够保持不变。**P-R曲线会发生明显变化。因此ROC曲线能够尽可能地降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。\nMRR MRR（Mean Reciprocal Rank，平均倒数排名）是用来评价信息检索系统性能的常见指标。\nMRR计算方法如下：\n对于每个查询，将检索结果按照相关性从高到低排序。 找到排名第一个相关的结果，将其排名的倒数作为该查询的得分。 对所有查询的得分求平均，即为MRR。 公式表示为：MRR = (1/N) * ∑(1/Ranki)，其中N为查询数，Ranki为第i个查询的相关结果排名。\nMAP MAP（Mean Average Precision，平均精度均值）是用来评价信息检索系统性能的常见指标。\nMAP计算方法如下：\n对于每个查询，将检索结果按照相关性从高到低排序。 对于每个相关的结果，计算其累计精度（Cumulative Precision），即前面所有相关结果的精度的平均值。 对所有查询的累计精度求平均，即为MAP。 公式表示为：MAP = (1/N) * ∑(APi)，其中N为查询数，APi为第i个查询的平均精度。\n在实际计算中，为了处理无相关结果或相关结果数不足的情况，可以采用微调参数或插值等方法进行调整。\nARPU 每用户平均收入\n“**ARPU即每用户平均收入。**用于衡量电信运营商和互联网公司业务收入的指标。 ARPU注重的是一个时间段内运营商从每个用户所得到的收入。\n学习率 （衰减、warmup、自适应、平时自己使用的时候对lr有什么调整心得吗）\nwarmup 由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。\nlr 2e-5, 1e-5, 5e-6\n常见的学习率有以下几种：\n固定学习率（Fixed Learning Rate）：在整个训练过程中，学习率保持不变，例如常见的学习率0.1、0.01、0.001等。 学习率衰减（Learning Rate Decay）：在训练过程中逐渐降低学习率，例如学习率按照指数或线性函数进行衰减。 周期性学习率（Cyclic Learning Rate）：将学习率在一定区间内周期性地变化，例如将学习率在0.01到0.1之间进行循环。 自适应学习率（Adaptive Learning Rate）：根据模型参数的梯度变化情况自适应地调整学习率大小，例如Adagrad、RMSProp、Adam等。 学习率预热（Learning Rate Warmup）：在训练初期，将学习率逐渐增加到设定值，可以帮助模型更快地找到合适的方向，避免陷入局部最优解。 不同的学习率调节方法适用于不同的场景和模型，需要根据具体情况选择合适的方法和参数。在实际使用中，通常需要进行学习率的调参，以达到更好的模型性能。\n激活函数 作用 激活函数是一种数学函数，通常用于神经网络的非线性变换，将神经元的输入转换为输出。它们在神经网络中起到了非常重要的作用，主要有以下几个方面：\n引入非线性：激活函数将神经元的输入进行非线性变换，从而使神经网络能够学习到非线性的复杂函数关系。如果没有激活函数，多个线性变换的组合仍然是一个线性变换，那么神经网络的表达能力会非常有限。 改善模型的鲁棒性：激活函数能够使神经网络对输入数据的微小变化更加敏感，从而使得模型更加鲁棒，能够处理多样性的输入数据。 压缩数据：一些激活函数，例如sigmoid函数，能够将神经元的输出压缩到一个有限的范围内，使得模型的输出也保持在有限范围内。这对于一些需要输出概率值的任务非常有用。 防止梯度消失：在深度神经网络中，如果使用线性变换作为激活函数，那么在反向传播时，梯度会以指数级别逐渐消失。而使用一些非线性激活函数，如ReLU，能够避免这种情况的发生，从而更好地训练深度神经网络。 总之，激活函数是神经网络中不可或缺的组成部分，它们为神经网络引入了非线性，提高了模型的表达能力和鲁棒性，同时避免了梯度消失等问题，为深度学习的发展做出了巨大贡献。\n常见的激活函数有以下几种：\nSigmoid函数：将输入映射到0到1之间，具有平滑的导数，但容易出现梯度消失的问题。\nTanh函数：将输入映射到-1到1之间，也具有平滑的导数，但容易出现梯度消失的问题。\nReLU函数：在输入为正时返回输入，否则返回0，避免了梯度消失的问题，但容易出现神经元死亡的问题。\nLeakyReLU函数：在输入为负时返回一个小的斜率，从而避免神经元死亡的问题。\nELU函数：在输入为负时返回一个指数级别的值，从而避免神经元死亡的问题。\nGELU 高斯误差线性单元：激活函数GELU的灵感来源于 relu 和 dropout，在激活中引入了随机正则的思想。gelu通过输入自身的概率分布情况，决定抛弃还是保留当前的神经元。似乎是 NLP 领域的当前最佳；尤其在 Transformer 模型中表现最好,能避免梯度消失问题。\nSwish函数：在ReLU基础上增加了一个sigmoid函数，可以获得更好的性能。\n不同的激活函数适用于不同的场景和模型，需要根据具体情况选择合适的激活函数。其中，ReLU是目前最常用的激活函数之一，因为它简单高效，并且在实践中表现良好。但是，ReLU也存在一些缺点，例如神经元死亡的问题。其他激活函数的优缺点如下：\nSigmoid和Tanh函数的优点在于它们可以将输入映射到0到1或-1到1之间，因此适用于需要对输入进行归一化的场景。但是，它们容易出现梯度消失的问题，从而导致训练过程缓慢或无法收敛。 LeakyReLU和ELU函数的优点在于它们可以避免神经元死亡的问题，并且相对ReLU函数而言，有更平滑的导数。但是，它们的计算成本较高，需要更多的计算资源。 Swish函数的优点在于它可以获得更好的性能，但是它的计算成本也较高。 因此，在选择激活函数时，需要综合考虑模型的性能、计算成本等因素，并根据具体情况进行选择。\n如何等概率地从n个样本中采样m个 每个位置i以 (m - k) / (n - i - 1) 的概率决定当前数是否选，k为前面已经抽出的数的个数。\nPre-LN和Post-LN 可以，这种方法被称为Pre-LN，它在warm-up阶段对超参数的变化更不敏感，甚至不需要warm-up，并且它在各层之间的梯度范式是几乎不变的，因此收敛速度很快。\n训练Transformer初始阶段中，过大的学习率会出现梯度爆炸的情况，这里梯度爆炸主要是由于在LN层之间连接了残差模块，导致在训练初始阶段输出层附近期望梯度非常大；所以，一般先采用极小的学习率进行预热（即warm-up），然后在逐渐增大学习率。由于warm-up阶段采用的学习率非常小，导致了Post-LN Transformer训练时间增加；与此同时，warm-up阶段对最终结果也有着很大的影响。\n该论文中，说明了warm-up阶段存在的必要性**，指出其根源在于LN层的位置导致层次梯度范数增长，进而导致了Post-LN Transformer的不稳定性。**根据理论和分析，Layer Normalization被放置在残差模块之间时，会导致接近输出层的期望梯度变大，如果舍弃warm-up阶段，直接采用较大的学习率进行迭代，不能有效的提升模型，优化阶段容易震荡。Layer Normalization的位置，对于控制梯度范围有着很重要的作用。\n在之前的研究中，Pre-LN Transformer结构已经被尝试用于不同的系统中，表明了随着堆叠层数的增加，Pre-LN Transformer的效果要优于Post-LN。论文中用实验和理论论证了该结构不需warm-up阶段，依然达到了良好的结果。Pre-LN Transformer模型的结构图如图1，主要区别是LN层放置在了residual连接中。\n一般认为，Post-Norm在残差之后做归一化，对参数正则化的效果更强，进而模型的收敛性也会更好；而Pre-Norm有一部分参数直接加在了后面，没有对这部分参数进行正则化，可以在反向时防止梯度爆炸或者梯度消失，大模型的训练难度大，因而使用Pre-Norm较多。\n学习率预热的原因 由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。\n词向量 word2Vec Word2Vec是一种广泛应用于自然语言处理领域的词向量表示模型。**它通过将语料库中的单词映射到高维空间中的向量，从而捕捉到了单词之间的语义和语法关系。**Word2Vec模型由Google的Tomas Mikolov等人于2013年提出，它基于神经网络模型，可分为两种算法：连续词袋模型（Continuous Bag of Words，CBOW）和Skip-gram模型。这两种模型的核心思想都是基于上下文来预测中心词或上下文词。具体而言，CBOW模型是通过给定上下文中的词来预测中心词，而Skip-gram模型则是通过给定中心词来预测上下文中的词。模型训练完成后，可以使用这些词向量进行语义分析、情感分析、文本分类、聚类等任务，这些任务不仅能够提高模型的准确性，而且也能够降低计算复杂度。\n普通的CBOW模型通过上下文来预测中心词，抛弃了词序信息，注意不同的论文有不同的描述，但本质都是一样的。\n词向量： 模型输入的是One-Hot Vector，隐藏层并没有激活函数，只是线性的单元，输入层维度和输出层维度是一样的，使用的是Softmax回归，模型训练完成后，我们需要的是这个模型通过训练数据所学到的参数。而这些参数，就是输入x的向量化表示，这个向量便是词向量。\nSkip-Gram 在Skip-gram模型中，输入是一个中心词，而输出则是预测在给定上下文中可能出现的词。具体地，Skip-gram模型的训练过程可以分为以下几步：\n对于每一个词，找到它在语料库中的上下文词汇集合（窗口大小为N，即上下文包含当前词左右各N个词）。 将每个上下文词表示成一个独热编码向量（one-hot vector），这个向量的维度等于语料库中单词的个数，只有当前词对应的位置上的值为1，其它位置上的值均为0。 将中心词表示成一个向量（也称为输入向量），这个向量的维度与上下文词汇集合的总数相等。 使用神经网络对输入向量进行处理，得到一个输出向量，这个输出向量的维度也与上下文词汇集合的总数相等。 将输出向量通过softmax函数，将其转化为概率分布，用于预测每个上下文词在当前中心词下的概率。 使用交叉熵损失函数来度量预测结果与真实结果之间的差距，然后使用梯度下降算法来更新模型参数。 Skip-gram模型的核心思想是通过训练来学习每个单词的词向量，并且在学习的过程中捕捉到单词之间的语义和语法关系。在训练过程中，每个单词的词向量在不断地被调整，以便使得这个单词在给定上下文中出现的概率最大化。这样，训练结束后，每个单词都会得到一个向量表示，这个向量的维度很小，但是却能够反映出这个单词的语义和语法特征。\nCBOW CBOW模型的输入是一个上下文窗口中的多个单词，而输出则是这些单词的平均值，这个平均值可以表示为中心词的概率分布。CBOW模型的训练过程可以分为以下几个步骤：\n对于每一个词，找到它在语料库中的上下文词汇集合（窗口大小为N，即上下文包含当前词左右各N个词）。 将每个上下文词表示成一个独热编码向量（one-hot vector），这个向量的维度等于语料库中单词的个数，只有当前词对应的位置上的值为1，其它位置上的值均为0。 将所有的上下文词的独热编码向量加起来，得到一个上下文向量，这个向量的维度等于上下文词汇集合的总数。 将上下文向量输入到一个神经网络中，神经网络的隐层节点数为m，输出层的节点数为语料库中单词的个数。 神经网络的输出层使用softmax函数，将上下文向量映射为语料库中每个单词的概率分布，表示在给定上下文的情况下，每个单词出现的概率。 使用交叉熵损失函数来度量预测结果与真实结果之间的差距，然后使用梯度下降算法来更新模型参数。 CBOW模型的训练过程是一个监督学习的过程，模型通过最小化交叉熵损失函数来调整模型参数，以便使得模型能够尽可能地准确地预测中心词。CBOW模型的优点是训练速度快，对生僻词有更好的处理能力，但是对于频繁出现的词汇，可能无法捕捉到其复杂的语义信息。\n**流程：**输入为上下文单词的one-hot编码，之后乘以共享的输入权重矩阵W，所得到的向量相加求平均作为隐层向量，乘以输出权重矩阵W’，得到的向量经过激活函数的处理得到一个V维的概率分布，其中每一维代表的是一个单词，概率最大的是预测出来的中间词。\n使用梯度下降来更新W和W’。训练完毕后，输入层的每个单词与矩阵W相乘得到的向量就是word embedding。（已知周围词预测中心词）\nWord2vec的softmax softmax需要对每个词语都计算输出概率，并进行归一化，计算量很大；\nword2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。\n1）用huffman编码做词表示 2）把N分类变成了log(N)个2分类。 如要预测的term（足球）的编码长度为4，则可以把预测为’足球’，转换为4次二分类问题，在每个二分类上用二元逻辑回归的方法（sigmoid）； 3）逻辑回归的二分类中，sigmoid函数导数有很好的性质，(\\sigma^{’}(x) = \\sigma(x)(1-\\sigma(x))) 4）采用随机梯度上升求解二分类，每计算一个样本更新一次误差函数 和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，**根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。**在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为\"Hierarchical\nSoftmax\"，将N分类问题转换为logN次二分类问题。\nword2vec ELMo BERT有哪些区别 word2vec得到的是静态词向量，同一个词在不同文章对应一个词向量；ELMo采用双层双向LSTM，将前向的结果和后向的结果进行拼接，最大化似然概率，看似解决了一词多义的问题，但其实是假的双向建模，前向和后向无法并行，而且会自己看到自己；而bert采用MLM实现真正的双向encoding，用transformer做encoder，可以有更深的层数、具有更好并行性，并且线性的Transformer比lstm更易免受mask标记影响，只需要通过self-attention减小mask标记权重即可，而lstm类似黑盒模型，很难确定其内部对于mask标记的处理方式。\nhttps://mp.weixin.qq.com/s?__biz=Mzk0MzIzODM5MA==\u0026mid=2247485335\u0026idx=1\u0026sn=2ee9291bb5059a74ec4b9b8e1275b276\u0026chksm=c337ba0ef4403318c36a8a3674116a4619a794312dac52ab8d6e8d6a11973cdcb2c1071c87b9rd\n排序算法及其时间复杂度 冒泡排序（Bubble Sort）：O(n^2) 选择排序（Selection Sort）：O(n^2) 插入排序（Insertion Sort）：O(n^2) 快速排序（Quick Sort）：O(nlogn) 归并排序（Merge Sort）：O(nlogn) 堆排序（Heap Sort）：O(nlogn) 计数排序（Counting Sort）：O(n+k)，其中 k 为数据范围 桶排序（Bucket Sort）：O(n+k)，其中 k 为桶的数量 基数排序（Radix Sort）：O(nk)，其中 k 为最大数的位数 Pointwise、pairwise、listwise pointwise把排序问题当成2分类问题，训练样本形式就是\u003c查询，文档，标签\u003e，训练目标是最小化交叉熵损失，优点是实现简单、训练集构造简单，缺点是没有考虑文档之间的相对顺序；pairwise是对每一个数据样本做一个比较关系，当一个文档比另一个文档相关排序更靠前的话，就是正例，否则便是负例，相比pointwise多了文档间的顺序关系，但是也只考虑了两个文档的先后顺序，且没有考虑文档在结果列表中出现的位置，而且对于训练数据来说，不同的查询，相关文档数量差异很大，转换为文档对之后，有的查询可能有几百对文档，有的可能只有几十个，这样不加均一化地在一起学习，模型会优先考虑文档对数量多的查询，带来偏差；listwise的基本思路是直接优化结果列表，或者说直接优化 NDCG这样的指标，从而能够学习到最佳排序结果，缺点就是训练复杂度很高。\n在信息检索和排序任务中，常见的排序方法有Pointwise、Pairwise和Listwise三种，它们在处理排序问题时各有不同的策略和优势。\nPointwise方法将排序问题视为独立的二分类或回归问题。每个训练样本的形式为$\\langle\\text{查询}, \\text{文档}, \\text{标签}\\rangle$，训练目标是最小化交叉熵损失。这种方法实现简单，训练数据构造容易。然而，它没有考虑文档之间的相对顺序，只考虑每个文档的独立评分。\nPairwise方法处理每对文档的相对关系，每对样本的形式为$\\langle\\text{查询}, \\text{文档}_i, \\text{文档}_j\\rangle$，标签表示哪个文档更相关。训练目标是最小化每对文档的顺序错误。相对于Pointwise方法，Pairwise方法考虑了文档之间的相对顺序，提升了排序性能，但它只考虑了两个文档的顺序关系，未考虑文档在列表中的具体位置。此外，不同查询的相关文档数量不均衡，可能引入偏差。\n在Pairwise方法中，常见的损失函数包括Hinge Loss和RankNet Loss。Hinge Loss通过公式$\\text{loss} = \\max(0, \\text{margin} - (S(q, d^+) - S(q, d^-)))$计算，其中(S(q, d^+))和(S(q, d^-))分别是文档的相关性得分。RankNet Loss则利用交叉熵损失计算两个文档的顺序关系，对于两个文档 (i) 和 (j)，相关性得分为$ s_i $和$ s_j$：\n若 $$ P_{ij} = \\frac{1}{1 + e^{-(s_i - s_j)}} $$ (i) 比 (j) 更相关，则 (P_{ij} \u003e 0.5)，反之 (P_{ij} \u003c 0.5)。\nPairwise方法的优化策略包括调节不同难度的pair（如为高差异和低差异的pair设置不同的margin值）、调整正负样本的采样比例以避免样本不平衡对模型的影响（通过均衡采样或加权实现），以及在训练过程中有针对性地挖掘那些难以区分的负样本以提高模型的鲁棒性。\nListwise方法直接优化结果列表或像NDCG这样的评价指标，考虑了整个文档列表的排序关系，能学习到更全局的最佳排序结果。虽然它能进一步提升排序效果，但也带来了更高的训练复杂度。LambdaRank方法在RankNet的基础上考虑了信息检索中的评价指标，如NDCG，由于这些指标不可导或导数不存在，直接简单地在RankNet的梯度上再乘一项新定义的Lambda梯度，以更关注top k个结果的排序。\n综上所述，Pairwise方法通过考虑文档间的相对顺序，能有效提高排序性能，但也需要通过调节margin、均衡采样和挖掘难负例等策略来优化模型性能，而Listwise方法则通过全局优化排序指标，进一步提升排序效果，但带来了更高的训练复杂度。\nFaiss向量近似检索的原理 Faiss（Facebook AI Similarity Search）是一款高效的向量相似度搜索和聚类开源库，适用于大规模、高维度的向量数据。其核心原理可以分为以下几个部分：\n向量量化（Vector Quantization）：\nProduct Quantization (PQ)：将高维向量分成多个低维子向量，每个子向量独立进行聚类，用聚类中心表示原始子向量，从而降低计算和存储成本。 Inverted File System (IVF)：将向量分配到不同的簇中，每个簇内的向量形成倒排表。查询时，首先确定查询向量所属的簇，然后在相应的倒排表中进行搜索，提高查询速度。 索引结构（Index Structure）：\nFlat Index：暴力搜索所有向量，适用于小规模数据集，精确但速度较慢。 IVF Index：通过聚类将数据划分到倒排列表中，查询时只在部分列表中搜索。 HNSW (Hierarchical Navigable Small World)：基于小世界图的索引结构，构建分层图加速搜索，适用于高精度、高效率的最近邻搜索。 PQ Index：分块量化向量，压缩存储，适用于大规模、高维向量数据。 查询过程（Query Process）：\n预处理：对查询向量进行标准化或量化。 簇选择：使用IVF时，首先确定查询向量最接近的簇。 近邻搜索：在选择的簇中进行近邻搜索，找到与查询向量最相似的向量。 重排序：对候选向量进行重排序，确保返回的结果是最相似的向量。 优化策略（Optimization Strategies）：\nSIMD指令：利用CPU的SIMD指令并行计算，加速距离计算。 多线程并行：利用多线程并行处理多个查询，进一步提高检索速度。 GPU加速：在支持GPU的环境中，利用GPU的并行计算能力加速向量量化和最近邻搜索。 总结 Faiss通过结合向量量化、倒排索引和小世界图等技术，实现了高效的向量相似度搜索，适用于不同规模和维度的向量数据。\n对比学习 对比学习属于自监督学习的一种。\n要做好对比学习，主要要解决以下问题：\n如何构建正例和负例数据，使得数据更多样化、更合理。 模型结构设计，让模型更好地提取到数据的特征。 损失函数设计，让正例间距离越近、负例间距离越远，并避免模型坍塌(Collapse)，即避免不同输入通过模型计算后映射到了同一个值。 距离函数，上述损失函数中用到的距离计算，使用什么函数更合理。 InfoNCE Loss InfoNCE (Info Noise-Contrastive Estimation) Loss 是一种用于对比学习（contrastive learning）的损失函数，主要用于自监督学习任务。它通过最大化相似样本的相似度、最小化不相似样本的相似度来学习有效的特征表示。InfoNCE Loss 的原理和计算过程如下：\n原理 InfoNCE Loss 通过一个噪声对比估计方法来最大化查询样本和正样本（相似样本）之间的相似度，同时最小化查询样本和负样本（不相似样本）之间的相似度。它的目标是提高模型在区分正负样本对上的能力，从而学到更有意义的表示。\n计算过程 定义样本对：\n假设我们有一个查询样本 $q $ 和一个正样本 $k^+ $ 以及 $N $ 个负样本 $ k^-_1, k^-_2, \\ldots, k^-_N $。 相似度计算：\n通常使用点积（或余弦相似度）计算查询样本与正、负样本的相似度。 $$ s(q, k) = \\frac{q \\cdot k}{|q| |k|} $$ 计算损失：\nInfoNCE Loss 的公式如下： $$ \\mathcal{L} = -\\log \\frac{\\exp(s(q, k^+) / \\tau)}{\\exp(s(q, k^+) / \\tau) + \\sum_{i=1}^N \\exp(s(q, k^-_i) / \\tau)} $$ 其中，(\\tau) 是温度参数，用于调节分布的平滑程度。 解释：\n分子部分表示查询样本 ( q ) 和正样本 ( k^+ ) 之间的相似度。 分母部分表示查询样本 ( q ) 与所有样本（包括正样本和负样本）的相似度总和。 通过最大化该比值，模型被训练为提高正样本相似度，同时降低负样本相似度。 优点 有效性：在无监督和自监督学习任务中，InfoNCE Loss 能够有效地学到有意义的特征表示。 灵活性：可以应用于各种对比学习框架，如SimCLR、MoCo等。 示例应用 以 SimCLR 为例，其主要步骤如下：\n对输入图像进行两次随机数据增强，生成两个视图。 使用神经网络分别编码两个视图，得到两个特征向量。 计算两个特征向量之间的相似度（正样本），并与其他图像的特征向量进行对比（负样本）。 使用 InfoNCE Loss 计算损失，并通过反向传播更新网络参数。 总结 InfoNCE Loss 是一种强大的工具，用于自监督对比学习，通过最大化相似样本之间的相似度和最小化不相似样本之间的相似度，来学习有效的特征表示。这种方法在各种自监督学习任务中表现出了强大的性能。\nInfoNCE loss和交叉熵的关系 总结 共同点：InfoNCE Loss 和交叉熵损失的目标都是最小化预测概率与真实概率之间的差异。InfoNCE Loss 通过对比正负样本对来实现这一点，本质上是交叉熵损失在对比学习中的应用。 不同点：交叉熵损失通常用于标准的分类任务，而 InfoNCE Loss 是专门设计用于对比学习任务，通过对比相似和不相似样本对来学习有用的特征表示。 因此，InfoNCE Loss 可以看作是交叉熵损失在对比学习场景中的特化形式，它利用了相似度计算和 softmax 函数来处理正负样本对，从而实现有效的特征学习。\n对比学习中temperature的作用 在对比学习中，温度参数（temperature parameter，通常记作 (\\tau)）在计算损失函数（如 InfoNCE Loss）时起到了调节相似度分布平滑度的重要作用。具体来说：\n调节相似度分布的平滑度：\n温度参数 (\\tau) 用于调节 softmax 函数的平滑度。当 (\\tau) 较小时，相似度的分布会变得更加尖锐，模型会更关注最相似的正样本和最不相似的负样本。反之，当 (\\tau) 较大时，相似度的分布会变得更加平滑，模型对所有样本的关注度更加均匀。 控制对负样本的惩罚力度：\n较小的 (\\tau) 会强调正负样本对之间的区分，使得模型对负样本的惩罚更加严格。较大的 (\\tau) 则会减小对负样本的惩罚，使得训练过程更为平滑。 影响学习速度和稳定性：\n较小的 (\\tau) 会导致梯度变大，可能加快学习速度，但也可能导致训练不稳定或陷入局部最优。较大的 (\\tau) 则会使梯度变小，训练过程更为平稳，但可能需要更多的训练时间来收敛。 如果温度系数设的越大，logits分布变得越平滑，那么对比损失会对所有的负样本一视同仁，导致模型学习没有轻重。如果温度系数设的过小，则模型会越关注特别困难的负样本，但其实那些负样本很可能是潜在的正样本，这样会导致模型很难收敛或者泛化能力差。\n总之，温度系数的作用就是它控制了模型对负样本的区分度。\nTF Serving Tensorflow则提供了TFserving方案来部署线上模型推理。\nTFServing具有如下明显的优势：\n(1) 支持docker部署：常用的部署方式是使用docker部署，将宿主机的模型目录挂载在docker的虚拟目录下，拉取的docker镜像可选择支持CPU或GPU，也提供了版本选择，方便调试。\n(2) 支持多种通信方式：目前TF Serving的客户端和服务端支持的的通信方式为：gRPCAPI和RESTfull API，前者端口为8500，后者为8501，两种方式分别对应了不同的传输方法。\n(3) 挂载模型简单：使用TF Serving部署模型，不需要部署代码，但需要必要的客户端代码。因为模型使用挂载的方式，对同一模型的不同版本，Serving会自动刷新选择最新的模型版本，更新版本也不需要重启服务。同时TFServing也支持多模型部署，因而使用起来比较方便。\n在实际的操作过程中，发现部署TFServing存在以下几个难点，或者说部署时要特别注意的地方：\n(1) 模型要求：对于大多数训练模型都是以CKPT文件保存，但是TFServing推荐的模型保存方式为PB模型。PB文件里保存了图的结构和变量数据，文件大小相对CKPT来说较小。但是对于CKPT模型，还必须找出它的输入输出节点，才能转换为PB模型。如果使用的是不熟悉的CKPT模型，那么模型转换操作会比较复杂。\n(2) 参数设置：PB模型的参数信息在生成模型的时候已经保存在模型内，并且在Serving启动后被直接调用，所以如果想要对参数进行修改会相对困难。目前看到最多的解决方法是直接在生成PB模型的时候修改，模型应用会比较死板。\n推荐系统 召回 召回模型从传统的召回算法，如基于用户的协同过滤、基于商品的协同过滤、基于矩阵分解的召回算法等，演变到embedding相关的模型，如 Item2Vec 召回，FM召回，以及基于Graph Embedding 的召回模型，如DeepWalk，Node2Vec等模型，对于一些Item还有包含Word2Vec、FastText、Bert等基于内容语义的召回算法，当然还有如Youtube DNN、DSSM等基于深度学习的召回算法。\nFM召回 https://toutiao.io/posts/gxvzeiv/preview \u003c—–老好了！\nFM召回的主流作法，是用生成的user embedding直接查找最相近的item embedding。\nDCN DSSM https://zhuanlan.zhihu.com/p/636689560\n多路召回的必要性 在召回阶段，我们通常要考虑召回率和计算速度，以新闻推荐为例，为了保证用户尽可能地感兴趣，同时还要兼顾热点和时效性数据，常常会使用多路召回。下图显示了多路召回的方法。\n推荐系统排序阶段常用模型 初期阶段，主要是进行各种特征工程，模型主要使用LR模型。\n中期阶段，进行二阶、 高阶特征交叉，使用FM/FFM、 GBDT+LR、 XGBoost等树模型。\n深度阶段， 开始将特征映射至多维空间中， 然后再通过多层网络去学习特征之间的相关性（ FNN、 PNN、 Wide \u0026 Deep、NFM、 AFM、 DeepFM、 xDeepFM等）。\nWide\u0026Deep https://zhuanlan.zhihu.com/p/617767615\nDeepFM CTR预估是目前推荐系统的核心技术，其目标是预估用户点击推荐内容的概率。DeepFM模型包含FM和DNN两部分，FM模型可以抽取low-order（低阶）特征，DNN可以抽取high-order（高阶）特征。低阶特征可以理解为线性的特征组合，高阶特征，可以理解为经过多次线性-非线性组合操作之后形成的特征，为高度抽象特征。无需Wide\u0026Deep模型人工特征工程。由于输入仅为原始特征，而且FM和DNN共享输入向量特征，DeepFM模型训练速度很快。\n注解：Wide\u0026Deep是一种融合浅层（wide）模型和深层（deep）模型进行联合训练的框架，综合利用浅层模型的记忆能力和深层模型的泛化能力，实现单模型对推荐系统准确性和扩展性的兼顾。\nhttps://zhuanlan.zhihu.com/p/636689560\nMMOE mmoe 实际上就是 多个门 的 moe 网络。\n输入多个专家的过程和moe无任何区别，这里唯一的不同是对每一个任务有一个门控网络。\nhttps://developer.aliyun.com/article/1152526\n行为序列建模方法 DIN、DIEN、DSIN：https://blog.csdn.net/Ajdidfj/article/details/129381802\nDIN DIN模型提出的动机是利用target attention的方法，进行加权pooling，它为历史行为的物品和当前推荐物品计算一个attention score，然后加权pooling，这样的方法更能体现用户兴趣多样性。\nDIEN DIEN相比于之前的模型，即对用户的兴趣进行建模，又对建模出来的用户兴趣继续建模得到用户的兴趣变化过程。\n输入embedding,用户历史行为序列通过GRU（引入了一个损失，为了让行为序列中的每一个时刻都有一个target item进行监督训练，也就是使用下一个行为来监督兴趣状态的学习），通过注意力机制，再通过AUGRU,输出一个embedding,和另外的非行为相关特征进行concat。\nDIEN模型的重点就是如何将用户的行为序列转换成与用户兴趣相关的向量，在DIN中是直接通过与target item计算序列中每个元素的注意力分数，然后加权求和得到最终的兴趣表示向量。在DIEN中使用了两层结构来建模用户兴趣相关的向量。\nDSIN 这个是在DIEN的基础上又进行的一次演化，这个模型的改进出发点依然是如何通过用户的历史点击行为，从里面更好的提取用户的兴趣以及兴趣的演化过程，这个模型就是从user历史行为信息挖掘方向上进行演化的。\n作者发现用户的行为序列的组成单位，其实应该是会话(按照用户的点击时间划分开的一段行为)，每个会话里面的点击行为呢？ 会高度相似，而会话与会话之间的行为，就不是那么相似了，但是像DIN，DIEN这两个模型，DIN的话，是直接忽略了行为之间的序列关系，使得对用户的兴趣建模或者演化不是很充分，而DIEN的话改进了DIN的序列关系的忽略缺点，但是忽视了行为序列的本质组成结构。\n",
  "wordCount" : "1399",
  "inLanguage": "en",
  "datePublished": "2024-05-19T09:06:49+08:00",
  "dateModified": "2024-05-19T09:06:49+08:00",
  "author":[{
    "@type": "Person",
    "name": "Sanmu"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://welldonesanmu.github.io/posts/dl/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sanmu",
    "logo": {
      "@type": "ImageObject",
      "url": "https://welldonesanmu.github.io/img/avatars.jpg"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://welldonesanmu.github.io" accesskey="h" title="Home (Alt + H)">
                <img src="https://welldonesanmu.github.io/img/avatars.jpg" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://welldonesanmu.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://welldonesanmu.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://welldonesanmu.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://welldonesanmu.github.io">Home</a>&nbsp;»&nbsp;<a href="https://welldonesanmu.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      DL
    </h1>
    <div class="post-meta"><span title='2024-05-19 09:06:49 +0800 CST'>May 19, 2024</span>&nbsp;·&nbsp;Sanmu

</div>
  </header> 
  <div class="post-content"><h1 id="神经网络">神经网络<a hidden class="anchor" aria-hidden="true" href="#神经网络">#</a></h1>
<h2 id="dnnmlpann">DNN/MLP/ANN<a hidden class="anchor" aria-hidden="true" href="#dnnmlpann">#</a></h2>
<p>全连接神经网络/多层感知器</p>
<p>DNN是从多层感知器发展而来的第三代神经网络，DNN的网络结构分为三部分：输入层、隐藏层、输出层。</p>
<p>DNN的层与层之间是全连接的，第i层的任意一个神经元一定与第i+1层的任意一个神经元连接.</p>
<p><a href="https://www.cnblogs.com/init0ne/p/16630311.html">https://www.cnblogs.com/init0ne/p/16630311.html</a></p>
<hr>
<h2 id="knn">KNN<a hidden class="anchor" aria-hidden="true" href="#knn">#</a></h2>
<p>KNN（K-Nearest Neighbors，<strong>K近邻算法</strong>）是一种常见的无参数（non-parametric）分类和回归算法，常用于基于实例的学习（instance-based learning）。</p>
<p><strong>对于分类问题，KNN算法会查找与该未知样本最近的K个样本点，然后将这K个样本点中出现最多的类别作为该未知样本的预测类别。对于回归问题，KNN算法会查找与该未知样本最近的K个样本点，并将这K个样本点的输出值的平均值作为该未知样本的预测输出值。</strong></p>
<p>KNN算法中，距离度量是非常重要的一环。通常使用欧氏距离（Euclidean distance）或曼哈顿距离（Manhattan distance）等常见的距离度量方法。另外，KNN算法还需要确定K值的大小，通常通过交叉验证（cross-validation）等技术进行调整。</p>
<p>KNN算法的<strong>优点是简单易懂、易于实现</strong>、对于大规模数据集也可以高效处理。<strong>缺点是</strong>需要对每个未知样本进行计算距离和排序，当数据集规模很大时，<strong>计算成本会非常高，同时对于高维度的数据集，样本之间的距离可能会失去准确性，从而影响KNN算法的表现。</strong></p>
<hr>
<h2 id="rnn">RNN<a hidden class="anchor" aria-hidden="true" href="#rnn">#</a></h2>
<p>RNN是一种神经网络结构，它通过在不同时间步之间共享权重来处理序列数据。与其他神经网络结构不同的是，<strong>RNN 具有记忆能力</strong>，可以在处理当前数据时利用之前的信息。它们的优点在于可以接受任意长度的输入序列，并且可以利用序列中之前的信息来帮助预测下一个值。</p>
<p>RNN 的一个基本单元是循环单元（recurrent unit），也称为记忆单元（memory cell），可以是简单的结构（如 Vanilla RNN）或复杂的结构（如 LSTM 和 GRU）。这些循环单元接受输入和先前的隐藏状态，产生一个新的隐藏状态，并输出一个值，该值可以被用于下一时间步的计算。通过不断地将先前的隐藏状态传递给下一时间步，RNN 可以利用历史信息来影响当前的输出。</p>
<p>RNN 的训练可以通过反向传播算法进行，这可以通过计算当前时间步的误差来更新网络的参数，以改善其性能。但是，RNN 的训练存在梯度消失或梯度爆炸等问题，这可能导致在训练过程中难以传递长期依赖的信息。为了解决这些问题，出现了一些改进的 RNN 结构，如 LSTM（长短时记忆网络）和 GRU（门控循环单元）。这些结构可以更好地处理长期依赖和短期信息。</p>
<h4 id="优缺点"><strong>优缺点</strong><a hidden class="anchor" aria-hidden="true" href="#优缺点">#</a></h4>
<p>优点：</p>
<ol>
<li>RNN 具有记忆能力，可以利用序列数据之前的信息来预测下一个值，从而更好地处理序列数据。</li>
<li>RNN 可以接受任意长度的输入序列，并且可以在每个时间步骤使用相同的参数，从而可以处理变长的序列数据。</li>
<li>RNN 的参数可以通过反向传播算法来训练，这使得其可以学习到数据之间的关系，从而可以在自然语言处理、语音识别等领域中得到广泛应用。</li>
</ol>
<p>缺点：</p>
<ol>
<li>RNN 计算的时间复杂度与序列长度成正比，处理长序列时可能会面临较大的计算开销和内存消耗。</li>
<li>RNN 的训练可能会受到梯度消失或梯度爆炸等问题的影响，尤其是在处理长序列数据时，这些问题可能会导致模型难以捕捉到序列中的长期依赖关系。</li>
<li>RNN 的记忆能力有限，无法捕捉到非常长的历史信息，这可能会影响模型的预测性能。</li>
</ol>
<hr>
<h2 id="rnn为什么会梯度消失或爆炸"><strong>RNN为什么会梯度消失或爆炸</strong><a hidden class="anchor" aria-hidden="true" href="#rnn为什么会梯度消失或爆炸">#</a></h2>
<p>RNN（循环神经网络）在处理长序列时，存在梯度消失或梯度爆炸的问题，这是由于其计算方式和训练过程的特性所导致的。</p>
<p>具体来说，RNN 的计算方式是通过重复应用相同的权重来处理序列数据，即<strong>在每个时间步使用相同的权重矩阵进行计算，这种计算方式会导致随着序列长度的增加，信息在网络中传递时会经过多次的矩阵乘法，从而使得梯度在反向传播过程中会不断地被乘以相同的权重，从而可能会出现指数级的增长或衰减，这就是梯度爆炸或梯度消失的问题。</strong></p>
<p><strong>RNN的梯度消失不是真正的梯度消失，当下的梯度总会在，只是前面的梯度没了。RNN的梯度消失指的是当下梯度用不到前面的梯度了。</strong></p>
<p>当梯度爆炸发生时，梯度的值变得非常大，导致网络参数更新过快，使得网络的性能变得不稳定。当梯度消失发生时，梯度的值变得非常小，导致网络参数更新过慢，使得网络无法学习到长期依赖的信息，从而影响了网络的预测性能。</p>
<p>为了解决这些问题，一些改进的 RNN 结构被提出，如 LSTM（长短时记忆网络）和 GRU（门控循环单元），它们使用了一些特殊的门机制来控制信息的流动和更新，从而更好地处理了长序列数据，并有效地缓解了梯度消失或梯度爆炸的问题。</p>
<hr>
<h2 id="lstm"><strong>LSTM</strong><a hidden class="anchor" aria-hidden="true" href="#lstm">#</a></h2>
<p>LSTM（长短时记忆网络）是一种特殊类型的循环神经网络（RNN），它<strong>在处理序列数据时能够有效地捕捉长期依赖关系</strong>，通过使用一组称为“门”的结构来控制信息的流动，包括输入门、遗忘门和输出门。这些门可以在不同的时间步骤决定从当前输入和前一个时间步骤的输出中选择哪些信息需要被传递和保留。此外，LSTM还包括一个称为细胞状态的重要部分，可以在时间步骤之间存储和传递信息。</p>
<p>LSTM的优点在于它可以避免梯度消失问题，而且能够处理长期依赖关系，即使序列长度很长。此外，LSTM还具有较好的鲁棒性和泛化能力，可以适应不同的任务和数据集。</p>
<h2 id="三个门的作用"><strong>三个门的作用</strong><a hidden class="anchor" aria-hidden="true" href="#三个门的作用">#</a></h2>
<p>1)遗忘门ft控制上一个时刻的内部状态ct-1需要遗忘多少信息。</p>
<p>2)输入门it控制当前的候选状态有多少信息需要保存。</p>
<p>3)输出门ot控制当前时刻的内部状态ct有多少信息需要输出给外部状态ht</p>
<h2 id="激活函数"><strong>激活函数</strong><a hidden class="anchor" aria-hidden="true" href="#激活函数">#</a></h2>
<p>LSTM（长短时记忆）是一种递归神经网络，由输入门、遗忘门、输出门和一个单元状态组成。每个门都有一个对应的激活函数。以下是LSTM中使用的激活函数：</p>
<ol>
<li>Sigmoid函数：用于输入门、遗忘门和输出门的门控制器。Sigmoid函数将输入值映射到0到1之间的输出值，用于控制门的开放程度。</li>
<li>双曲正切函数（Tanh）：用于更新单元状态。Tanh函数将输入值映射到-1到1之间的输出值，用于控制单元状态的更新。</li>
</ol>
<h2 id="优缺点-1"><strong>优缺点</strong><a hidden class="anchor" aria-hidden="true" href="#优缺点-1">#</a></h2>
<p><strong>优点:</strong></p>
<ol>
<li>解决了梯度消失问题：在传统的循环神经网络中，随着时间步长的增加，梯度可以变得非常小甚至消失。LSTM 使用门结构来控制信息的流动，从而避免了梯度消失问题。</li>
<li>能够处理长期依赖关系：LSTM 在细胞状态中存储信息，能够在时间步骤之间传递长期依赖关系。</li>
<li>对于不同的序列长度有良好的适应性：LSTM 可以自适应地调整序列长度，并对输入和输出数据的变化进行处理。</li>
<li>可以处理多种类型的数据：LSTM 适用于多种类型的数据，如文本、语音、图像等。</li>
</ol>
<p><strong>缺点:</strong></p>
<ol>
<li>计算代价高：LSTM 中包含大量的参数，因此需要更多的计算代价。此外，LSTM 的训练也需要更多的计算资源。</li>
<li>容易出现过拟合：在使用 LSTM 时，由于模型的复杂性，容易出现过拟合的问题，因此需要采取适当的正则化方法。</li>
<li>需要大量的数据训练：为了训练 LSTM，需要大量的数据进行训练。如果数据集较小，则容易出现过拟合的问题。</li>
</ol>
<h2 id="lstm怎么缓解梯度消失和爆炸问题"><strong>lstm怎么缓解梯度消失和爆炸问题</strong><a hidden class="anchor" aria-hidden="true" href="#lstm怎么缓解梯度消失和爆炸问题">#</a></h2>
<p><a href="https://zhuanlan.zhihu.com/p/136223550">LSTM通过门控结构来解决梯度消失或爆炸问题</a><a href="https://zhuanlan.zhihu.com/p/136223550">1</a><a href="https://blog.csdn.net/coolerzZ/article/details/110522821">2</a><a href="https://www.cnblogs.com/bonelee/p/10475453.html">3</a><a href="https://zhuanlan.zhihu.com/p/136223550">。LSTM的遗忘门可以选择性地保留或删除远距离信息，从而缓解梯度消失</a><a href="https://zhuanlan.zhihu.com/p/136223550">1</a><a href="https://blog.csdn.net/coolerzZ/article/details/110522821">。LSTM的输入门和输出门可以控制信息的流入和流出，从而减少梯度爆炸的风险</a><a href="https://blog.csdn.net/coolerzZ/article/details/110522821">2</a><a href="https://zhuanlan.zhihu.com/p/149819433">4</a><a href="https://zhuanlan.zhihu.com/p/149819433">。LSTM还使用了tanh激活函数，使得梯度不会无限增大或减小</a><a href="https://zhuanlan.zhihu.com/p/149819433">4</a>。</p>
<h2 id="lstm和transformer的比较"><strong>LSTM和Transformer的比较</strong><a hidden class="anchor" aria-hidden="true" href="#lstm和transformer的比较">#</a></h2>
<p>Transformer和LSTM都是深度学习中常用的序列模型，但它们在结构和应用方面有一些不同。</p>
<p>Transformer和LSTM的最大区别，就是LSTM的训练是迭代的、串行的，必须要等当前字处理完，才可以处理下一个字。而Transformer的训练时并行的，即所有<strong>字</strong>是同时训练的，这样就大大增加了计算效率。</p>
<p>LSTM是一种递归神经网络（Recurrent Neural Network, RNN），用于处理序列数据。它的主要结构是一个神经元，它接收输入和上一个时间步的隐藏状态，并输出当前时间步的隐藏状态和输出。LSTM的主要组成部分包括输入门、遗忘门和输出门。</p>
<p>Transformer是一种基于注意力机制（Attention Mechanism）的神经网络，用于处理序列数据。它不依赖于递归结构，而是使用自注意力机制来建立全局联系。Transformer由编码器和解码器组成，其中编码器和解码器都由多个编码器层和解码器层组成。</p>
<p>LSTM在处理长序列时容易出现梯度消失或梯度爆炸的问题。为了解决这个问题，LSTM通常采用反向传播和门控制技术。</p>
<p>Transformer的训练相对于LSTM更容易收敛，因为它没有递归结构和门控制机制。Transformer通常采用自适应优化器（如Adam）和残差连接来训练。</p>
<hr>
<h2 id="transformerattention"><strong>Transformer、Attention</strong><a hidden class="anchor" aria-hidden="true" href="#transformerattention">#</a></h2>
<p>Transformer的设计基于一个核心思想，即用自注意力机制（self-attention）来捕捉序列之间的依赖关系，从而更好地处理序列数据。自注意力机制允许模型对输入序列中的所有位置进行编码，而不像传统的RNN只能处理有限的局部上下文。因此，Transformer能够同时处理长序列和短序列，从而提高模型的效率和准确性。</p>
<h2 id="transformer的结构"><strong>transformer的结构</strong><a hidden class="anchor" aria-hidden="true" href="#transformer的结构">#</a></h2>
<p><img loading="lazy" src="https://welldonesanmu2.github.io/picx-images-hosting/20240519/image.7zq97xt4ai.webp" alt="image"  />
</p>
<h2 id="transformer的激活函数"><strong>Transformer的激活函数</strong><a hidden class="anchor" aria-hidden="true" href="#transformer的激活函数">#</a></h2>
<ul>
<li><strong>ReLU</strong>：用于前馈神经网络层中的激活函数。</li>
<li><strong>GELU</strong>：用于一些Transformer变体（如BERT）中的前馈神经网络层。</li>
<li><strong>Softmax</strong>：用于自注意力机制中的权重归一化。</li>
</ul>
<p>GELU也是Transformer中常用的激活函数，尤其是在一些变体中，如BERT（Bidirectional Encoder Representations from Transformers）。与ReLU相比，GELU在激活时考虑了输入的标准正态分布。</p>
<h2 id="transformer的优化器"><strong>Transformer的优化器</strong><a hidden class="anchor" aria-hidden="true" href="#transformer的优化器">#</a></h2>
<p>Adam</p>
<h2 id="什么时候只使用encoder什么时候只使用decoder什么时候二者一起使用"><strong>什么时候只使用encoder，什么时候只使用decoder，什么时候二者一起使用</strong><a hidden class="anchor" aria-hidden="true" href="#什么时候只使用encoder什么时候只使用decoder什么时候二者一起使用">#</a></h2>
<p>Transformer是一个非常流行的深度学习模型，通常被用于自然语言处理和其他序列数据的任务。它包含了encoder和decoder两个部分。</p>
<p>当只需要对输入序列进行编码表示，而不需要生成输出序列时，通常只使用Transformer的encoder部分。例如，当我们需要对一组文本进行聚类或分类时，可以使用Transformer的encoder部分对每个文本进行编码表示，然后对这些表示进行聚类或分类。</p>
<p>当需要从输入序列生成输出序列时，需要使用Transformer的decoder部分。例如，当我们需要进行机器翻译或文本摘要时，可以使用Transformer的decoder部分从输入序列生成相应的输出序列。</p>
<p>当需要同时进行输入序列编码和输出序列生成时，需要使用Transformer的encoder和decoder部分。例如，在对话系统中，我们需要将用户输入编码为表示，然后使用decoder生成系统的回复。</p>
<p>总之，根据具体任务的要求，我们可以灵活地使用Transformer的encoder、decoder或两者结合的部分来构建模型。</p>
<h2 id="为什么用position-encoding"><strong>为什么用position encoding</strong><a hidden class="anchor" aria-hidden="true" href="#为什么用position-encoding">#</a></h2>
<p>Transformer之所以使用position encoding而不是position embedding，有以下几个原因：</p>
<ul>
<li><a href="https://bing.com/search?q=Transformer+position+encoding+position+embedding">position encoding可以表示任意长度的序列，而不受限于固定的最大长度。</a><a href="https://bing.com/search?q=Transformer+position+encoding+position+embedding">1</a><a href="https://zhuanlan.zhihu.com/p/442717956">2</a></li>
<li><a href="https://bing.com/search?q=Transformer+position+encoding+position+embedding">position encoding可以捕捉到相对距离和绝对距离的信息，而不仅仅是一个序号。</a><a href="https://bing.com/search?q=Transformer+position+encoding+position+embedding">1</a><a href="https://zhuanlan.zhihu.com/p/442717956">2</a></li>
<li><a href="https://bing.com/search?q=Transformer+position+encoding+position+embedding">position encoding可以与词向量进行线性变换，从而适应不同层次的特征提取。</a><a href="https://bing.com/search?q=Transformer+position+encoding+position+embedding">1</a><a href="https://zhuanlan.zhihu.com/p/442717956">2</a></li>
</ul>
<p>当然，position embedding也有其优点，例如：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/360539748">position embedding可以通过训练学习到更合适的位置表示，而不需要人为地设计函数形式。</a><a href="https://zhuanlan.zhihu.com/p/360539748">3</a><a href="https://qiita.com/masaki_kitayama/items/01a214c07b2efa8aed1b">4</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/360539748">position embedding可以更容易地处理非连续或无序的序列，例如图像或树状结构。</a><a href="https://zhuanlan.zhihu.com/p/360539748">3</a><a href="https://qiita.com/masaki_kitayama/items/01a214c07b2efa8aed1b">4</a></li>
</ul>
<h2 id="相对位置编码和绝对位置编码"><strong>相对位置编码和绝对位置编码</strong><a hidden class="anchor" aria-hidden="true" href="#相对位置编码和绝对位置编码">#</a></h2>
<p>Transformer使用的三角式绝对位置编码。<strong>BERT使用的是训练出来的绝对位置编码</strong></p>
<ul>
<li>绝对位置编码是指为序列中每个位置固定地分配一个编码向量。</li>
<li>相对位置编码是指为序列中的每个位置分配一个相对于其他位置的编码向量。</li>
</ul>
<p>绝对位置编码适用于序列长度较小的任务，而相对位置编码适用于序列长度较大的任务。绝对位置编码可以提供每个位置的绝对位置信息，但是由于它是固定的，无法适应不同任务和不同长度的序列。相对位置编码则可以根据不同任务和不同长度的序列自适应地学习到不同位置之间的相对位置关系，因此更加灵活和适用于更多的任务。</p>
<h2 id="位置编码还有什么"><strong>位置编码还有什么</strong><a hidden class="anchor" aria-hidden="true" href="#位置编码还有什么">#</a></h2>
<p>静态编码、绝对编码、相对编码、旋转编码、复数域编码、Neural ODE编码</p>
<h2 id="feed-forward"><strong>feed forward</strong><a hidden class="anchor" aria-hidden="true" href="#feed-forward">#</a></h2>
<p>第一层的激活函数为 Relu，第二层不使用激活函数</p>
<h2 id="attention公式"><strong>attention公式</strong><a hidden class="anchor" aria-hidden="true" href="#attention公式">#</a></h2>
<p>$$
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$</p>
<ul>
<li>query向量类比于询问。某个token问：“其余的token都和我有多大程度的相关呀？”</li>
<li>key向量类比于索引。某个token说：“我把每个询问内容的回答都压缩了下装在我的key里”</li>
<li>value向量类比于回答。某个token说：“我把我自身涵盖的信息又抽取了一层装在我的value里”</li>
</ul>
<h2 id="dk是qk的维度"><strong>dk是Q、K的维度</strong><a hidden class="anchor" aria-hidden="true" href="#dk是qk的维度">#</a></h2>
<p>首先Q、K、V都源于输入特征本身，是根据输入特征产生的向量，但目前我们现在无需关注是如何产生这组向量的。</p>
<p>V可以看做表示单个输入特征的向量。当我们直接把一组V输入到网络中进行训练，那这个网络就是没有引入Attention机制的网络。</p>
<p>但如果引入Attention，就需要将这组V分别乘以一组权重 α ，那么就可以做到有重点性地关注输入特征，如同人的注意力一般</p>
<h2 id="attention为何选择点乘而不是加法"><strong>attention为何选择点乘而不是加法</strong><a hidden class="anchor" aria-hidden="true" href="#attention为何选择点乘而不是加法">#</a></h2>
<h3 id="1-计算效率">1. 计算效率<a hidden class="anchor" aria-hidden="true" href="#1-计算效率">#</a></h3>
<p>点乘运算可以使用高度优化的矩阵乘法算法（如BLAS库），并且可以很好地并行化，适合在GPU等硬件上高效计算。</p>
<h3 id="2-向量相似性度量">2. 向量相似性度量<a hidden class="anchor" aria-hidden="true" href="#2-向量相似性度量">#</a></h3>
<p>点乘是衡量向量相似性的有效方法。对于两个向量 <strong>a</strong> 和 <strong>b</strong>，它们的点乘 <strong>a</strong>⋅<strong>b</strong> 的值与它们的夹角相关。点乘结果越大，表示两个向量越相似。这种相似性度量对计算注意力权重非常合适。</p>
<h3 id="3-线性变换的性质">3. 线性变换的性质<a hidden class="anchor" aria-hidden="true" href="#3-线性变换的性质">#</a></h3>
<p>自注意力机制中的查询（Query）、键（Key）和值（Value）向量都是通过线性变换得到的。点乘操作保留了这些线性变换的性质，便于梯度计算和参数更新。</p>
<h2 id="self-attention的伪代码"><strong>self-attention的伪代码</strong><a hidden class="anchor" aria-hidden="true" href="#self-attention的伪代码">#</a></h2>
<p><a href="https://blog.csdn.net/qq_44949041/article/details/128087174">https://blog.csdn.net/qq_44949041/article/details/128087174</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"> <span class="n">准备输入</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="n">Input</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>  <span class="n">Input</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>   <span class="n">Input</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="n">初始化权重</span>
</span></span><span class="line"><span class="cl"><span class="n">w_key</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">w_query</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">w_value</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">w_key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">w_key</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w_query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">w_query</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">w_value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="n">导出key</span><span class="p">,</span> <span class="n">query</span> <span class="ow">and</span> <span class="n">value的表示</span>
</span></span><span class="line"><span class="cl"><span class="n">keys</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_key</span>
</span></span><span class="line"><span class="cl"><span class="n">querys</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_query</span>
</span></span><span class="line"><span class="cl"><span class="n">values</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_value</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="n">tensor</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">querys</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"> <span class="n">计算输入的注意力得分</span><span class="p">(</span><span class="n">attention</span> <span class="n">scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">attn_scores</span> <span class="o">=</span> <span class="n">querys</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl"> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">],</span>   <span class="n">attention</span> <span class="n">scores</span> <span class="kn">from</span> <span class="nn">Query</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">16.</span><span class="p">,</span> <span class="mf">12.</span><span class="p">],</span>   <span class="n">attention</span> <span class="n">scores</span> <span class="kn">from</span> <span class="nn">Query</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">12.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">]])</span>  <span class="n">attention</span> <span class="n">scores</span> <span class="kn">from</span> <span class="nn">Query</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl"> <span class="n">计算softmax</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">softmax</span>
</span></span><span class="line"><span class="cl"><span class="n">attn_scores_softmax</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="n">tensor</span><span class="p">([[</span><span class="mf">6.3379e-02</span><span class="p">,</span> <span class="mf">4.6831e-01</span><span class="p">,</span> <span class="mf">4.6831e-01</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">6.0337e-06</span><span class="p">,</span> <span class="mf">9.8201e-01</span><span class="p">,</span> <span class="mf">1.7986e-02</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">2.9539e-04</span><span class="p">,</span> <span class="mf">8.8054e-01</span><span class="p">,</span> <span class="mf">1.1917e-01</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"> <span class="n">For</span> <span class="n">readability</span><span class="p">,</span> <span class="n">approximate</span> <span class="n">the</span> <span class="n">above</span> <span class="k">as</span> <span class="n">follows</span>
</span></span><span class="line"><span class="cl"><span class="n">attn_scores_softmax</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">attn_scores_softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">attn_scores_softmax</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="n">将attention</span> <span class="n">scores乘以value</span>
</span></span><span class="line"><span class="cl"><span class="n">weighted_values</span> <span class="o">=</span> <span class="n">values</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">attn_scores_softmax</span><span class="o">.</span><span class="n">T</span><span class="p">[:,:,</span><span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"> <span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">         <span class="p">[[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">4.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mf">2.0000</span><span class="p">,</span> <span class="mf">8.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mf">1.8000</span><span class="p">,</span> <span class="mf">7.2000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">         <span class="p">[[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">3.0000</span><span class="p">,</span> <span class="mf">1.5000</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mf">0.2000</span><span class="p">,</span> <span class="mf">0.6000</span><span class="p">,</span> <span class="mf">0.3000</span><span class="p">]]])</span>
</span></span><span class="line"><span class="cl"> <span class="n">对加权后的value求和以得到输出</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">weighted_values</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="n">tensor</span><span class="p">([[</span><span class="mf">2.0000</span><span class="p">,</span> <span class="mf">7.0000</span><span class="p">,</span> <span class="mf">1.5000</span><span class="p">],</span>   <span class="n">Output</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">2.0000</span><span class="p">,</span> <span class="mf">8.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>   <span class="n">Output</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">         <span class="p">[</span><span class="mf">2.0000</span><span class="p">,</span> <span class="mf">7.8000</span><span class="p">,</span> <span class="mf">0.3000</span><span class="p">]])</span>  <span class="n">Output</span> <span class="mi">3</span>
</span></span></code></pre></div><h2 id="self-attention的时间复杂度"><strong>self-attention的时间复杂度</strong><a hidden class="anchor" aria-hidden="true" href="#self-attention的时间复杂度">#</a></h2>
<p>Transformer的self attention机制因为要对任意两个单词做attention计算，所以时间复杂度是$n^2$，n是输入的长度。</p>
<h2 id="self-attention的优化"><strong>self-attention的优化</strong><a hidden class="anchor" aria-hidden="true" href="#self-attention的优化">#</a></h2>
<ul>
<li>
<p>Linformer：论文核心贡献是发现self attention矩阵P是低秩矩阵，从而可以通过SVD构建另一个维度小的低秩矩阵P</p>
</li>
<li>
<p>CCNet：论文主要思路，区别与Non-Local 中的全局attention，这篇文章提出只在特征点所对应的十字上进行attention。从而将复杂度从 O(N2) 降低到 O(N)</p>
</li>
</ul>
<h2 id="self-attention有什么优点"><strong>self-attention有什么优点？</strong><a hidden class="anchor" aria-hidden="true" href="#self-attention有什么优点">#</a></h2>
<ol>
<li>可以建模长程依赖关系</li>
</ol>
<p>传统的RNN模型在处理长序列时容易出现梯度消失或梯度爆炸的问题，而**Self-Attention模型可以捕捉序列中不同位置之间的依赖关系，从而能够更好地处理长序列数据。**由于Self-Attention模型中每个位置的表示都是通过与其他位置的表示进行加权求和得到的，因此模型能够在不同位置之间建立直接的联系，而不需要通过逐个时间步地传递信息。</p>
<ol start="2">
<li>可以并行化计算</li>
</ol>
<p>传统的RNN模型在处理长序列时需要逐个时间步地计算，因此难以实现高效的并行计算。而Self-Attention模型中的每个位置都可以同时计算，因此可以通过矩阵乘法等高效的并行计算方式来加速计算。</p>
<ol start="3">
<li>可以学习不同位置之间的关系</li>
</ol>
<p>Self-Attention模型中，每个位置的表示都是通过与其他位置的表示进行加权求和得到的，从而能够捕捉不同位置之间的关系。这种方法可以学习到不同位置之间的相互作用，例如文本中不同单词之间的关系，从而能够更好地理解文本的含义。</p>
<ol start="4">
<li>可以学习多个关注点</li>
</ol>
<p>Self-Attention模型中，通过计算Query和Key的点积得到Attention权重，可以学习到不同的关注点。这使得模型能够同时关注多个方面的信息，例如机器翻译中源语言和目标语言之间的对应关系。</p>
<h2 id="self-attention为什么要除根号dk进行scaled"><strong>self-attention为什么要除根号dk（进行scaled）</strong><a hidden class="anchor" aria-hidden="true" href="#self-attention为什么要除根号dk进行scaled">#</a></h2>
<p><a href="https://blog.csdn.net/tailonh/article/details/120544719">self-attention为什么要除根号dk的原因是为了防止梯度消失</a><a href="https://bing.com/search?q=self-attention%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%99%A4%E6%A0%B9%E5%8F%B7dk">1</a><a href="https://blog.csdn.net/tailonh/article/details/120544719">2</a>。</p>
<ul>
<li><a href="https://blog.csdn.net/tailonh/article/details/120544719">Q和K的内积结果随着维度变大，方差会变大，可能会出现很大的值</a><a href="https://blog.csdn.net/tailonh/article/details/120544719">2</a><a href="https://blog.csdn.net/ytusdc/article/details/121622205">3</a><a href="https://blog.csdn.net/zqm_0015/article/details/103905125">4</a>。</li>
<li><a href="https://blog.csdn.net/tailonh/article/details/120544719">这样会导致softmax函数的饱和区间，梯度几乎为0</a><a href="https://bing.com/search?q=self-attention%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%99%A4%E6%A0%B9%E5%8F%B7dk">1</a><a href="https://blog.csdn.net/tailonh/article/details/120544719">2</a>。</li>
<li><a href="https://blog.csdn.net/tailonh/article/details/120544719">除根号dk可以降低Q和K的数值，保证输出结果不会过大</a><a href="https://blog.csdn.net/tailonh/article/details/120544719">2</a><a href="https://blog.csdn.net/ytusdc/article/details/121622205">3</a><a href="https://blog.csdn.net/zqm_0015/article/details/103905125">4</a>。</li>
</ul>
<p>使softmax的输入输出方差保持一致，防止梯度消失，加速收敛。</p>
<h2 id="self-attention不除根号dk有哪些替代方案"><strong>self-attention不除根号dk有哪些替代方案</strong><a hidden class="anchor" aria-hidden="true" href="#self-attention不除根号dk有哪些替代方案">#</a></h2>
<p>在Transformer模型中，Self-Attention层的公式中除以根号d_k是为了缩放注意力权重，使其具有更合理的尺度。如果不除以根号d_k，则可能导致注意力权重的尺度过大或过小，从而影响模型的训练效果。</p>
<p>如果不想使用除以根号d_k的缩放方法，也可以使用其他的缩放方法。以下是几种常见的替代方案：</p>
<ol>
<li><strong>缩放到单位范数</strong></li>
</ol>
<p>除了除以根号d_k之外，可以将每个Query向量、Key向量和Value向量除以其向量的范数（L2范数）来进行缩放，使它们成为单位向量。这种方法可以保证每个向量的尺度都是一致的，而且每个向量的尺度不受向量维度的影响。</p>
<ol start="2">
<li>缩放到[-1, 1]或[0, 1]范围内</li>
</ol>
<p>可以将点积的结果除以d_k，然后使用tanh或sigmoid等函数将结果缩放到[-1, 1]或[0, 1]范围内。这种方法可以使点积的结果具有一定的尺度，并且不会出现梯度消失或梯度爆炸的问题。</p>
<ol start="3">
<li><strong>缩放到常数范围内</strong></li>
</ol>
<p>可以将点积的结果除以一个常数（比如8），以缩放到一个较小的范围内。这种方法可以避免注意力权重过大或过小的问题，同时也可以使模型的训练更加稳定。</p>
<p>总之，只要能做到每层参数的梯度保持在训练敏感的范围内，不要太大，不要太小就行了。</p>
<h2 id="mask"><strong>Mask！！！</strong><a hidden class="anchor" aria-hidden="true" href="#mask">#</a></h2>
<p>在Transformer中，由于自注意力机制（self-attention mechanism）的存在，每个词都要与其它所有词计算相似度得到加权和作为其新的表示。<strong>如果不考虑mask，模型将会把当前位置之后的词也考虑进来，这样就泄露了未来信息，导致模型在预测时出现了信息泄露的问题。</strong></p>
<p><strong>在Transformer中，有两种常见的mask，分别是padding mask和sequence mask。</strong></p>
<ul>
<li><strong>padding mask用于将填充的位置掩盖掉</strong>：保证句子的长度一样所以需要进行填充。</li>
<li>**sequence mask用于将当前位置之后的位置掩盖掉。**这样，在进行self-attention计算时，模型就不会考虑掩盖的位置，从而保证了模型的正确性。</li>
</ul>
<h2 id="self-attention怎么处理padding"><strong>self-attention怎么处理padding</strong><a hidden class="anchor" aria-hidden="true" href="#self-attention怎么处理padding">#</a></h2>
<p><a href="https://zhuanlan.zhihu.com/p/359414250">self-attention处理padding的一种方法是使用mask</a><a href="https://zhuanlan.zhihu.com/p/359414250">1</a>。</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/359414250">mask是一个二维矩阵，用来表示哪些位置是padding，哪些位置是有效的</a><a href="https://zhuanlan.zhihu.com/p/359414250">1</a>。</li>
<li><a href="https://zhuanlan.zhihu.com/p/359414250">mask的值为0或1，0表示padding，1表示有效</a><a href="https://zhuanlan.zhihu.com/p/359414250">1</a>。</li>
<li><a href="https://zhuanlan.zhihu.com/p/359414250">在计算attention时，将mask与点积结果相加，使得padding位置的值变为负无穷</a><a href="https://zhuanlan.zhihu.com/p/359414250">1</a>。</li>
<li><a href="https://zhuanlan.zhihu.com/p/359414250">这样在softmax时，padding位置的权重就会变为0，不影响有效位置的权重</a><a href="https://zhuanlan.zhihu.com/p/359414250">1</a>。</li>
</ul>
<p>总结：使padding位置的值变为负无穷，这样softmax之后padding位置的权重就会变为0，不影响有效位置的权重。</p>
<h2 id="不同batch可以padding到不同长度吗"><strong>不同batch可以padding到不同长度吗</strong><a hidden class="anchor" aria-hidden="true" href="#不同batch可以padding到不同长度吗">#</a></h2>
<p><strong>同一个batch内要padding到一样长度，不同batch之间可以不一样</strong></p>
<p>一般batch个句子经过embedding层之后，它的维度会变成 [batch_size, seq_length, embedding_size]，</p>
<p>不同batch之间padding的长度可以不一样，是因为神经网络模型的参数与 seq_length 这个维度并不相关。如对于RNN/LSTM 等循环神经网络，seq_length 这个维度只决定 RNN/LSTM 循环计算多少步，而不会影响模型的参数。对于Transformer，输入batch个句子中的每一个token，<strong>都是乘以同一个 Q K V参数矩阵</strong>，再去做 Self-Attention ，所以参数量和参数维度都是和 seq_length 无关的。</p>
<h2 id="multi-head-attention-优势"><strong>multi-head attention 优势</strong><a hidden class="anchor" aria-hidden="true" href="#multi-head-attention-优势">#</a></h2>
<p><a href="https://zhuanlan.zhihu.com/p/266448080">Multi-head attention是一种注意力机制，它可以让模型同时关注不同位置和不同表示子空间的信息</a><a href="https://zhuanlan.zhihu.com/p/266448080">1</a><a href="https://www.zhihu.com/question/341222779">2</a>。它的优势有：</p>
<ul>
<li>
<p><a href="">扩展了模型专注于不同位置的能力，比如解决代词指代的问题</a><a href="https://bing.com/search?q=multi-head+attention+%E4%BC%98%E5%8A%BF">3</a>。</p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/266448080">为注意力层提供了多个表示子空间，增加了模型的表达能力和泛化能力</a><a href="https://zhuanlan.zhihu.com/p/266448080">1</a><a href="https://www.zhihu.com/question/341222779">2</a>。</p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/365386753">可以并行计算多个注意力头，提高了计算效率</a><a href="https://zhuanlan.zhihu.com/p/365386753">4</a>。</p>
</li>
<li>
<p>由于神经网络的参数可以表示为在高维空间的稀疏矩阵，而高维的稀疏矩阵是可以分解为多个低维的稠密矩阵</p>
<p>如果不采用多头注意力机制的话，万一注意力矩阵中出现了某个极大值，再经过softmax就会使得网络只关注到极大值处的特征，而忽略了其它处的特征。多头注意力机制则可以缓解这个问题，因为将原本的特征拆分成多个头一个，每个样本可以关注到的特征变丰富了，不会因为某个头内的注意力矩阵出现了极大值而忽略了其它特征。（简单点理解就是原本的attention只是一个attention score的计算， 而转换成多头之后变成了多个attention score的叠加，减小了模型的方差，类似于bagging</p>
</li>
</ul>
<h2 id="并行化怎么体现"><strong>并行化怎么体现</strong><a hidden class="anchor" aria-hidden="true" href="#并行化怎么体现">#</a></h2>
<p>Encoder侧：模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。</p>
<p>Decode引入sequence mask就是为了并行化训练，Decoder推理过程没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。</p>
<h2 id="multi-head-attention怎么并行计算"><strong>multi-head attention怎么并行计算</strong><a hidden class="anchor" aria-hidden="true" href="#multi-head-attention怎么并行计算">#</a></h2>
<p>​	<a href="https://zhuanlan.zhihu.com/p/358206572">multi-head attention怎么并行计算的原理是</a><a href="https://bing.com/search?q=multi-head+attention%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97">1</a><a href="https://zhuanlan.zhihu.com/p/358206572">2</a>：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/358206572">将Q，K，V分别经过一个线性层，再分解为h个头</a><a href="https://bing.com/search?q=multi-head+attention%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97">1</a><a href="https://zhuanlan.zhihu.com/p/358206572">2</a>。</li>
<li><a href="https://zhuanlan.zhihu.com/p/358206572">每个头分别计算自己的attention向量</a><a href="https://bing.com/search?q=multi-head+attention%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97">1</a><a href="https://zhuanlan.zhihu.com/p/358206572">2</a>。</li>
<li><a href="https://zhuanlan.zhihu.com/p/358206572">将h个attention向量拼接起来，再经过一个线性层输出</a><a href="https://bing.com/search?q=multi-head+attention%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97">1</a><a href="https://zhuanlan.zhihu.com/p/358206572">2</a>。</li>
</ul>
<h2 id="multi-head-attention为什么需要对每个head进行降维"><strong>multi-head attention为什么需要对每个head进行降维</strong><a hidden class="anchor" aria-hidden="true" href="#multi-head-attention为什么需要对每个head进行降维">#</a></h2>
<p>将原有的<strong>高维空间转化为多个低维空间</strong>并再最后进行拼接，形成同样维度的输出，借此丰富特性信息</p>
<h2 id="qkv矩阵可以一样吗"><strong>qkv矩阵可以一样吗</strong><a hidden class="anchor" aria-hidden="true" href="#qkv矩阵可以一样吗">#</a></h2>
<p>在通常的注意力机制中，Q (query)、K (key)、V (value) 三个矩阵都是用于计算注意力权重和上下文向量的重要参数。</p>
<p>理论上来说，Q、K、V 三个矩阵可以是相同的，但通常情况下它们是不同的矩阵。如果这三个矩阵相同，表示所有的输入都使用相同的权重计算注意力向量。这种情况下，模型可能会更加简单，但是可能会限制模型的表达能力，从而导致模型的性能不佳。</p>
<p>因此，一般来说，为了使模型具有更强的表达能力，同时避免过多的参数，通常会使用不同的矩阵来计算 Q、K、V。这也是在 Transformer 中使用了多头注意力机制的原因之一，通过多个 Q、K、V 矩阵的计算，可以让模型学习到更加丰富的特征。</p>
<h2 id="qkv可以加激活吗"><strong>qkv可以加激活吗</strong><a hidden class="anchor" aria-hidden="true" href="#qkv可以加激活吗">#</a></h2>
<p>一般情况下，qkv的线性变换不需要加激活函数，因为它们的输出是用于计算注意力权重和输出的中间结果，不同于神经网络中的隐藏层输出。</p>
<p>在Transformer中，当需要加入激活函数时，一般是在Multi-Head Attention的输出上进行操作，而不是在qkv的线性变换上进行操作。例如，在Transformer Decoder中，Multi-Head Attention的输出会与解码器的输入进行拼接后再通过一个前馈神经网络（Feed-Forward Network）进行处理，在前馈神经网络中可以加入激活函数。</p>
<p>总的来说，在Transformer等模型中，qkv的线性变换不需要加激活函数，而具体的激活函数一般是在后续的网络层中进行添加的。</p>
<h2 id="两个词相互attention结果一样吗"><strong>两个词相互attention结果一样吗</strong><a hidden class="anchor" aria-hidden="true" href="#两个词相互attention结果一样吗">#</a></h2>
<p>在一般的Attention机制中，一个词所对应的Query向量和另一个词所对应的Key向量是不同的，因此a对b做attention和b对a做attention的值一般是不同的。</p>
<p>具体地说，设$a$和$b$分别为输入句子中的两个词，$q_a$和$q_b$分别为它们对应的Query向量，$k_a$和$k_b$分别为它们对应的Key向量，$v$为Value向量，则a对b做attention的值为：</p>
<p>$$\text{Attention}(q_a, k_b, v) = \frac{\exp(q_a k_b^\top)}{\sum_{j=1}^n \exp(q_a k_j^\top)} v_b$$</p>
<p>而b对a做attention的值为：</p>
<p>$$\text{Attention}(q_b, k_a, v) = \frac{\exp(q_b k_a^\top)}{\sum_{j=1}^n \exp(q_b k_j^\top)} v_a$$</p>
<p>其中$n$为输入句子的长度。因为$q_a$和$q_b$、$k_a$和$k_b$是不同的向量，所以两个Attention的值一般是不同的。</p>
<h2 id="bahdanau-attention和-luong-attenion的区别"><strong>Bahdanau Attention和 Luong Attenion的区别</strong><a hidden class="anchor" aria-hidden="true" href="#bahdanau-attention和-luong-attenion的区别">#</a></h2>
<p><strong>Bahdanau Attention计算过程</strong></p>
<ol>
<li>前一次的隐藏状态和encoder的output match计算得到attention weight</li>
<li>attention weight 和 encoder output计算得到context vector</li>
<li>context vector作为当前时间步的输入，同时当前时间步的输入还有前一次的隐藏状态</li>
<li>得到当前时间步的输出和隐藏状态</li>
</ol>
<p><strong>Luong Attenion计算过程</strong></p>
<ol>
<li>GRU计算得到的decoder的hidden_state(若没有则可以初始化一个)</li>
<li>hidden state 和encoder的hidden计算得到a_t(attention weight)</li>
<li>attention weight 和 encoder 的output计算得到context vector</li>
<li>context vector 和 GRU的当前时间步的output合并计算得到最终的输出</li>
</ol>
<h2 id="交叉熵的预测值p是怎么来的"><strong>交叉熵的预测值p是怎么来的</strong><a hidden class="anchor" aria-hidden="true" href="#交叉熵的预测值p是怎么来的">#</a></h2>
<p>cls降维，768维降到1维。</p>
<h2 id="残差和ln的作用"><strong>残差和LN的作用</strong><a hidden class="anchor" aria-hidden="true" href="#残差和ln的作用">#</a></h2>
<p>防止梯度消失，加速收敛</p>
<h2 id="transformer中有哪些结构缓解梯度消失和爆炸"><strong>Transformer中有哪些结构缓解梯度消失和爆炸</strong><a hidden class="anchor" aria-hidden="true" href="#transformer中有哪些结构缓解梯度消失和爆炸">#</a></h2>
<p>残差连接、层归一化</p>
<hr>
<h2 id="样本不均衡长尾分布"><strong>样本不均衡/长尾分布</strong><a hidden class="anchor" aria-hidden="true" href="#样本不均衡长尾分布">#</a></h2>
<p>Focal Loss、过/欠采样、以搜代分、词典匹配、Prompt</p>
<h3 id="欠过采样"><strong>欠/过采样</strong><a hidden class="anchor" aria-hidden="true" href="#欠过采样">#</a></h3>
<ul>
<li>
<p>随机过采样（Random Oversampling）：在正例样本中随机选择一些样本进行复制。</p>
</li>
<li>
<p>SMOTE算法（Synthetic Minority Over-sampling Technique）：在正例样本中，对于每个样本，选择最近的K个正例样本，并随机从这K个样本中选择一个，根据正例样本与其最近邻的差值向量，按照一定比例生成新的正例样本。</p>
</li>
<li>
<p>ADASYN算法（Adaptive Synthetic Sampling）：在SMOTE算法的基础上，考虑到在样本分布不均匀的情况下，一些正例样本可能比其他正例样本更容易被分类错误，因此对每个正例样本增加的数量是不同的，并且与其最近邻的负例样本的数量成正比。</p>
</li>
</ul>
<h3 id="focal-loss"><strong>Focal Loss</strong><a hidden class="anchor" aria-hidden="true" href="#focal-loss">#</a></h3>
<p>Focal loss是基于二分类交叉熵CE的。它是一个动态缩放的交叉熵损失，通过一个动态缩放因子，可以动态降低训练过程中易区分样本的权重，从而将重心快速聚焦在那些难区分的样本上。
$$
\text{FL}(p_t) = -\alpha_t (1-p_t)^\gamma \log(p_t)
$$
引入了一个权重因子α ∈ [ 0 , 1 ] ，当为正样本时，权重因子就是α，当为负样本时，权重因子为1-α。</p>
<p>gamma是为一个参数，范围在 [0,5]。当Pt趋向于1，即说明该样本是易区分样本，此时调制因子<img loading="lazy" src="https://img-blog.csdnimg.cn/6ffe946cfeeb490c9786d76f7c476cb6.png" alt="img"  />
是趋向于0，即减低了易区分样本的损失比例。</p>
<p>**多分类focal loss：**权重系数α维护成一个列表，三分类中第0类权重0.2，第1类权重0.3，第2类权重0.5</p>
<h2 id="过拟合"><strong>过拟合</strong><a hidden class="anchor" aria-hidden="true" href="#过拟合">#</a></h2>
<p>正则化、增加训练数据、数据增强、标签平滑、BatchNorm、Early-Stop、交叉验证、Dropout、Pre-trained、引入先验知识</p>
<h3 id="dropout"><strong>Dropout</strong><a hidden class="anchor" aria-hidden="true" href="#dropout">#</a></h3>
<p>Dropout说的简单一点就是：我们在前向传播的时候，随机丢弃掉一些神经元（让某个神经元的激活值以一定的概率p停止工作），让它不会太依赖某些局部的特征，使模型泛化性更强。</p>
<p>在进行Dropout操作时，每个神经元的输出被置为0的概率为p，保留的概率为1-p。因此，**为了保持网络输出的期望不变，需要在训练过程中对Dropout层的输出进行rescale，即将输出值乘以1/(1-p)。**这样，即使某些神经元被Dropout，网络的输出仍能保持期望不变。</p>
<p>需要注意的是，<strong>当进行模型推理时，不需要进行Dropout操作</strong>，因此也不需要进行rescale，不然得到的就是一个随机的结果了。<strong>在模型推理时，需要将Dropout层的保留概率设置为1，即保留所有神经元的输出。</strong></p>
<h3 id="在什么地方做dropout"><strong>在什么地方做Dropout</strong><a hidden class="anchor" aria-hidden="true" href="#在什么地方做dropout">#</a></h3>
<p>Dropout是一种在神经网络中用于减少过拟合的正则化技术，它随机地将一些神经元输出设置为零，这样可以减少神经元之间的依赖性，从而减少模型的复杂度。<strong>在训练过程中，Dropout应该在神经网络的隐藏层上进行，而不是在输入层或输出层上进行，而且是在激活函数之前。</strong></p>
<p>通常，Dropout会在神经网络的每个隐藏层中进行。在实践中，一般将Dropout放置在卷积层和全连接层之间。在卷积层后面的Dropout有助于减少特征图之间的依赖性，而在全连接层之前的Dropout有助于减少神经元之间的依赖性。</p>
<p>需要注意的是，Dropout在训练过程中起作用，而在测试过程中不会起作用，因此在测试时不需要使用Dropout。在测试过程中，可以通过缩放每个神经元的输出来近似Dropout的效果，通常使用的比例为0.5。</p>
<h3 id="为什么在激活函数之后"><strong>为什么在激活函数之后</strong><a hidden class="anchor" aria-hidden="true" href="#为什么在激活函数之后">#</a></h3>
<p>在神经网络的Dropout层一般放在激活函数之后。这是因为Dropout是一种正则化方法，它的作用是随机地让一部分神经元的输出值变为0，从而减少了神经元之间的相互依赖性，防止过拟合。<strong>如果将Dropout放在激活函数之前，那么就会使得神经元输出值为0的概率变成一个常数，这样就会破坏掉激活函数的非线性特性，从而影响模型的表达能力。</strong></p>
<hr>
<h2 id="优化器"><strong>优化器</strong><a hidden class="anchor" aria-hidden="true" href="#优化器">#</a></h2>
<h3 id="为什么采用梯度下降来进行优化"><strong>为什么采用梯度下降来进行优化</strong><a hidden class="anchor" aria-hidden="true" href="#为什么采用梯度下降来进行优化">#</a></h3>
<p>梯度下降是一种常用的数值优化算法，被广泛应用于机器学习和深度学习中。其优点如下：</p>
<ol>
<li>高效：梯度下降算法的计算量相对较小，能够快速收敛到局部最优解。</li>
<li>可扩展性：梯度下降算法可以应用于大规模数据集，支持并行化处理。</li>
<li>可解释性：梯度下降算法能够生成梯度信息，可以帮助我们理解模型的行为和优化过程。</li>
<li>通用性：梯度下降算法是一种通用的优化方法，可以用于各种模型的训练和参数优化。</li>
</ol>
<p><strong>梯度下降算法的核心思想是利用函数的梯度信息来不断更新模型参数，以达到最小化损失函数的目的</strong>。虽然梯度下降算法可能会收敛到局部最优解，但是在实际应用中，它仍然是一种非常有效和可靠的优化方法。</p>
<h3 id="sgd"><strong>SGD</strong><a hidden class="anchor" aria-hidden="true" href="#sgd">#</a></h3>
<p>梯度下降，BGD整个数据集计算平均梯度（计算所有数据的梯度非常耗时），SGD随机抽样一个样本的梯度更新（很快，但随机性高，噪声影响严重，不一定向整体最优点下降），miniGD随机选一个小batch的样本计算平均梯度更新（得到的梯度下降方向是局部最优的，整体速度快）</p>
<h3 id="adam"><strong>Adam</strong><a hidden class="anchor" aria-hidden="true" href="#adam">#</a></h3>
<p><strong>SGD下降方法的缺点是参数更新方向只依赖于当前batch计算出的梯度，因此十分的不稳定。为了抑制SGD的震荡，引入了动量</strong>，认为梯度下降的过程中可以<strong>加入惯性</strong>。Adam的出现就自然而然了，它是前述方法的集大成者。动量在SGD的基础上增加了一阶动量，AdaGrad和AdaDelta在SGD的基础上增加了二阶动量。<strong>Adam实际上</strong>就是将Momentum和RMSprop集合在一起，<strong>把一阶动量和二阶动量都使用起来了</strong></p>
<p>1.<strong>一阶动量：表示参数梯度的一阶矩估计</strong>，它类似于SGD中的动量项。<strong>它的作用是累积过去梯度的方向信息，从而更好地控制参数更新的方向和速度。</strong></p>
<p>2.<strong>二阶动量：表示参数梯度的二阶矩估计</strong>，它可以理解为对梯度的方差进行估计。<strong>它的作用是调整每个参数的学习率，从而更好地控制参数更新的步长。</strong></p>
<h3 id="adam-w"><strong>Adam-W</strong><a hidden class="anchor" aria-hidden="true" href="#adam-w">#</a></h3>
<p>Adamw 即 Adam + weight decate（权重衰减） ,<strong>效果与 Adam + L2正则相同</strong>,但是计算效率更高,因为L2正则化需要在loss中加入正则项,之后再算梯度,最后在反向传播,而Adamw直接将正则项的梯度加入反向传播的公式中,省去了手动在loss中加正则项这一步</p>
<hr>
<h2 id="评价指标"><strong>评价指标</strong><a hidden class="anchor" aria-hidden="true" href="#评价指标">#</a></h2>
<p>（Acc、Precision、Recall、F1、ROC、AUC、代码实现AUC）</p>
<h3 id="accprecisionrecall"><strong>Acc、Precision、Recall</strong><a hidden class="anchor" aria-hidden="true" href="#accprecisionrecall">#</a></h3>
<p>这些指标是评估二分类或多分类模型性能的常见指标，下面是它们的计算方法：</p>
<p>假设我们有一个二分类问题，其中有两个类别，分别为正类（positive）和负类（negative）。</p>
<ul>
<li>
<p>准确率（Accuracy）：是分类正确的样本数与总样本数之比，表示模型分类的准确程度。</p>
<p>Accuracy=TP+TN/TP+TN+FP+FN</p>
<p>其中，TP（True Positive）表示真正类，即被正确地预测为正类的样本数；TN（True Negative）表示真负类，即被正确地预测为负类的样本数；FP（False Positive）表示假正类，即被错误地预测为正类的样本数；FN（False Negative）表示假负类，即被错误地预测为负类的样本数。</p>
</li>
<li>
<p>精确率（Precision）：表示预测为正类的样本中真正为正类的比例，是衡量模型的预测精度的指标。</p>
<p>Precision=TP/TP+FP</p>
</li>
<li>
<p>召回率（Recall）：表示真正为正类的样本中被正确预测为正类的比例，是衡量模型识别正例的能力的指标。</p>
<p>Recall=TP/TP+FN</p>
</li>
<li>
<p>F1值（F1 score）：综合了精确率和召回率，是一个综合指标，用于平衡模型的精度和召回率。</p>
<p>F1=2×（Precision×Recall）/（Precision+Recall）</p>
</li>
</ul>
<p>其中，精确率和召回率越高，模型的性能就越好，F1值也就越高。</p>
<h3 id="ndcg"><strong>NDCG</strong><a hidden class="anchor" aria-hidden="true" href="#ndcg">#</a></h3>
<p>NDCG（Normalized Discounted Cumulative Gain）是一种用于评估推荐系统排序性能的指标，它是DCG（Discounted Cumulative Gain）的归一化版本。NDCG可以衡量推荐系统在不同的位置推荐的物品对用户的吸引力程度。</p>
<p>在计算NDCG之前，我们首先需要计算DCG。DCG的计算方法如下：</p>
<ol>
<li>假设我们有一个用户历史上的行为序列和一个推荐列表。</li>
<li>首先计算每个物品的相关度得分 $r_i$。可以根据用户历史行为数据，给已经发生过的行为打分，例如点击、购买等。</li>
<li>推荐列表中第 $$i$$ 个物品的得分为 $r_i$。</li>
<li>根据推荐列表中的顺序计算每个物品的排名权重 $w_i$，其中第 $$i$$ 个物品的排名权重为 $w_i = \frac{1}{\log_2(i+1)}$。这个公式表示排名越靠前的物品权重越大。</li>
<li>计算DCG，即对于前 $$k$$ 个推荐物品，计算它们的加权得分之和，其中 $$k$$ 通常是推荐列表的长度。具体来说，DCG 的计算公式为：</li>
</ol>
<p>$$DCG_k = \sum_{i=1}^k \frac{r_i}{\log_2(i+1)}$$</p>
<ol>
<li>计算NDCG，即将DCG归一化，具体做法是将DCG除以理论最优DCG。理论最优DCG是将推荐列表中的物品按照相关度得分从高到低排序后的DCG值，即</li>
</ol>
<h3 id="roc"><strong>ROC</strong><a hidden class="anchor" aria-hidden="true" href="#roc">#</a></h3>
<p>ROC曲线是一种常用的评价二分类模型性能的方法，其全称为&quot;Receiver Operating Characteristic curve&quot;，中文名为“受试者工作特征曲线”。</p>
<p>ROC曲线是一条图形化的曲线，其横轴代表假正率（False Positive Rate，FPR），纵轴代表真正率（True Positive Rate，TPR），在二分类问题中，真正率表示正确预测为正例的样本数占所有正例样本的比例，假正率则表示错误预测为正例的样本数占所有负例样本的比例。</p>
<p>ROC曲线是通过改变分类器的阈值来绘制的，当阈值从最大值开始逐渐降低时，分类器会将一些本来被分类为负例的样本改为正例，从而增加真正率，但也会增加假正率，ROC曲线反映了这种折衷关系。理想情况下，ROC曲线应该尽可能地靠近左上角，此时真正率高、假正率低，表示分类器的性能更优。</p>
<p><strong>在ROC曲线上，每一个点对应了一个不同的阈值下的真正率和假正率</strong>，<strong>曲线下方的面积（Area Under the Curve，AUC）<strong>则是ROC曲线的总体性能指标，AUC的</strong>取值范围在0-1之间，越接近1表示分类器性能越好。</strong></p>
<h3 id="auc"><strong>AUC</strong><a hidden class="anchor" aria-hidden="true" href="#auc">#</a></h3>
<p><strong>在ROC曲线上，每一个点对应了一个不同的阈值下的真正率和假正率</strong>，<strong>曲线下方的面积（Area Under the Curve，AUC）<strong>则是ROC曲线的总体性能指标，AUC的</strong>取值范围在0-1之间，越接近1表示分类器性能越好。</strong></p>
<p><strong>AUC不受正负样本比例的影响，对样本不均衡的问题鲁棒。</strong></p>
<p>原因：</p>
<p>在很多排序场景下，尤其是当前许多数据集正负样例都不太均衡；或者说因训练集过大，可能会对数据进行负采样等操作。这擦操作的前提是建立在AUC值不会受到正负样本比例的影响。看过很多博客也都在讨论：<strong>为什么AUC不会受正负样例不平衡的影响？为什么排序喜欢选择AUC作为评判指标。</strong></p>
<p>AUC不受正负样本比例影响的核心原因在于它评估的是模型对正负样本的相对排序能力，而不是具体的分类结果。TPR和FPR在不同阈值下的变化构成了ROC曲线，AUC通过计算这条曲线下的面积来反映模型性能。因此，无论正负样本的比例如何变化，只要模型对样本的排序能力不变，AUC值也不会显著变化。这使得AUC在处理不均衡数据和排序任务中尤为适用。</p>
<h3 id="auc的计算">AUC的计算<a hidden class="anchor" aria-hidden="true" href="#auc的计算">#</a></h3>
<p><a href="https://zhuanlan.zhihu.com/p/462734871">https://zhuanlan.zhihu.com/p/462734871</a></p>
<p>AUC（Area Under the Curve）通常用于评估二分类模型的性能，其计算涉及ROC曲线的构建及其下方面积的求解。计算AUC的步骤如下：</p>
<h3 id="1-计算roc曲线">1. 计算ROC曲线<a hidden class="anchor" aria-hidden="true" href="#1-计算roc曲线">#</a></h3>
<p>ROC曲线是通过遍历不同的阈值来绘制的，反映模型的真正率（True Positive Rate, TPR）和假正率（False Positive Rate, FPR）之间的关系。</p>
<h4 id="计算tpr和fpr">计算TPR和FPR：<a hidden class="anchor" aria-hidden="true" href="#计算tpr和fpr">#</a></h4>
<ul>
<li>
<p><strong>真正率（TPR）</strong>: 在不同阈值下，所有正样本中被正确识别为正样本的比例：
$$
\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}
$$</p>
<p>其中，TP是True Positives，FN是False Negatives。</p>
</li>
<li>
<p><strong>假正率（FPR）</strong>: 在不同阈值下，所有负样本中被错误识别为正样本的比例：
$$
\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
$$
其中，FP是False Positives，TN是True Negatives。</p>
</li>
</ul>
<p>AUC的计算过程涉及：</p>
<ol>
<li>根据模型的预测概率对样本排序。</li>
<li>在不同阈值下计算TPR和FPR。</li>
<li>使用梯形法则计算ROC曲线下的面积，得到AUC。</li>
</ol>
<p>AUC值介于0到1之间，越接近1表示分类器性能越好。AUC是一个衡量模型整体性能的有效指标，特别适用于样本不均衡的情况。</p>
<h3 id="roc和pr曲线"><strong>ROC和PR曲线</strong><a hidden class="anchor" aria-hidden="true" href="#roc和pr曲线">#</a></h3>
<p>AUC是ROC曲线下方区域面积。（PR曲线下的面积叫做AP即Average Precision）。这里想先对比ROC和PR曲线。</p>
<ul>
<li>
<p>ROC曲线：</p>
</li>
<li>
<p>- 横轴是<strong>FPR</strong>（负类样本中被判定为正类的比例)</p>
<p>- 纵轴是<strong>TPR（recall）</strong>（正类样本中被判定为正类的样本）</p>
</li>
<li>
<p>PR曲线：</p>
</li>
<li>
<p>- 横轴是**召回率（Recall）**查全率（TPR）。即正确预测为正的占全部实际为正的比例</p>
<p>- 纵轴是**精确率（Precision）<strong>查准率</strong>、**灵敏性（Sensitivity）。即正确预测为正的占全部预测为正的比例</p>
</li>
</ul>
<p>一些性质：</p>
<ul>
<li>
<p>如果一条曲线在ROC曲线中压过另一条曲线，那么他在PR曲线中也会相同的全面优于另一条曲线 （<a href="https://zhuanlan.zhihu.com/p/140790167">挂枝儿：ROC曲线与PR曲线的关系</a>）</p>
</li>
<li>
<p>如果一条曲线在PR曲线中压过另一条曲线，那么他在ROC曲线中也会相同的全面优于另一条曲线，通过下图应该能比较好的理解 （<a href="https://zhuanlan.zhihu.com/p/140790167">挂枝儿：ROC曲线与PR曲线的关系</a>）</p>
</li>
<li>
<p>一个好的模型在ROC图上的表现一定是偏左上角的，而在PR曲线中一定是偏右上角的。在同一个数据集下。（<a href="https://zhuanlan.zhihu.com/p/140790167">挂枝儿：ROC曲线与PR曲线的关系</a>）</p>
</li>
<li>
<p>由上面的公式可知，ROC曲线不在意真实正类的概率，而PR曲线中的precision会关注“真实的正类”，所以更适合于大量负类、少量正类这种“大海捞针”问题。（参考：<a href="https://link.zhihu.com/?target=https%3A//stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves">ROC vs precision-and-recall curves</a>）</p>
</li>
<li>
<p>ROC曲线的估计过于乐观（对负例错判为正例不敏感）。这是因为ROC横轴是FPR，而FPR对于负样本的数量变化不敏感。当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。（参考：<a href="https://zhuanlan.zhihu.com/p/34655990">wdmad：机器学习之类别不平衡问题 (2) —— ROC和PR曲线</a>）</p>
</li>
<li>
<p>当测试集中的**正负样本的分布变化的时候，ROC曲线能够保持不变。**P-R曲线会发生明显变化。因此ROC曲线能够尽可能地降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。</p>
</li>
</ul>
<h3 id="mrr"><strong>MRR</strong><a hidden class="anchor" aria-hidden="true" href="#mrr">#</a></h3>
<p>MRR（Mean Reciprocal Rank，平均倒数排名）是用来评价信息检索系统性能的常见指标。</p>
<p>MRR计算方法如下：</p>
<ol>
<li>对于每个查询，将检索结果按照相关性从高到低排序。</li>
<li>找到排名第一个相关的结果，将其排名的倒数作为该查询的得分。</li>
<li>对所有查询的得分求平均，即为MRR。</li>
</ol>
<p>公式表示为：MRR = (1/N) * ∑(1/Ranki)，其中N为查询数，Ranki为第i个查询的相关结果排名。</p>
<h3 id="map"><strong>MAP</strong><a hidden class="anchor" aria-hidden="true" href="#map">#</a></h3>
<p>MAP（Mean Average Precision，平均精度均值）是用来评价信息检索系统性能的常见指标。</p>
<p>MAP计算方法如下：</p>
<ol>
<li>对于每个查询，将检索结果按照相关性从高到低排序。</li>
<li>对于每个相关的结果，计算其累计精度（Cumulative Precision），即前面所有相关结果的精度的平均值。</li>
<li>对所有查询的累计精度求平均，即为MAP。</li>
</ol>
<p>公式表示为：MAP = (1/N) * ∑(APi)，其中N为查询数，APi为第i个查询的平均精度。</p>
<p>在实际计算中，为了处理无相关结果或相关结果数不足的情况，可以采用微调参数或插值等方法进行调整。</p>
<h3 id="arpu"><strong>ARPU</strong><a hidden class="anchor" aria-hidden="true" href="#arpu">#</a></h3>
<p>每用户平均收入</p>
<p>“**ARPU即每用户平均收入。**用于衡量电信运营商和互联网公司业务收入的指标。 ARPU注重的是一个时间段内运营商从每个用户所得到的收入。</p>
<h2 id="学习率"><strong>学习率</strong><a hidden class="anchor" aria-hidden="true" href="#学习率">#</a></h2>
<p>（衰减、warmup、自适应、平时自己使用的时候对lr有什么调整心得吗）</p>
<h3 id="warmup"><strong>warmup</strong><a hidden class="anchor" aria-hidden="true" href="#warmup">#</a></h3>
<p>由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p>
<h3 id="lr"><strong>lr</strong><a hidden class="anchor" aria-hidden="true" href="#lr">#</a></h3>
<p>2e-5, 1e-5, 5e-6</p>
<p>常见的学习率有以下几种：</p>
<ol>
<li>固定学习率（Fixed Learning Rate）：在整个训练过程中，学习率保持不变，例如常见的学习率0.1、0.01、0.001等。</li>
<li>学习率衰减（Learning Rate Decay）：在训练过程中逐渐降低学习率，例如学习率按照指数或线性函数进行衰减。</li>
<li>周期性学习率（Cyclic Learning Rate）：将学习率在一定区间内周期性地变化，例如将学习率在0.01到0.1之间进行循环。</li>
<li>自适应学习率（Adaptive Learning Rate）：根据模型参数的梯度变化情况自适应地调整学习率大小，例如Adagrad、RMSProp、Adam等。</li>
<li>学习率预热（Learning Rate Warmup）：在训练初期，将学习率逐渐增加到设定值，可以帮助模型更快地找到合适的方向，避免陷入局部最优解。</li>
</ol>
<p>不同的学习率调节方法适用于不同的场景和模型，需要根据具体情况选择合适的方法和参数。在实际使用中，通常需要进行学习率的调参，以达到更好的模型性能。</p>
<hr>
<h2 id="激活函数-1"><strong>激活函数</strong><a hidden class="anchor" aria-hidden="true" href="#激活函数-1">#</a></h2>
<h3 id="作用"><strong>作用</strong><a hidden class="anchor" aria-hidden="true" href="#作用">#</a></h3>
<p>激活函数是一种数学函数，通常用于神经网络的非线性变换，将神经元的输入转换为输出。它们在神经网络中起到了非常重要的作用，主要有以下几个方面：</p>
<ol>
<li><strong>引入非线性</strong>：激活函数将神经元的输入进行非线性变换，从而使神经网络能够学习到非线性的复杂函数关系。如果没有激活函数，多个线性变换的组合仍然是一个线性变换，那么神经网络的表达能力会非常有限。</li>
<li>改善模型的鲁棒性：激活函数能够使神经网络对输入数据的微小变化更加敏感，从而使得模型更加鲁棒，能够处理多样性的输入数据。</li>
<li>压缩数据：一些激活函数，例如sigmoid函数，能够将神经元的输出压缩到一个有限的范围内，使得模型的输出也保持在有限范围内。这对于一些需要输出概率值的任务非常有用。</li>
<li>防止梯度消失：在深度神经网络中，如果使用线性变换作为激活函数，那么在反向传播时，梯度会以指数级别逐渐消失。而使用一些非线性激活函数，如ReLU，能够避免这种情况的发生，从而更好地训练深度神经网络。</li>
</ol>
<p>总之，激活函数是神经网络中不可或缺的组成部分，它们为神经网络引入了非线性，提高了模型的表达能力和鲁棒性，同时避免了梯度消失等问题，为深度学习的发展做出了巨大贡献。</p>
<p>常见的激活函数有以下几种：</p>
<ol>
<li>
<p>Sigmoid函数：将输入映射到0到1之间，具有平滑的导数，但容易出现梯度消失的问题。</p>
</li>
<li>
<p>Tanh函数：将输入映射到-1到1之间，也具有平滑的导数，但容易出现梯度消失的问题。</p>
</li>
<li>
<p>ReLU函数：在输入为正时返回输入，否则返回0，避免了梯度消失的问题，但容易出现神经元死亡的问题。</p>
</li>
<li>
<p>LeakyReLU函数：在输入为负时返回一个小的斜率，从而避免神经元死亡的问题。</p>
</li>
<li>
<p>ELU函数：在输入为负时返回一个指数级别的值，从而避免神经元死亡的问题。</p>
</li>
<li>
<p>GELU 高斯误差线性单元：激活函数GELU的灵感来源于 relu 和 dropout，在激活中引入了<strong>随机正则</strong>的思想。gelu通过输入自身的概率分布情况，决定抛弃还是保留当前的神经元。似乎是 NLP 领域的当前最佳；尤其在 Transformer 模型中表现最好,能避免梯度消失问题。</p>
</li>
<li>
<p>Swish函数：在ReLU基础上增加了一个sigmoid函数，可以获得更好的性能。</p>
</li>
</ol>
<p>不同的激活函数适用于不同的场景和模型，需要根据具体情况选择合适的激活函数。其中，ReLU是目前最常用的激活函数之一，因为它简单高效，并且在实践中表现良好。但是，ReLU也存在一些缺点，例如神经元死亡的问题。其他激活函数的优缺点如下：</p>
<ol>
<li>Sigmoid和Tanh函数的优点在于它们可以将输入映射到0到1或-1到1之间，因此适用于需要对输入进行归一化的场景。但是，它们容易出现梯度消失的问题，从而导致训练过程缓慢或无法收敛。</li>
<li>LeakyReLU和ELU函数的优点在于它们可以避免神经元死亡的问题，并且相对ReLU函数而言，有更平滑的导数。但是，它们的计算成本较高，需要更多的计算资源。</li>
<li>Swish函数的优点在于它可以获得更好的性能，但是它的计算成本也较高。</li>
</ol>
<p>因此，在选择激活函数时，需要综合考虑模型的性能、计算成本等因素，并根据具体情况进行选择。</p>
<hr>
<h2 id="如何等概率地从n个样本中采样m个"><strong>如何等概率地从n个样本中采样m个</strong><a hidden class="anchor" aria-hidden="true" href="#如何等概率地从n个样本中采样m个">#</a></h2>
<p>每个位置i以 (m - k) / (n - i - 1) 的概率决定当前数是否选，k为前面已经抽出的数的个数。</p>
<hr>
<h2 id="pre-ln和post-ln"><strong>Pre-LN和Post-LN</strong><a hidden class="anchor" aria-hidden="true" href="#pre-ln和post-ln">#</a></h2>
<p>可以，这种方法被称为Pre-LN，它在warm-up阶段对超参数的变化更不敏感，甚至不需要warm-up，并且它在各层之间的梯度范式是几乎不变的，因此收敛速度很快。</p>
<p>训练Transformer初始阶段中，过大的学习率会出现梯度爆炸的情况，这里梯度爆炸主要是由于在LN层之间连接了残差模块，导致在训练初始阶段输出层附近期望梯度非常大；所以，一般先采用极小的学习率进行预热（即warm-up），然后在逐渐增大学习率。由于warm-up阶段采用的学习率非常小，导致了Post-LN Transformer训练时间增加；与此同时，warm-up阶段对最终结果也有着很大的影响。</p>
<p>该论文中，说明了warm-up阶段存在的必要性**，指出其根源在于LN层的位置导致层次梯度范数增长，进而导致了Post-LN Transformer的不稳定性。**根据理论和分析，Layer Normalization被放置在残差模块之间时，会导致接近输出层的期望梯度变大，如果舍弃warm-up阶段，直接采用较大的学习率进行迭代，不能有效的提升模型，优化阶段容易震荡。Layer Normalization的位置，对于控制梯度范围有着很重要的作用。</p>
<p>在之前的研究中，Pre-LN Transformer结构已经被尝试用于不同的系统中，表明了随着堆叠层数的增加，Pre-LN Transformer的效果要优于Post-LN。<strong>论文中用实验和理论论证了该结构不需warm-up阶段，依然达到了良好的结果。Pre-LN Transformer模型的结构图如图1，主要区别是LN层放置在了residual连接中。</strong></p>
<p>一般认为，Post-Norm在残差之后做归一化，对参数正则化的效果更强，进而模型的收敛性也会更好；而Pre-Norm有一部分参数直接加在了后面，没有对这部分参数进行正则化，可以在反向时防止梯度爆炸或者梯度消失，大模型的训练难度大，因而使用Pre-Norm较多。</p>
<hr>
<h2 id="学习率预热的原因"><strong>学习率预热的原因</strong><a hidden class="anchor" aria-hidden="true" href="#学习率预热的原因">#</a></h2>
<p>由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p>
<hr>
<h2 id="词向量"><strong>词向量</strong><a hidden class="anchor" aria-hidden="true" href="#词向量">#</a></h2>
<h3 id="word2vec"><strong>word2Vec</strong><a hidden class="anchor" aria-hidden="true" href="#word2vec">#</a></h3>
<p>Word2Vec是一种广泛应用于自然语言处理领域的词向量表示模型。**它通过将语料库中的单词映射到高维空间中的向量，从而捕捉到了单词之间的语义和语法关系。**Word2Vec模型由Google的Tomas Mikolov等人于2013年提出，它基于神经网络模型，可分为两种算法：连续词袋模型（Continuous Bag of Words，CBOW）和Skip-gram模型。这两种模型的核心思想都是基于上下文来预测中心词或上下文词。具体而言，CBOW模型是通过给定上下文中的词来预测中心词，而Skip-gram模型则是通过给定中心词来预测上下文中的词。模型训练完成后，可以使用这些词向量进行语义分析、情感分析、文本分类、聚类等任务，这些任务不仅能够提高模型的准确性，而且也能够降低计算复杂度。</p>
<p>普通的CBOW模型通过上下文来预测中心词，抛弃了词序信息，注意不同的论文有不同的描述，但本质都是一样的。</p>
<p><strong>词向量</strong>： 模型输入的是One-Hot Vector，隐藏层并没有激活函数，只是线性的单元，输入层维度和输出层维度是一样的，使用的是Softmax回归，模型训练完成后，我们需要的是这个模型通过训练数据所学到的参数。而这些参数，就是输入x的向量化表示，这个向量便是词向量。</p>
<h3 id="skip-gram"><strong>Skip-Gram</strong><a hidden class="anchor" aria-hidden="true" href="#skip-gram">#</a></h3>
<p>在Skip-gram模型中，输入是一个中心词，而输出则是预测在给定上下文中可能出现的词。具体地，Skip-gram模型的训练过程可以分为以下几步：</p>
<ol>
<li>对于每一个词，找到它在语料库中的上下文词汇集合（窗口大小为N，即上下文包含当前词左右各N个词）。</li>
<li>将每个上下文词表示成一个独热编码向量（one-hot vector），这个向量的维度等于语料库中单词的个数，只有当前词对应的位置上的值为1，其它位置上的值均为0。</li>
<li>将中心词表示成一个向量（也称为输入向量），这个向量的维度与上下文词汇集合的总数相等。</li>
<li>使用神经网络对输入向量进行处理，得到一个输出向量，这个输出向量的维度也与上下文词汇集合的总数相等。</li>
<li>将输出向量通过softmax函数，将其转化为概率分布，用于预测每个上下文词在当前中心词下的概率。</li>
<li>使用交叉熵损失函数来度量预测结果与真实结果之间的差距，然后使用梯度下降算法来更新模型参数。</li>
</ol>
<p>Skip-gram模型的核心思想是通过训练来学习每个单词的词向量，并且在学习的过程中捕捉到单词之间的语义和语法关系。在训练过程中，每个单词的词向量在不断地被调整，以便使得这个单词在给定上下文中出现的概率最大化。这样，训练结束后，每个单词都会得到一个向量表示，这个向量的维度很小，但是却能够反映出这个单词的语义和语法特征。</p>
<h3 id="cbow"><strong>CBOW</strong><a hidden class="anchor" aria-hidden="true" href="#cbow">#</a></h3>
<p>CBOW模型的输入是一个上下文窗口中的多个单词，而输出则是这些单词的平均值，这个平均值可以表示为中心词的概率分布。CBOW模型的训练过程可以分为以下几个步骤：</p>
<ol>
<li>对于每一个词，找到它在语料库中的上下文词汇集合（窗口大小为N，即上下文包含当前词左右各N个词）。</li>
<li>将每个上下文词表示成一个独热编码向量（one-hot vector），这个向量的维度等于语料库中单词的个数，只有当前词对应的位置上的值为1，其它位置上的值均为0。</li>
<li>将所有的上下文词的独热编码向量加起来，得到一个上下文向量，这个向量的维度等于上下文词汇集合的总数。</li>
<li>将上下文向量输入到一个神经网络中，神经网络的隐层节点数为m，输出层的节点数为语料库中单词的个数。</li>
<li>神经网络的输出层使用softmax函数，将上下文向量映射为语料库中每个单词的概率分布，表示在给定上下文的情况下，每个单词出现的概率。</li>
<li>使用交叉熵损失函数来度量预测结果与真实结果之间的差距，然后使用梯度下降算法来更新模型参数。</li>
</ol>
<p>CBOW模型的训练过程是一个监督学习的过程，模型通过最小化交叉熵损失函数来调整模型参数，以便使得模型能够尽可能地准确地预测中心词。CBOW模型的优点是训练速度快，对生僻词有更好的处理能力，但是对于频繁出现的词汇，可能无法捕捉到其复杂的语义信息。</p>
<p>**流程：**输入为上下文单词的one-hot编码，之后乘以共享的输入权重矩阵W，所得到的向量相加求平均作为隐层向量，乘以输出权重矩阵W’，得到的向量经过激活函数的处理得到一个V维的概率分布，其中每一维代表的是一个单词，概率最大的是预测出来的中间词。</p>
<p>使用梯度下降来更新W和W’。训练完毕后，输入层的每个单词与矩阵W相乘得到的向量就是word embedding。（已知周围词预测中心词）</p>
<h3 id="word2vec的softmax"><strong>Word2vec的softmax</strong><a hidden class="anchor" aria-hidden="true" href="#word2vec的softmax">#</a></h3>
<p>softmax需要对每个词语都计算输出概率，并进行归一化，计算量很大；</p>
<p>word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。</p>
<ul>
<li>1）用huffman编码做词表示</li>
<li>2）把N分类变成了log(N)个2分类。 如要预测的term（足球）的编码长度为4，则可以把预测为&rsquo;足球&rsquo;，转换为4次二分类问题，在每个二分类上用二元逻辑回归的方法（sigmoid）；</li>
<li>3）逻辑回归的二分类中，sigmoid函数导数有很好的性质，(\sigma^{&rsquo;}(x) = \sigma(x)(1-\sigma(x)))</li>
<li>4）采用随机梯度上升求解二分类，每计算一个样本更新一次误差函数</li>
</ul>
<p>和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，**根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。**在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为&quot;Hierarchical</p>
<p>Softmax&quot;，将N分类问题转换为logN次二分类问题。</p>
<h3 id="word2vec-elmo-bert有哪些区别"><strong>word2vec ELMo BERT有哪些区别</strong><a hidden class="anchor" aria-hidden="true" href="#word2vec-elmo-bert有哪些区别">#</a></h3>
<p>word2vec得到的是静态词向量，同一个词在不同文章对应一个词向量；ELMo采用双层双向LSTM，将前向的结果和后向的结果进行拼接，最大化似然概率，看似解决了一词多义的问题，但其实是假的双向建模，前向和后向无法并行，而且会自己看到自己；而bert采用MLM实现真正的双向encoding，用transformer做encoder，可以有更深的层数、具有更好并行性，并且线性的Transformer比lstm更易免受mask标记影响，只需要通过self-attention减小mask标记权重即可，而lstm类似黑盒模型，很难确定其内部对于mask标记的处理方式。</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzk0MzIzODM5MA==&amp;mid=2247485335&amp;idx=1&amp;sn=2ee9291bb5059a74ec4b9b8e1275b276&amp;chksm=c337ba0ef4403318c36a8a3674116a4619a794312dac52ab8d6e8d6a11973cdcb2c1071c87b9rd">https://mp.weixin.qq.com/s?__biz=Mzk0MzIzODM5MA==&amp;mid=2247485335&amp;idx=1&amp;sn=2ee9291bb5059a74ec4b9b8e1275b276&amp;chksm=c337ba0ef4403318c36a8a3674116a4619a794312dac52ab8d6e8d6a11973cdcb2c1071c87b9rd</a></p>
<hr>
<h2 id="排序算法及其时间复杂度"><strong>排序算法及其时间复杂度</strong><a hidden class="anchor" aria-hidden="true" href="#排序算法及其时间复杂度">#</a></h2>
<ol>
<li>冒泡排序（Bubble Sort）：O(n^2)</li>
<li>选择排序（Selection Sort）：O(n^2)</li>
<li>插入排序（Insertion Sort）：O(n^2)</li>
<li>快速排序（Quick Sort）：O(nlogn)</li>
<li>归并排序（Merge Sort）：O(nlogn)</li>
<li>堆排序（Heap Sort）：O(nlogn)</li>
<li>计数排序（Counting Sort）：O(n+k)，其中 k 为数据范围</li>
<li>桶排序（Bucket Sort）：O(n+k)，其中 k 为桶的数量</li>
<li>基数排序（Radix Sort）：O(nk)，其中 k 为最大数的位数</li>
</ol>
<hr>
<h2 id="pointwisepairwiselistwise"><strong>Pointwise、pairwise、listwise</strong><a hidden class="anchor" aria-hidden="true" href="#pointwisepairwiselistwise">#</a></h2>
<p>pointwise把排序问题当成2分类问题，训练样本形式就是&lt;查询，文档，标签&gt;，训练目标是最小化交叉熵损失，优点是实现简单、训练集构造简单，缺点是没有考虑文档之间的相对顺序；pairwise是对每一个数据样本做一个比较关系，当一个文档比另一个文档相关排序更靠前的话，就是正例，否则便是负例，相比pointwise多了文档间的顺序关系，但是也只考虑了两个文档的先后顺序，且没有考虑文档在结果列表中出现的位置，而且对于训练数据来说，不同的查询，相关文档数量差异很大，转换为文档对之后，有的查询可能有几百对文档，有的可能只有几十个，这样不加均一化地在一起学习，模型会优先考虑文档对数量多的查询，带来偏差；listwise的基本思路是直接优化结果列表，或者说直接优化 NDCG这样的指标，从而能够学习到最佳排序结果，缺点就是训练复杂度很高。</p>
<p>在信息检索和排序任务中，常见的排序方法有Pointwise、Pairwise和Listwise三种，它们在处理排序问题时各有不同的策略和优势。</p>
<p>Pointwise方法将排序问题视为独立的二分类或回归问题。每个训练样本的形式为$\langle\text{查询}, \text{文档}, \text{标签}\rangle$，训练目标是最小化交叉熵损失。这种方法实现简单，训练数据构造容易。然而，它没有考虑文档之间的相对顺序，只考虑每个文档的独立评分。</p>
<p>Pairwise方法处理每对文档的相对关系，每对样本的形式为$\langle\text{查询}, \text{文档}_i, \text{文档}_j\rangle$，标签表示哪个文档更相关。训练目标是最小化每对文档的顺序错误。相对于Pointwise方法，Pairwise方法考虑了文档之间的相对顺序，提升了排序性能，但它只考虑了两个文档的顺序关系，未考虑文档在列表中的具体位置。此外，不同查询的相关文档数量不均衡，可能引入偏差。</p>
<p>在Pairwise方法中，常见的损失函数包括Hinge Loss和RankNet Loss。Hinge Loss通过公式$\text{loss} = \max(0, \text{margin} - (S(q, d^+) - S(q, d^-)))$计算，其中(S(q, d^+))和(S(q, d^-))分别是文档的相关性得分。RankNet Loss则利用交叉熵损失计算两个文档的顺序关系，对于两个文档 (i) 和 (j)，相关性得分为$ s_i $和$ s_j$：</p>
<p>若
$$
P_{ij} = \frac{1}{1 + e^{-(s_i - s_j)}}
$$
(i) 比 (j) 更相关，则 (P_{ij} &gt; 0.5)，反之 (P_{ij} &lt; 0.5)。</p>
<p>Pairwise方法的优化策略包括调节不同难度的pair（如为高差异和低差异的pair设置不同的margin值）、调整正负样本的采样比例以避免样本不平衡对模型的影响（通过均衡采样或加权实现），以及在训练过程中有针对性地挖掘那些难以区分的负样本以提高模型的鲁棒性。</p>
<p>Listwise方法直接优化结果列表或像NDCG这样的评价指标，考虑了整个文档列表的排序关系，能学习到更全局的最佳排序结果。虽然它能进一步提升排序效果，但也带来了更高的训练复杂度。LambdaRank方法在RankNet的基础上考虑了信息检索中的评价指标，如NDCG，由于这些指标不可导或导数不存在，直接简单地在RankNet的梯度上再乘一项新定义的Lambda梯度，以更关注top k个结果的排序。</p>
<p>综上所述，Pairwise方法通过考虑文档间的相对顺序，能有效提高排序性能，但也需要通过调节margin、均衡采样和挖掘难负例等策略来优化模型性能，而Listwise方法则通过全局优化排序指标，进一步提升排序效果，但带来了更高的训练复杂度。</p>
<h2 id="faiss向量近似检索的原理"><strong>Faiss向量近似检索的原理</strong><a hidden class="anchor" aria-hidden="true" href="#faiss向量近似检索的原理">#</a></h2>
<p>Faiss（Facebook AI Similarity Search）是一款高效的向量相似度搜索和聚类开源库，适用于大规模、高维度的向量数据。其核心原理可以分为以下几个部分：</p>
<ol>
<li>
<p><strong>向量量化（Vector Quantization）</strong>：</p>
<ul>
<li><strong>Product Quantization (PQ)</strong>：将高维向量分成多个低维子向量，每个子向量独立进行聚类，用聚类中心表示原始子向量，从而降低计算和存储成本。</li>
<li><strong>Inverted File System (IVF)</strong>：将向量分配到不同的簇中，每个簇内的向量形成倒排表。查询时，首先确定查询向量所属的簇，然后在相应的倒排表中进行搜索，提高查询速度。</li>
</ul>
</li>
<li>
<p><strong>索引结构（Index Structure）</strong>：</p>
<ul>
<li><strong>Flat Index</strong>：暴力搜索所有向量，适用于小规模数据集，精确但速度较慢。</li>
<li><strong>IVF Index</strong>：通过聚类将数据划分到倒排列表中，查询时只在部分列表中搜索。</li>
<li><strong>HNSW (Hierarchical Navigable Small World)</strong>：基于小世界图的索引结构，构建分层图加速搜索，适用于高精度、高效率的最近邻搜索。</li>
<li><strong>PQ Index</strong>：分块量化向量，压缩存储，适用于大规模、高维向量数据。</li>
</ul>
</li>
<li>
<p><strong>查询过程（Query Process）</strong>：</p>
<ul>
<li><strong>预处理</strong>：对查询向量进行标准化或量化。</li>
<li><strong>簇选择</strong>：使用IVF时，首先确定查询向量最接近的簇。</li>
<li><strong>近邻搜索</strong>：在选择的簇中进行近邻搜索，找到与查询向量最相似的向量。</li>
<li><strong>重排序</strong>：对候选向量进行重排序，确保返回的结果是最相似的向量。</li>
</ul>
</li>
<li>
<p><strong>优化策略（Optimization Strategies）</strong>：</p>
<ul>
<li><strong>SIMD指令</strong>：利用CPU的SIMD指令并行计算，加速距离计算。</li>
<li><strong>多线程并行</strong>：利用多线程并行处理多个查询，进一步提高检索速度。</li>
<li><strong>GPU加速</strong>：在支持GPU的环境中，利用GPU的并行计算能力加速向量量化和最近邻搜索。</li>
</ul>
</li>
</ol>
<h3 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h3>
<p>Faiss通过结合向量量化、倒排索引和小世界图等技术，实现了高效的向量相似度搜索，适用于不同规模和维度的向量数据。</p>
<hr>
<h2 id="对比学习"><strong>对比学习</strong><a hidden class="anchor" aria-hidden="true" href="#对比学习">#</a></h2>
<p>对比学习属于自监督学习的一种。</p>
<p>要做好对比学习，主要要解决以下问题：</p>
<ul>
<li>如何构建正例和负例数据，使得数据更多样化、更合理。</li>
<li>模型结构设计，让模型更好地提取到数据的特征。</li>
<li>损失函数设计，让正例间距离越近、负例间距离越远，并避免模型坍塌(Collapse)，即避免不同输入通过模型计算后映射到了同一个值。</li>
<li>距离函数，上述损失函数中用到的距离计算，使用什么函数更合理。</li>
</ul>
<h3 id="infonce-loss">InfoNCE Loss<a hidden class="anchor" aria-hidden="true" href="#infonce-loss">#</a></h3>
<p>InfoNCE (Info Noise-Contrastive Estimation) Loss 是一种用于对比学习（contrastive learning）的损失函数，主要用于自监督学习任务。它通过最大化相似样本的相似度、最小化不相似样本的相似度来学习有效的特征表示。InfoNCE Loss 的原理和计算过程如下：</p>
<h3 id="原理">原理<a hidden class="anchor" aria-hidden="true" href="#原理">#</a></h3>
<p>InfoNCE Loss 通过一个噪声对比估计方法来最大化查询样本和正样本（相似样本）之间的相似度，同时最小化查询样本和负样本（不相似样本）之间的相似度。它的目标是提高模型在区分正负样本对上的能力，从而学到更有意义的表示。</p>
<h3 id="计算过程">计算过程<a hidden class="anchor" aria-hidden="true" href="#计算过程">#</a></h3>
<ol>
<li>
<p><strong>定义样本对</strong>：</p>
<ul>
<li>假设我们有一个查询样本 $q $ 和一个正样本 $k^+ $ 以及 $N $ 个负样本 $ k^-_1, k^-_2, \ldots, k^-_N $。</li>
</ul>
</li>
<li>
<p><strong>相似度计算</strong>：</p>
<ul>
<li>通常使用点积（或余弦相似度）计算查询样本与正、负样本的相似度。
$$
s(q, k) = \frac{q \cdot k}{|q| |k|}
$$</li>
</ul>
</li>
<li>
<p><strong>计算损失</strong>：</p>
<ul>
<li>InfoNCE Loss 的公式如下：
$$
\mathcal{L} = -\log \frac{\exp(s(q, k^+) / \tau)}{\exp(s(q, k^+) / \tau) + \sum_{i=1}^N \exp(s(q, k^-_i) / \tau)}
$$
其中，(\tau) 是温度参数，用于调节分布的平滑程度。</li>
</ul>
</li>
<li>
<p><strong>解释</strong>：</p>
<ul>
<li>分子部分表示查询样本 ( q ) 和正样本 ( k^+ ) 之间的相似度。</li>
<li>分母部分表示查询样本 ( q ) 与所有样本（包括正样本和负样本）的相似度总和。</li>
<li>通过最大化该比值，模型被训练为提高正样本相似度，同时降低负样本相似度。</li>
</ul>
</li>
</ol>
<h3 id="优点">优点<a hidden class="anchor" aria-hidden="true" href="#优点">#</a></h3>
<ul>
<li><strong>有效性</strong>：在无监督和自监督学习任务中，InfoNCE Loss 能够有效地学到有意义的特征表示。</li>
<li><strong>灵活性</strong>：可以应用于各种对比学习框架，如SimCLR、MoCo等。</li>
</ul>
<h3 id="示例应用">示例应用<a hidden class="anchor" aria-hidden="true" href="#示例应用">#</a></h3>
<p>以 SimCLR 为例，其主要步骤如下：</p>
<ol>
<li>对输入图像进行两次随机数据增强，生成两个视图。</li>
<li>使用神经网络分别编码两个视图，得到两个特征向量。</li>
<li>计算两个特征向量之间的相似度（正样本），并与其他图像的特征向量进行对比（负样本）。</li>
<li>使用 InfoNCE Loss 计算损失，并通过反向传播更新网络参数。</li>
</ol>
<h3 id="总结-1">总结<a hidden class="anchor" aria-hidden="true" href="#总结-1">#</a></h3>
<p>InfoNCE Loss 是一种强大的工具，用于自监督对比学习，通过最大化相似样本之间的相似度和最小化不相似样本之间的相似度，来学习有效的特征表示。这种方法在各种自监督学习任务中表现出了强大的性能。</p>
<h3 id="infonce-loss和交叉熵的关系">InfoNCE loss和交叉熵的关系<a hidden class="anchor" aria-hidden="true" href="#infonce-loss和交叉熵的关系">#</a></h3>
<h3 id="总结-2">总结<a hidden class="anchor" aria-hidden="true" href="#总结-2">#</a></h3>
<ul>
<li><strong>共同点</strong>：InfoNCE Loss 和交叉熵损失的目标都是最小化预测概率与真实概率之间的差异。InfoNCE Loss 通过对比正负样本对来实现这一点，本质上是交叉熵损失在对比学习中的应用。</li>
<li><strong>不同点</strong>：交叉熵损失通常用于标准的分类任务，而 InfoNCE Loss 是专门设计用于对比学习任务，通过对比相似和不相似样本对来学习有用的特征表示。</li>
</ul>
<p>因此，InfoNCE Loss 可以看作是交叉熵损失在对比学习场景中的特化形式，它利用了相似度计算和 softmax 函数来处理正负样本对，从而实现有效的特征学习。</p>
<h3 id="对比学习中temperature的作用">对比学习中temperature的作用<a hidden class="anchor" aria-hidden="true" href="#对比学习中temperature的作用">#</a></h3>
<p>在对比学习中，温度参数（temperature parameter，通常记作 (\tau)）在计算损失函数（如 InfoNCE Loss）时起到了调节相似度分布平滑度的重要作用。具体来说：</p>
<ol>
<li>
<p><strong>调节相似度分布的平滑度</strong>：</p>
<ul>
<li>温度参数 (\tau) 用于调节 softmax 函数的平滑度。当 (\tau) 较小时，相似度的分布会变得更加尖锐，模型会更关注最相似的正样本和最不相似的负样本。反之，当 (\tau) 较大时，相似度的分布会变得更加平滑，模型对所有样本的关注度更加均匀。</li>
</ul>
</li>
<li>
<p><strong>控制对负样本的惩罚力度</strong>：</p>
<ul>
<li>较小的 (\tau) 会强调正负样本对之间的区分，使得模型对负样本的惩罚更加严格。较大的 (\tau) 则会减小对负样本的惩罚，使得训练过程更为平滑。</li>
</ul>
</li>
<li>
<p><strong>影响学习速度和稳定性</strong>：</p>
<ul>
<li>较小的 (\tau) 会导致梯度变大，可能加快学习速度，但也可能导致训练不稳定或陷入局部最优。较大的 (\tau) 则会使梯度变小，训练过程更为平稳，但可能需要更多的训练时间来收敛。</li>
</ul>
</li>
</ol>
<p>如果<strong>温度系数设的越大</strong>，logits分布变得越平滑，那么<strong>对比损失会对所有的负样本一视同仁，导致模型学习没有轻重</strong>。如果<strong>温度系数设的过小</strong>，则<strong>模型会越关注特别困难的负样本</strong>，但其实那些负样本很可能是潜在的正样本，这样会导致模型很难收敛或者泛化能力差。</p>
<p><strong>总之，温度系数的作用就是它控制了模型对负样本的区分度。</strong></p>
<hr>
<h2 id="tf-serving"><strong>TF Serving</strong><a hidden class="anchor" aria-hidden="true" href="#tf-serving">#</a></h2>
<p>Tensorflow则提供了TFserving方案来部署线上模型推理。</p>
<p>TFServing具有如下明显的优势：</p>
<p>(1) 支持docker部署：常用的部署方式是使用docker部署，将宿主机的模型目录挂载在docker的虚拟目录下，拉取的docker镜像可选择支持CPU或GPU，也提供了版本选择，方便调试。</p>
<p>(2) 支持多种通信方式：目前TF Serving的客户端和服务端支持的的通信方式为：gRPCAPI和RESTfull API，前者端口为8500，后者为8501，两种方式分别对应了不同的传输方法。</p>
<p>(3) 挂载模型简单：使用TF Serving部署模型，不需要部署代码，但需要必要的客户端代码。因为模型使用挂载的方式，对同一模型的不同版本，Serving会自动刷新选择最新的模型版本，更新版本也不需要重启服务。同时TFServing也支持多模型部署，因而使用起来比较方便。</p>
<p>在实际的操作过程中，发现部署TFServing存在以下几个难点，或者说部署时要特别注意的地方：</p>
<p>(1) 模型要求：对于大多数训练模型都是以CKPT文件保存，但是TFServing推荐的模型保存方式为PB模型。PB文件里保存了图的结构和变量数据，文件大小相对CKPT来说较小。但是对于CKPT模型，还必须找出它的输入输出节点，才能转换为PB模型。如果使用的是不熟悉的CKPT模型，那么模型转换操作会比较复杂。</p>
<p>(2) 参数设置：PB模型的参数信息在生成模型的时候已经保存在模型内，并且在Serving启动后被直接调用，所以如果想要对参数进行修改会相对困难。目前看到最多的解决方法是直接在生成PB模型的时候修改，模型应用会比较死板。</p>
<hr>
<h2 id="推荐系统">推荐系统<a hidden class="anchor" aria-hidden="true" href="#推荐系统">#</a></h2>
<h3 id="召回"><strong>召回</strong><a hidden class="anchor" aria-hidden="true" href="#召回">#</a></h3>
<p>召回模型从传统的召回算法，如基于用户的协同过滤、基于商品的协同过滤、基于矩阵分解的召回算法等，演变到embedding相关的模型，如 Item2Vec 召回，FM召回，以及基于Graph Embedding 的召回模型，如DeepWalk，Node2Vec等模型，对于一些Item还有包含Word2Vec、FastText、Bert等基于内容语义的召回算法，当然还有如Youtube DNN、DSSM等基于深度学习的召回算法。</p>
<p><img loading="lazy" src="https://pic1.zhimg.com/80/v2-c354dce0c7a0d2dfa1c1684a85017dd0_1440w.webp" alt=""  />
</p>
<h4 id="fm召回"><strong>FM召回</strong><a hidden class="anchor" aria-hidden="true" href="#fm召回">#</a></h4>
<p><a href="https://toutiao.io/posts/gxvzeiv/preview">https://toutiao.io/posts/gxvzeiv/preview</a>  &lt;&mdash;&ndash;老好了！</p>
<p>FM召回的主流作法，是用生成的user embedding直接查找最相近的item embedding。</p>
<h4 id="dcn"><strong>DCN</strong><a hidden class="anchor" aria-hidden="true" href="#dcn">#</a></h4>
<h4 id="dssm"><strong>DSSM</strong><a hidden class="anchor" aria-hidden="true" href="#dssm">#</a></h4>
<p><a href="https://zhuanlan.zhihu.com/p/636689560">https://zhuanlan.zhihu.com/p/636689560</a></p>
<h3 id="多路召回的必要性"><strong>多路召回的必要性</strong><a hidden class="anchor" aria-hidden="true" href="#多路召回的必要性">#</a></h3>
<p>在召回阶段，我们通常要考虑召回率和计算速度，以新闻推荐为例，为了保证用户尽可能地感兴趣，同时还要兼顾热点和时效性数据，常常会使用多路召回。下图显示了多路召回的方法。</p>
<p><img loading="lazy" src="https://pic3.zhimg.com/80/v2-b9c8f2f64009edee39b317b5de0c690e_1440w.webp" alt=""  />
</p>
<h2 id="推荐系统排序阶段常用模型"><strong>推荐系统排序阶段常用模型</strong><a hidden class="anchor" aria-hidden="true" href="#推荐系统排序阶段常用模型">#</a></h2>
<p>初期阶段，主要是进行各种特征工程，模型主要使用LR模型。</p>
<p>中期阶段，进行二阶、 高阶特征交叉，使用FM/FFM、 GBDT+LR、 XGBoost等树模型。</p>
<p>深度阶段， 开始将特征映射至多维空间中， 然后再通过多层网络去学习特征之间的相关性（ FNN、 PNN、 Wide &amp; Deep、NFM、 AFM、 DeepFM、 xDeepFM等）。</p>
<p><img loading="lazy" src="https://pic2.zhimg.com/80/v2-a6e7671e10f4d3c735b398edb5eeee1d_1440w.webp" alt=""  />
</p>
<h3 id="widedeep"><strong>Wide&amp;Deep</strong><a hidden class="anchor" aria-hidden="true" href="#widedeep">#</a></h3>
<p><a href="https://zhuanlan.zhihu.com/p/617767615">https://zhuanlan.zhihu.com/p/617767615</a></p>
<h3 id="deepfm"><strong>DeepFM</strong><a hidden class="anchor" aria-hidden="true" href="#deepfm">#</a></h3>
<p>CTR预估是目前推荐系统的核心技术，其目标是预估用户点击推荐内容的概率。DeepFM模型包含FM和DNN两部分，FM模型可以抽取low-order（低阶）特征，DNN可以抽取high-order（高阶）特征。低阶特征可以理解为线性的特征组合，高阶特征，可以理解为经过多次线性-非线性组合操作之后形成的特征，为高度抽象特征。无需Wide&amp;Deep模型人工特征工程。由于输入仅为原始特征，而且FM和DNN共享输入向量特征，DeepFM模型训练速度很快。</p>
<p>注解：Wide&amp;Deep是一种融合浅层（wide）模型和深层（deep）模型进行联合训练的框架，综合利用浅层模型的记忆能力和深层模型的泛化能力，实现单模型对推荐系统准确性和扩展性的兼顾。</p>
<p><a href="https://zhuanlan.zhihu.com/p/636689560">https://zhuanlan.zhihu.com/p/636689560</a></p>
<h3 id="mmoe"><strong>MMOE</strong><a hidden class="anchor" aria-hidden="true" href="#mmoe">#</a></h3>
<p>mmoe 实际上就是 多个门 的 moe 网络。</p>
<p>输入多个专家的过程和moe无任何区别，这里唯一的不同是对每一个任务有一个门控网络。</p>
<p><a href="https://developer.aliyun.com/article/1152526">https://developer.aliyun.com/article/1152526</a></p>
<h3 id="行为序列建模方法"><strong>行为序列建模方法</strong><a hidden class="anchor" aria-hidden="true" href="#行为序列建模方法">#</a></h3>
<p>DIN、DIEN、DSIN：https://blog.csdn.net/Ajdidfj/article/details/129381802</p>
<h4 id="din"><strong>DIN</strong><a hidden class="anchor" aria-hidden="true" href="#din">#</a></h4>
<p>DIN模型提出的动机是利用target attention的方法，进行加权<a href="https://so.csdn.net/so/search?q=pooling&amp;spm=1001.2101.3001.7020">pooling</a>，它为历史行为的物品和当前推荐物品计算一个attention score，然后加权pooling，这样的方法更能体现用户兴趣多样性。</p>
<h4 id="dien"><strong>DIEN</strong><a hidden class="anchor" aria-hidden="true" href="#dien">#</a></h4>
<p>DIEN相比于之前的模型，即对用户的兴趣进行建模，又对建模出来的用户兴趣继续建模得到用户的兴趣变化过程。</p>
<p>输入embedding,用户历史行为序列通过GRU（引入了一个损失，为了让行为序列中的每一个时刻都有一个target item进行监督训练，也就是使用下一个行为来监督兴趣状态的学习），通过注意力机制，再通过AUGRU,输出一个embedding,和另外的非行为相关特征进行concat。</p>
<p>DIEN模型的重点就是如何将用户的行为序列转换成与用户兴趣相关的向量，在DIN中是直接通过与target item计算序列中每个元素的注意力分数，然后加权求和得到最终的兴趣表示向量。在DIEN中使用了两层结构来建模用户兴趣相关的向量。</p>
<h4 id="dsin"><strong>DSIN</strong><a hidden class="anchor" aria-hidden="true" href="#dsin">#</a></h4>
<p>这个是在DIEN的基础上又进行的一次演化，这个模型的改进出发点依然是如何通过用户的历史点击行为，从里面更好的提取用户的兴趣以及兴趣的演化过程，这个模型就是从user历史行为信息挖掘方向上进行演化的。</p>
<p>作者发现用户的行为序列的组成单位，其实应该是会话(按照用户的点击时间划分开的一段行为)，每个会话里面的点击行为呢？ 会高度相似，而会话与会话之间的行为，就不是那么相似了，但是像DIN，DIEN这两个模型，DIN的话，是直接忽略了行为之间的序列关系，使得对用户的兴趣建模或者演化不是很充分，而DIEN的话改进了DIN的序列关系的忽略缺点，但是忽视了行为序列的本质组成结构。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://welldonesanmu.github.io/tags/dl/">DL</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://welldonesanmu.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AB%E8%82%A1/">
    <span class="title">Next »</span>
    <br>
    <span>机器学习八股</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://welldonesanmu.github.io">Sanmu</a></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
